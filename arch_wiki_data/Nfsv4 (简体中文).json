{
  "title": "Nfsv4 (简体中文)",
  "url": "https://wiki.archlinux.org/title/Nfsv4_(%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87)",
  "sections": [
    {
      "title": "Introduction",
      "level": 1,
      "content": "Related articles\n\n- NFS/Troubleshooting\n\nFrom Wikipedia:\n\nThe NFS FAQ lists file systems well tested and details limitations regarding FAT32.\n\n- By default, NFS is not encrypted. Configure #TLS encryption, or configure Kerberos (sec=krb5p to provide Kerberos-based encryption), or tunnel NFS through an encrypted VPN (such as WireGuard) when dealing with sensitive data.\n- Unlike Samba, NFS does not have any user authentication by default, client access is restricted by their IP-address/hostname. Kerberos is available if stronger authentication is wanted.\n- NFS expects the user and/or user group IDs are the same on both the client and server (unless Kerberos is used). Use NFSv4 idmapping or overrule the UID/GID manually by using anonuid/anongid together with all_squash in /etc/exports.\n- NFS does not support POSIX ACLs. The NFS server will still enforce ACLs, but clients will not be able to see or modify them.\n\n"
    },
    {
      "title": "Installation",
      "level": 2,
      "content": "Both client and server only require the installation of the nfs-utils package.\n\nIt is highly recommended to use a time synchronization daemon to keep client/server clocks in sync. Without accurate clocks on all nodes, NFS can introduce unwanted delays.\n\n"
    },
    {
      "title": "Server configuration",
      "level": 2,
      "content": "Global configuration options are set in /etc/nfs.conf. Users of simple configurations should not need to edit this file.\n\nThe NFS server needs a list of directories to share, in the form of exports (see exports(5) for details) which one must define in /etc/exports or /etc/exports.d/*.exports. By default, the directories are exported with their paths as-is; for example:\n\n```\n/etc/exports\n```\n\n```\n/data/music    192.168.1.0/24(rw)\n```\n\nThe above will make the directory /data/music mountable as MyServer:/data/music for both NFSv3 and NFSv4.\n\n"
    },
    {
      "title": "Custom export root",
      "level": 3,
      "content": "Shares may be relative to the so-called NFS root. A good security practice is to define a NFS root in a discrete directory tree which will keep users limited to that mount point. Bind mounts are used to link the share mount point to the actual directory elsewhere on the filesystem. An NFS root used to be mandatory for NFSv4 in the past; it is now optional (as of kernel 2.6.33 and nfs-utils 1.2.2, which implement a virtual root).\n\nConsider this following example wherein:\n\n1. The NFS root is /srv/nfs.\n1. The export is /srv/nfs/music via a bind mount to the actual target /mnt/music.\n\n```\n# mkdir -p /srv/nfs/music /mnt/music\n# mount --bind /mnt/music /srv/nfs/music\n```\n\nTo make the bind mount persistent across reboots, add it to fstab:\n\n```\n/etc/fstab\n```\n\n```\n/mnt/music /srv/nfs/music  none   bind   0   0\n```\n\nAdd directories to be shared and limit them to a range of addresses via a CIDR or hostname(s) of client machines that will be allowed to mount them in /etc/exports, e.g.:\n\n```\n/etc/exports\n```\n\n```\n/srv/nfs        192.168.1.0/24(rw,fsid=root)\n/srv/nfs/music  192.168.1.0/24(rw,sync)\n/srv/nfs/home   192.168.1.0/24(rw,sync)\n/srv/nfs/public 192.168.1.0/24(ro,all_squash,insecure) desktop(rw,sync,all_squash,anonuid=99,anongid=99) # map to user/group - in this case nobody\n```\n\nWhen using NFSv4, the option fsid=root or fsid=0 denotes the \"root\" export; if such an export is present, then all other directories must be below it. The rootdir option in the /etc/nfs.conf file has no effect on this. The default behavior, when there is no fsid=0 export, is to behave the same way as in NFSv3.\n\nIn the above example, because /srv/nfs is designated as the root, the export /srv/nfs/music is now mountable as MyServer:/music via NFSv4 – note that the root prefix is omitted.\n\nNote: **all** \n\n- For NFSv3 (not needed for NFSv4), the crossmnt option makes it possible for clients to access all filesystems mounted on a filesystem marked with crossmnt and clients will not be required to mount every child export separately. Note this may not be desirable if a child is shared with a different range of addresses.\n- Instead of crossmnt, one can also use the nohide option on child exports so that they can be automatically mounted when a client mounts the root export. Being different from crossmnt, nohide still respects address ranges of child exports. Note that the option is also NFSv3-specific; NFSv4 always behaves as if nohide was enabled.\n- The insecure option allows clients to connect from ports above 1023. (Presumably only the root user can use low-numbered ports, so blocking other ports by default creates a superficial barrier to access. In practice neither omitting nor including the insecure option provides any meaningful improvement or detriment to security.)\n- Use an asterisk (*) to allow access from any interface.\n\nIt should be noted that modifying /etc/exports while the server is running will require a re-export for changes to take effect:\n\n```\n# exportfs -arv\n```\n\nTo view the current loaded exports state in more detail, use:\n\n```\n# exportfs -v\n```\n\nFor more information about all available options see exports(5).\n\n"
    },
    {
      "title": "Starting the server",
      "level": 3,
      "content": "- To provide both NFSv3 and NFSv4 service, start and enable nfs-server.service.\n- To provide NFSv4 service exclusively, start and enable nfsv4-server.service.\n\nUsers of protocol version 4 exports will probably want to mask at a minimum both rpcbind.service and rpcbind.socket to prevent superfluous services from running. See FS#76453. Additionally, consider masking nfs-server.service which is pulled in for some reason as well.\n\n"
    },
    {
      "title": "Restricting NFS to interfaces/IPs",
      "level": 3,
      "content": "By default, starting nfs-server.service will listen for connections on all network interfaces, regardless of /etc/exports. This can be changed by defining which IPs and/or hostnames to listen on.\n\n```\n/etc/nfs.conf\n```\n\n```\n[nfsd]\nhost=192.168.1.123\n# Alternatively, use the hostname.\n# host=myhostname\n```\n\nRestart nfs-server.service to apply the changes immediately.\n\n"
    },
    {
      "title": "Firewall configuration",
      "level": 3,
      "content": "To enable access of NFSv4-servers through a firewall, TCP port 2049 must be opened for incoming connections. (NFSv4 uses a static port number; it does not use any auxiliary services such as mountd or portmapper.)\n\nTo enable access of NFSv3 servers, you will additionally need to open TCP/UDP port 111 for the portmapper (rpcbind), as well as the MOUNT (rpc.mountd) port. By default, rpc.mountd selects a port dynamically, so if you're behind a firewall you will want to edit /etc/nfs.conf to set a static port instead. Use rpcinfo -p to examine the exact ports in use on the NFSv3 server:\n\n```\n$ rpcinfo -p\n```\n\n```\n100003    3   tcp   2049  nfs\n100003    4   tcp   2049  nfs\n100227    3   tcp   2049  nfs_acl\n...\n```\n\n"
    },
    {
      "title": "Client configuration",
      "level": 2,
      "content": "Users intending to use NFS4 with Kerberos need to start and enable nfs-client.target.\n\n"
    },
    {
      "title": "Manual mounting",
      "level": 3,
      "content": "For NFSv3 use this command to show the server's exported file systems:\n\n```\n$ showmount -e servername\n```\n\nFor NFSv4 mount the root NFS directory and look around for available mounts:\n\n```\n# mount servername:/ /mountpoint/on/client\n```\n\nThen mount omitting the server's NFS export root:\n\n```\n# mount -t nfs -o vers=4 servername:/music /mountpoint/on/client\n```\n\nIf mount fails try including the server's export root (required for Debian/RHEL/SLES, some distributions need -t nfs4 instead of -t nfs):\n\n```\n# mount -t nfs -o vers=4 servername:/srv/nfs/music /mountpoint/on/client\n```\n\n"
    },
    {
      "title": "Mount using /etc/fstab",
      "level": 3,
      "content": "Using fstab is useful for a server which is always on, and the NFS shares are available whenever the client boots up. Edit /etc/fstab file, and add an appropriate line reflecting the setup. Again, the server's NFS export root is omitted.\n\n```\n/etc/fstab\n```\n\n```\nservername:/music   /mountpoint/on/client   nfs   defaults,timeo=900,retrans=5,_netdev\t0 0\n```\n\nSome additional mount options to consider:\n\n"
    },
    {
      "title": "Mount using /etc/fstab with systemd",
      "level": 3,
      "content": "Another method is using the x-systemd.automount option which mounts the filesystem upon access:\n\n```\n/etc/fstab\n```\n\n```\nservername:/home   /mountpoint/on/client  nfs  _netdev,noauto,x-systemd.automount,x-systemd.mount-timeout=10,timeo=14,x-systemd.idle-timeout=1min 0 0\n```\n\nTo make systemd aware of the changes to fstab, reload systemd and restart remote-fs.target [1].\n\nNote: **The factual accuracy of this article or section is disputed.** The factual accuracy of this article or section is disputed.\n\nThe factual accuracy of this article or section is disputed.\n\n- The noauto mount option will not mount the NFS share until it is accessed: use auto for it to be available immediately. If experiencing any issues with the mount failing due to the network not being up/available, enable NetworkManager-wait-online.service. It will ensure that network.target has all the links available prior to being active.\n- The users mount option would allow user mounts, but be aware it implies further options as noexec for example.\n- The x-systemd.idle-timeout=1min option will unmount the NFS share automatically after 1 minute of non-use. Good for laptops which might suddenly disconnect from the network.\n- If shutdown/reboot holds too long because of NFS, enable NetworkManager-wait-online.service to ensure that NetworkManager is not exited before the NFS volumes are unmounted.\n- Do not add the x-systemd.requires=network-online.target mount option as this can lead to ordering cycles within systemd [2]. systemd adds the network-online.target dependency to the unit for _netdev mount automatically.\n- Using the nocto option may improve performance for read-only mounts, but should be used only if the data on the server changes only occasionally.\n\n"
    },
    {
      "title": "As systemd unit",
      "level": 3,
      "content": "Create a new .mount file inside /etc/systemd/system, e.g. mnt-home.mount. See systemd.mount(5) for details.\n\nWhat= path to share\n\nWhere= path to mount the share\n\nOptions= share mounting options\n\n- Network mount units automatically acquire After dependencies on remote-fs-pre.target, network.target and network-online.target, and gain a Before dependency on remote-fs.target unless nofail mount option is set. Towards the latter a Wants unit is added as well.\n- Append noauto to Options preventing automatically mount during boot (unless it is pulled in by some other unit).\n- If you want to use a hostname for the server you want to share (instead of an IP address), add nss-lookup.target to After. This might avoid mount errors at boot time that do not arise when testing the unit.\n\n```\n/etc/systemd/system/mnt-home.mount\n```\n\n```\n[Unit]\nDescription=Mount home at boot\n\n[Mount]\nWhat=172.16.24.192:/home\nWhere=/mnt/home\nOptions=vers=4\nType=nfs\nTimeoutSec=30\n\n[Install]\nWantedBy=multi-user.target\n```\n\nTo use mnt-home.mount, start the unit and enable it to run on system boot.\n\n"
    },
    {
      "title": "automount",
      "level": 4,
      "content": "To automatically mount a share, one may use the following automount unit:\n\n```\n/etc/systemd/system/mnt-home.automount\n```\n\n```\n[Unit]\nDescription=Automount home\n\n[Automount]\nWhere=/mnt/home\n\n[Install]\nWantedBy=multi-user.target\n```\n\nDisable/stop the mnt-home.mount unit, and enable/start mnt-home.automount to automount the share when the mount path is being accessed.\n\n"
    },
    {
      "title": "Mount using autofs",
      "level": 3,
      "content": "Using autofs is useful when multiple machines want to connect via NFS; they could both be clients as well as servers. The reason this method is preferable over the earlier one is that if the server is switched off, the client will not throw errors about being unable to find NFS shares. See autofs#NFS network mounts for details.\n\n"
    },
    {
      "title": "NFSv4 idmapping",
      "level": 3,
      "content": "Note: **This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.** This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.\n\nThis article or section needs language, wiki syntax or style improvements. See Help:Style for reference.\n\nNote: **This article or section needs expansion.** This article or section needs expansion.\n\nThis article or section needs expansion.\n\nNote: **both** \n\n- NFSv4 idmapping does not solve all issues with the default sec=sys mount option. See #static mapping and [3]\n- NFSv4 idmapping needs to be enabled on both the client and server.\n- Another option is to make sure the user and group IDs (UID and GID) match on both the client and server.\n- Enabling/starting nfs-idmapd.service is not needed on the client to run, as it has been replaced with a new id mapper:\n\n```\n# dmesg | grep id_resolver\n```\n\n```\n[ 3238.356001] NFS: Registering the id_resolver key type\n[ 3238.356009] Key type id_resolver registered\n```\n\nNote: **clients** \n\n- Do not confuse the nfsidmap (only for nfs clients) with nfs-idmapd.service which is used by the NFS server and forks the process rpc.idmapd.\n- Both rpc.idmapd and nfsidmap share also some configurations from idmapd.conf(5).\n- See idmapd(8) and nfsidmap(8) for details.\n\nThe NFSv4 protocol represents the local system's UID and GID values on the wire as strings of the form user@domain. The process of translating from UID to string and string to UID is referred to as ID mapping.\n\n"
    },
    {
      "title": "Domain",
      "level": 4,
      "content": "- By default, the domain part of the string is the system's DNS domain name. It can also be specified in /etc/idmapd.conf if the system is multi-homed, or if the system's DNS domain name does not match the name of the system's Kerberos realm.\n- When the domain is not specified in /etc/idmapd.conf the local DNS server will be queried for the _nfsv4idmapdomain text record. If the record exists that will be used as the domain. When the record does not exist, the domain part of the DNS domain will used.\n\nDisplay the system's effective NFSv4 domain name on stdout.\n\n```\n# nfsidmap -d\n```\n\n```\ndomain.tld\n```\n\nEdit to match up the Domain on the server and/or client:\n\n```\n/etc/idmapd.conf\n```\n\n```\n[General]\nDomain = guestdomain.tld\n```\n\n"
    },
    {
      "title": "static mapping",
      "level": 4,
      "content": "- This mapping is only for the client to map uid locally. If you create a file owned by an uid (e.g. 1005) that is not known on the server. The file is stored with the uid 1005 on the server but wont be shown \"over the wire\" with the correct uid anymore.\n- You can see all entries in the keyring after interacting with the server aka. listing files:\n\n```\n# nfsidmap -l\n```\n\n```\n7 .id_resolver keys found:\n uid:nobody\n user:1\n uid:bin@domain.tld\n uid:foo@domain.tld\n gid:foo@domain.tld\n uid:remote_user@domain.tld\n uid:root@domain.tld\n```\n\n- You can clear the keyring with nfsidmap -c, but it is not needed, in the default setup after 10 Minutes entries expire.\n\nThese steps are only needed if the server and client have different user/group names. Changes are only done in the clients config file.\n\n```\n/etc/idmapd.conf\n```\n\n```\n[Translation]\n# The default is nsswitch and other methods exist.\nmethod = static,nsswitch\n\n[Static]\nfoo@domain.tld = local_foo\nremote_user@domain.tld = user\n```\n\n"
    },
    {
      "title": "fallback mapping",
      "level": 4,
      "content": "Only in the client configuration. Local user/group name to be used when a mapping cannot be completed:\n\n```\n/etc/idmapd.conf\n```\n\n```\n[Mapping]\nNobody-User = nobody\nNobody-Group = nobody\n```\n\n"
    },
    {
      "title": "Performance tuning",
      "level": 3,
      "content": "Note: **This article or section is out of date.** This article or section is out of date.\n\nThis article or section is out of date.\n\nWhen using NFS on a network with a significant number of clients one may increase the default NFS threads from 8 to 16 or even a higher, depending on the server/network requirements:\n\n```\n/etc/nfs.conf\n```\n\n```\n[nfsd]\nthreads=16\n```\n\nIt may be necessary to tune the rsize and wsize mount options to meet the requirements of the network configuration.\n\nIn recent linux kernels (>2.6.18) the size of I/O operations allowed by the NFS server (default max block size) varies depending on RAM size, with a maximum of 1M (1048576 bytes), the max block size of the server will be used even if nfs clients requires bigger rsize and wsize. See https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/5/html/5.8_technical_notes/known_issues-kernel It is possible to change the default max block size allowed by the server by writing to the /proc/fs/nfsd/max_block_size before starting nfsd. For example, the following command restores the previous default iosize of 32k:\n\n```\n# echo 32768 > /proc/fs/nfsd/max_block_size\n```\n\nTo make the change permanent, create a systemd-tmpfile:\n\n```\n/etc/tmpfiles.d/nfsd-block-size.conf\n```\n\n```\nw /proc/fs/nfsd/max_block_size - - - - 32768\n```\n\nTo mount with the increased rsize and wsize mount options:\n\n```\n# mount -t nfs -o rsize=32768,wsize=32768,vers=4 servername:/srv/nfs/music /mountpoint/on/client\n```\n\nFurthermore, despite the violation of NFS protocol, setting async instead of sync or sync,no_wdelay may potentially achieve a significant performance gain especially on spinning disks. Configure exports with this option and then execute exportfs -arv to apply.\n\n```\n/etc/exports\n```\n\n```\n/srv/nfs        192.168.1.0/24(rw,async,crossmnt,fsid=0)\n/srv/nfs/music  192.168.1.0/24(rw,async)\n```\n\n"
    },
    {
      "title": "Automatic mount handling",
      "level": 3,
      "content": "This trick is useful for NFS-shares on a wireless network and/or on a network that may be unreliable. If the NFS host becomes unreachable, the NFS share will be unmounted to hopefully prevent system hangs when using the hard mount option [4].\n\nMake sure that the NFS mount points are correctly indicated in fstab:\n\n```\n/etc/fstab\n```\n\n```\nlithium:/mnt/data           /mnt/data\t        nfs noauto 0 0\nlithium:/var/cache/pacman   /var/cache/pacman\tnfs noauto 0 0\n```\n\n- Use hostnames in fstab for this to work, not IP addresses.\n- In order to mount NFS shares with non-root users the users option has to be added.\n- The noauto mount option tells systemd to not automatically mount the shares at boot, otherwise this may cause the boot process to stall.\n\nCreate the auto_share script that will be used by cron or systemd/Timers to use ICMP ping to check if the NFS host is reachable:\n\n```\n/usr/local/bin/auto_share\n```\n\n```\n#!/bin/bash\n\nfunction net_umount {\n  umount -l -f $1 &>/dev/null\n}\n\nfunction net_mount {\n  mountpoint -q $1 || mount $1\n}\n\nNET_MOUNTS=$(sed -e '/^.*#/d' -e '/^.*:/!d' -e 's/\\t/ /g' /etc/fstab | tr -s \" \")$'\\n'b\n\nprintf %s \"$NET_MOUNTS\" | while IFS= read -r line\ndo\n  SERVER=$(echo $line | cut -f1 -d\":\")\n  MOUNT_POINT=$(echo $line | cut -f2 -d\" \")\n\n  # Check if server already tested\n  if [[ \"${server_ok[@]}\" =~ \"${SERVER}\" ]]; then\n    # The server is up, make sure the share are mounted\n    net_mount $MOUNT_POINT\n  elif [[ \"${server_notok[@]}\" =~ \"${SERVER}\" ]]; then\n    # The server could not be reached, unmount the share\n    net_umount $MOUNT_POINT\n  else\n    # Check if the server is reachable\n    ping -c 1 \"${SERVER}\" &>/dev/null\n\n    if [ $? -ne 0 ]; then\n      server_notok[${#server_notok[@]}]=$SERVER\n      # The server could not be reached, unmount the share\n      net_umount $MOUNT_POINT\n    else\n      server_ok[${#server_ok[@]}]=$SERVER\n      # The server is up, make sure the share are mounted\n      net_mount $MOUNT_POINT\n    fi\n  fi\ndone\n```\n\nNote: with:\n\n```\n# Check if the server is reachable\nping -c 1 \"${SERVER}\" &>/dev/null\n```\n\nwith:\n\n```\n# Check if the server is reachable\ntimeout 1 bash -c \": < /dev/tcp/${SERVER}/2049\"\n```\n\nMake sure the script is executable.\n\nNext check configure the script to run every X, in the examples below this is every minute.\n\n"
    },
    {
      "title": "Cron",
      "level": 4,
      "content": "```\n# crontab -e\n```\n\n```\n* * * * * /usr/local/bin/auto_share\n```\n\n"
    },
    {
      "title": "systemd/Timers",
      "level": 4,
      "content": "```\n/etc/systemd/system/auto_share.timer\n```\n\n```\n[Unit]\nDescription=Automount NFS shares every minute\n\n[Timer]\nOnCalendar=*-*-* *:*:00\n\n[Install]\nWantedBy=timers.target\n```\n\n```\n/etc/systemd/system/auto_share.service\n```\n\n```\n[Unit]\nDescription=Automount NFS shares\nAfter=network.target\n\n[Service]\nType=oneshot\nExecStart=/usr/local/bin/auto_share\n\n[Install]\nWantedBy=multi-user.target\n```\n\nFinally, enable and start auto_share.timer.\n\n"
    },
    {
      "title": "Using a NetworkManager dispatcher",
      "level": 4,
      "content": "NetworkManager can also be configured to run a script on network status change.\n\nThe easiest method for mount shares on network status change is to symlink the auto_share script:\n\n```\n# ln -s /usr/local/bin/auto_share /etc/NetworkManager/dispatcher.d/30-nfs.sh\n```\n\nHowever, in that particular case unmounting will happen only after the network connection has already been disabled, which is unclean and may result in effects like freezing of KDE Plasma applets.\n\nThe following script safely unmounts the NFS shares before the relevant network connection is disabled by listening for the down, pre-down and vpn-pre-down events, make sure the script is executable:\n\n```\n/etc/NetworkManager/dispatcher.d/30-nfs.sh\n```\n\n```\n#!/bin/sh\n\n# Find the connection UUID with \"nmcli con show\" in terminal.\n# All NetworkManager connection types are supported: wireless, VPN, wired...\nWANTED_CON_UUID=\"CHANGE-ME-NOW-9c7eff15-010a-4b1c-a786-9b4efa218ba9\"\n\nif [ \"$CONNECTION_UUID\" = \"$WANTED_CON_UUID\" ]; then\n    \n    # Script parameter $1: network interface name, not used\n    # Script parameter $2: dispatched event\n    \n    case \"$2\" in\n        \"up\")\n            mount -a -t nfs4,nfs \n            ;;\n        \"down\"|\"pre-down\"|\"vpn-pre-down\")\n            umount -l -a -t nfs4,nfs -f >/dev/null\n            ;;\n    esac\nfi\n```\n\nCreate a symlink inside /etc/NetworkManager/dispatcher.d/pre-down to catch the pre-down events:\n\n```\n# ln -s /etc/NetworkManager/dispatcher.d/30-nfs.sh /etc/NetworkManager/dispatcher.d/pre-down.d/30-nfs.sh\n```\n\n"
    },
    {
      "title": "TLS encryption",
      "level": 3,
      "content": "NFS traffic can be encrypted using TLS as of Linux 6.5 using the xprtsec=tls mount option. To begin, install the ktls-utilsAUR package on the client and server, and follow the below configuration steps for each.\n\n"
    },
    {
      "title": "Server",
      "level": 4,
      "content": "Create a private key and obtain a certificate containing your server's DNS name (see Transport Layer Security for more detail). These files do not need to be added to the system's trust store.\n\nEdit /etc/tlshd.conf to use these files, using your own values for x509.certificate and x509.private_key\n\n```\n/etc/tlshd.conf\n```\n\n```\n[authenticate.server]\nx509.certificate= /etc/nfsd-certificate.pem\nx509.private_key= /etc/nfsd-private-key.pem\n```\n\nNow start and enable tlshd.service.\n\n"
    },
    {
      "title": "Client",
      "level": 4,
      "content": "Add the server's TLS certificate generated in the previous step to the system's trust store (see Transport Layer Security for more detail).\n\nStart and enable tlshd.service.\n\nNow you should be able to mount the server using the server's DNS name:\n\n```\n# mount -o xprtsec=tls servername.domain:/ /mountpoint/on/client\n```\n\nChecking journalctl on the client should show that the TLS handshake was successful:\n\n```\n$ journalctl -b -u tlshd.service\n```\n\n```\nSep 28 11:14:46 client tlshd[227]: Built from ktls-utils 0.10 on Sep 26 2023 14:24:03\nSep 28 11:15:37 client tlshd[571]: Handshake with servername.domain (192.168.122.100) was successful\n```\n\n"
    },
    {
      "title": "Troubleshooting",
      "level": 2,
      "content": "There is a dedicated article NFS/Troubleshooting.\n\n"
    },
    {
      "title": "See also",
      "level": 2,
      "content": "- See also Avahi, a Zeroconf implementation which allows automatic discovery of NFS shares.\n- HOWTO: Diskless network boot NFS root\n- Microsoft Services for Unix NFS Client info\n- NFS on Snow Leopard\n- http://chschneider.eu/linux/server/nfs.shtml\n- How to do Linux NFS Performance Tuning and Optimization\n- Linux: Tune NFS Performance\n- Configuring an NFSv4-only Server\n\n"
    }
  ]
}