{
  "title": "Slurm-llnl",
  "url": "https://wiki.archlinux.org/title/Slurm-llnl",
  "sections": [
    {
      "title": "Introduction",
      "level": 1,
      "content": "Related articles\n\n- distcc\n- TORQUE\n\nSlurm (also referred as Slurm Workload Manager or slurm-llnl) is an open-source workload manager designed for Linux clusters of all sizes, used by many of the world's supercomputers and computer clusters. It provides three key functions. First it allocates exclusive and/or non-exclusive access to resources (computer nodes) to users for some duration of time so they can perform work. Second, it provides a framework for starting, executing, and monitoring work (typically a parallel job) on a set of allocated nodes. Finally, it arbitrates contention for resources by managing a queue of pending work.\n\n"
    },
    {
      "title": "Installation",
      "level": 2,
      "content": "Install the slurm-llnl package (or slurm-llnl-gitAUR). It pulls in munge ([1]), an authentication service, as a dependency. It is started as a requirement through slurmd's systemd service and encrypts the connection between the various hosts. Therefore make sure that all nodes in your cluster have the same key in /etc/munge/munge.key. Then start and enable munge.service.\n\nThe package itself has many more optional dependencies, though Slurm has to be recompiled to make use of them, after they have been installed.\n\n"
    },
    {
      "title": "Configuration",
      "level": 2,
      "content": "The configuration files for slurm-llnl reside under /etc/slurm-llnl. Prior to starting any slurm-services, it has to be configured properly by creating a configuration file at /etc/slurm-llnl/slurm.conf. Client and server may use the same configuration file, which can either be generated at the official website or by copying /etc/slurm-llnl/slurm.conf.example to /etc/slurm-llnl/slurm.conf and adapting it to ones liking.\n\nBy default the Slurm user, which was introduced to your system in the installation process, has 64030 as UID and GID, this simplifies the setup on multiple systems. UID and GID matches the one used in Debian, therefore they may be used side-by-side, but remember that binaries are not in the same directories on each and every distribution.\n\n"
    },
    {
      "title": "Client (compute node) configuration",
      "level": 3,
      "content": "On the client-side one may now safely start/enable slurmd.service.\n\n"
    },
    {
      "title": "Server (head node) configuration",
      "level": 3,
      "content": "Start/enable slurmctld.service.\n\nAdditionally you may want to start/enable slurmdbd.service, which handles a SQL database for easier management thereby logging somewhat essential process information.\n\n"
    },
    {
      "title": "Services fail to start on boot",
      "level": 3,
      "content": "If slurmd.service or slurmctld.service fail to start at boot but work fine when manually started, then the service may be trying to start before a network connection has been established. To verify this, add the lines associated with the failing service from below to the slurm.conf file:\n\n```\nslurm.conf\n```\n\n```\nSlurmctldDebug=info\nSlurmctldLogFile=/var/log/slurm-llnl/slurmctld.log\nSlurmdDebug=info\nSlurmdLogFile=/var/log/slurm-llnl/slurmd.log\n```\n\nThen, check the associated log file. If you notice the fatal exception mentions Address family not supported by protocol, then you may want to extend the unit so that it waits for a valid network connection via network-online.target.\n\n"
    },
    {
      "title": "Running RHEL based nodes side-by-side",
      "level": 3,
      "content": "On RedHat based distributions, slurm is running as root by default. [2] To add these nodes to the cluster, first create slurm user with UID and GID equal 64030 to match the one used in Arch Linux, then change slurm user with command slurm-setuser -u slurm -g slurm.\n\n"
    },
    {
      "title": "See also",
      "level": 2,
      "content": "- Slurm tutorials — Introduction to the Slurm Workload Manager for users and system administrators, plus some material for Slurm programmers\n- Quick Start Administrator Guide — Getting started guide\n- Slurm to manage jobs — Convenient Slurm Commands\n- Running Jobs — How Slurm is used at Harvard university\n\n"
    }
  ]
}