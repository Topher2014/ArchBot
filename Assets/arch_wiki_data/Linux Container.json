{
  "title": "Linux Container",
  "url": "https://wiki.archlinux.org/title/Linux_Container",
  "sections": [
    {
      "title": "Introduction",
      "level": 1,
      "content": "Related articles\n\n- /Using VPNs\n- Cgroups\n- Docker\n- Incus\n- LXD\n- Podman\n- systemd-nspawn\n\nLinux Containers (LXC) is a userspace interface for the Linux kernel containment features, providing a method for OS-level virtualization, using namespaces, cgroups and other Linux kernel capabilities(7) on the LXC host. lxc(7) is considered something in the middle between a chroot and a full-fledged virtual machine.\n\nIncus or LXD can be used as a manager for LXC. This page deals with using LXC directly.\n\nAlternatives for using containers comprise systemd-nspawn, Docker and Podman.\n\n"
    },
    {
      "title": "Privileged or unprivileged containers",
      "level": 2,
      "content": "LXC supports two types of containers: privileged and unprivileged.\n\nIn general, privileged containers are considered unsafe[1].\n\nOn unprivileged containers, the root UID within the container is mapped to an unprivileged UID on the host, which makes it more difficult for a hack inside the container to lead to consequences on the host system. In other words, if an attacker manages to escape the container, they should find themselves with limited or no rights on the host.\n\nThe Arch linux, linux-lts and linux-zen kernel packages currently provide out-of-the-box support for unprivileged containers. With the linux-hardened package, unprivileged containers are only available for the system administrator; hence additional kernel configuration changes are required to enable user namespaces for normal users there.\n\nThis article contains information for users to run either type of container, but additional steps may be required in order to use unprivileged containers.\n\n"
    },
    {
      "title": "An example to illustrate unprivileged containers",
      "level": 3,
      "content": "To illustrate the power of UID mapping, consider the output below from a running, unprivileged container. Therein, we see the containerized processes owned by the containerized root user in the output of ps:\n\n```\n[root@unprivileged_container /]# ps -ef | head -n 5\n```\n\n```\nUID        PID  PPID  C STIME TTY          TIME CMD\nroot         1     0  0 17:49 ?        00:00:00 /sbin/init\nroot        14     1  0 17:49 ?        00:00:00 /usr/lib/systemd/systemd-journald\ndbus        25     1  0 17:49 ?        00:00:00 /usr/bin/dbus-daemon --system --address=systemd: --nofork --nopidfile --systemd-activation\nsystemd+    26     1  0 17:49 ?        00:00:00 /usr/lib/systemd/systemd-networkd\n```\n\nOn the host, however, those containerized root processes are actually shown to be running as the mapped user (ID>99999), rather than the host's actual root user:\n\n```\n[root@host /]# lxc-info -Ssip --name sandbox\n```\n\n```\nState:          RUNNING\nPID:            26204\nCPU use:        10.51 seconds\nBlkIO use:      244.00 KiB\nMemory use:     13.09 MiB\nKMem use:       7.21 MiB\n```\n\n```\n[root@host /]# ps -ef | grep 26204 | head -n 5\n```\n\n```\nUID        PID  PPID  C STIME TTY          TIME CMD\n100000   26204 26200  0 12:49 ?        00:00:00 /sbin/init\n100000   26256 26204  0 12:49 ?        00:00:00 /usr/lib/systemd/systemd-journald\n100081   26282 26204  0 12:49 ?        00:00:00 /usr/bin/dbus-daemon --system --address=systemd: --nofork --nopidfile --systemd-activation\n100000   26284 26204  0 12:49 ?        00:00:00 /usr/lib/systemd/systemd-logind\n```\n\n"
    },
    {
      "title": "Required software",
      "level": 3,
      "content": "Installing lxc and arch-install-scripts will allow the host system to run privileged lxcs.\n\n"
    },
    {
      "title": "Enable support to run unprivileged containers (optional)",
      "level": 4,
      "content": "Modify /etc/lxc/default.conf to contain the following lines:\n\n```\nlxc.idmap = u 0 100000 65536\nlxc.idmap = g 0 100000 65536\n```\n\nIn other words, map a range of 65536 consecutive uids, starting from container-side uid 0, which shall be uid 100000 from the host’s point of view, up to and including container-side uid 65535, which the host will know as uid 165535. Apply that same mapping to gids.\n\nCreate or edit both subuid(5) at /etc/subuid and subgid(5) at /etc/subgid to contain the mapping to the containerized uid/gid pairs for each user who shall be able to run the containers. The example below is simply for the root user (and systemd system unit):\n\n```\n/etc/subuid\n```\n\n```\nroot:100000:65536\n```\n\n```\n/etc/subgid\n```\n\n```\nroot:100000:65536\n```\n\nNote: **The factual accuracy of this article or section is disputed.** The factual accuracy of this article or section is disputed.\n\nThe factual accuracy of this article or section is disputed.\n\nIn addition, running unprivileged containers as an unprivileged user only works if you delegate a cgroup in advance (the cgroup2 delegation model enforces this restriction, not liblxc). Use the following systemd command to delegate the cgroup (per LXC - Getting started: Creating unprivileged containers as a user):\n\n```\n$ systemd-run --unit=myshell --user --scope -p \"Delegate=yes\" lxc-start container_name\n```\n\nThis works similarly for other lxc commands.\n\nNote: **This article or section is a candidate for merging with cgroups#User delegation.** This article or section is a candidate for merging with cgroups#User delegation.\n\nThis article or section is a candidate for merging with cgroups#User delegation.\n\nAlternatively, delegate unprivileged cgroups by creating a systemd unit (per Rootless Containers: Enabling CPU, CPUSET, and I/O delegation):\n\n```\n/etc/systemd/system/user@.service.d/delegate.conf\n```\n\n```\n[Service]\nDelegate=cpu cpuset io memory pids\n```\n\nUsers wishing to run unprivileged containers on linux-hardened or their custom kernel need to complete several additional setup steps.\n\nFirstly, a kernel is required that has support for user namespaces (a kernel with CONFIG_USER_NS). All Arch Linux kernels have support for CONFIG_USER_NS. However, due to more general security concerns, the linux-hardened kernel does ship with user namespaces enabled only for the root user. There are two options to create unprivileged containers there:\n\n- Start the unprivileged containers only as root. Also give the sysctl setting user.max_user_namespaces a positive value to suit your environment if its current value is 0 (this fixes Failed to clone process in new user namespace errors seen in lxc info --show-log container_name).\n- Under linux-hardened & lxd 5.0.0 you may need to set /etc/subuid & /etc/subgid to use a range of root:1000000:65536. You may also need to start your first container as privileged. This fixes error newuidmap failed to write mapping \"newuidmap: uid range [0-1000000000) -> [1000000-1001000000) not allowed\"\n- Enable the sysctl setting kernel.unprivileged_userns_clone to allow normal users to run unprivileged containers. This can be done for the current session by running sysctl kernel.unprivileged_userns_clone=1 as root and can be made permanent with sysctl.d(5).\n\n"
    },
    {
      "title": "Host network configuration",
      "level": 3,
      "content": "LXC supports different virtual network types and devices (see lxc.container.conf(5) § NETWORK). A bridge device on the host is required for most types of virtual networking, which is illustrated in this section.\n\nThere are several main setups to consider:\n\n1. A host bridge\n1. A NAT bridge\n\nThe host bridge requires the host's network manager to manage a shared bridge interface. The host and any lxc will be assigned an IP address in the same network (for example 192.168.1.x). This might be more simplistic in cases where the goal is to containerize some network-exposed service like a webserver, or VPN server. The user can think of the lxc as just another PC on the physical LAN, and forward the needed ports in the router accordingly. The added simplicity can also be thought of as an added threat vector, again, if WAN traffic is being forwarded to the lxc, having it running on a separate range presents a smaller threat surface.\n\nThe NAT bridge does not require the host's network manager to manage the bridge. lxc ships with lxc-net which creates a NAT bridge called lxcbr0. The NAT bridge is a standalone bridge with a private network that is not bridged to the host's ethernet device or to a physical network. It exists as a private subnet in the host.\n\n"
    },
    {
      "title": "Using a host bridge",
      "level": 4,
      "content": "See Network bridge.\n\n"
    },
    {
      "title": "Using a NAT bridge",
      "level": 4,
      "content": "By default, lxc-net service is configured to create and use a bridge interface and a virtual Ethernet pair device, with one side assigned to the container and the other side on the host attached to the bridge. This is done automatically using dnsmasq.\n\nTo use this setup, first, Install dnsmasq, which is a dependency for lxc-net and then start and enable lxc-net.service.\n\nOne can override some lxc-net defaults by creating /etc/default/lxc-net file or editing /etc/default/lxc file using the following template:\n\n```\n/etc/default/lxc-net\n```\n\n```\n# Leave USE_LXC_BRIDGE as \"true\" if you want to use lxcbr0 for your\n# containers.  Set to \"false\" if you'll use virbr0 or another existing\n# bridge, or mavlan to your host's NIC.\nUSE_LXC_BRIDGE=\"true\"\n\n# If you change the LXC_BRIDGE to something other than lxcbr0, then\n# you will also need to update your /etc/lxc/default.conf as well as the\n# configuration (/var/lib/lxc/<container>/config) for any containers\n# already created using the default config to reflect the new bridge\n# name.\n# If you have the dnsmasq daemon installed, you'll also have to update\n# /etc/dnsmasq.d/lxc and restart the system wide dnsmasq daemon.\nLXC_BRIDGE=\"lxcbr0\"\nLXC_ADDR=\"10.0.3.1\"\nLXC_NETMASK=\"255.255.255.0\"\nLXC_NETWORK=\"10.0.3.0/24\"\nLXC_DHCP_RANGE=\"10.0.3.2,10.0.3.254\"\nLXC_DHCP_MAX=\"253\"\n# Uncomment the next line if you'd like to use a conf-file for the lxcbr0\n# dnsmasq.  For instance, you can use 'dhcp-host=mail1,10.0.3.100' to have\n# container 'mail1' always get ip address 10.0.3.100.\n#LXC_DHCP_CONFILE=/etc/lxc/dnsmasq.conf\n\n# Uncomment the next line if you want lxcbr0's dnsmasq to resolve the .lxc\n# domain.  You can then add \"server=/lxc/10.0.3.1' (or your actual $LXC_ADDR)\n# to your system dnsmasq configuration file (normally /etc/dnsmasq.conf,\n# or /etc/NetworkManager/dnsmasq.d/lxc.conf on systems that use NetworkManager).\n# Once these changes are made, restart the lxc-net and network-manager services.\n# 'container1.lxc' will then resolve on your host.\n#LXC_DOMAIN=\"lxc\"\n```\n\nIf you want a minimal setup configuration to get the container to run, just set USE_LXC_BRIDGE=\"true\".\n\nOptionally, create a configuration file to manually define the IP address of any containers:\n\n```\n/etc/lxc/dnsmasq.conf\n```\n\n```\ndhcp-host=playtime,10.0.3.100\n```\n\nDepending on which firewall the host machine is running, it might be necessary to allow inbound packets from lxcbr0 to the host, and outbound packets from lxcbr0 to traverse through the host to other networks. To test this, try to bring up a container configured to use DHCP for its IP assignment and see if lxc-net is able to assign an IP address to the container (check with lxc-ls -f. If no IP is assigned, the host's policies will need to be adjusted.\n\nUsers of ufw can simply run the following two lines to enable this:\n\n```\n# ufw allow in on lxcbr0\n# ufw route allow in on lxcbr0\n```\n\nAlternatively, users of nftables can modify /etc/nftables.conf (and reload it with nft -f /etc/nftables.conf; check if the config syntax is correct with nft -cf /etc/nftables.conf) to allow the container to have internet access (replace \"eth0\" with the device on your system that has internet access; list existing devices with ip link):\n\n```\n/etc/nftables.conf\n```\n\n```\ntable inet filter {\n  chain input {\n    ...\n    iifname \"lxcbr0\" accept comment \"Allow lxc containers\"\n    \n    pkttype host limit rate 5/second counter reject with icmpx type admin-prohibited\n    counter\n  }\n  chain forward {\n    ...\n    iifname \"lxcbr0\" oifname \"eth0\" accept comment \"Allow forwarding from lxcbr0 to eth0\"\n    iifname \"eth0\" oifname \"lxcbr0\" accept comment \"Allow forwarding from eth0 to lxcbr0\"\n  }\n}\n```\n\nAdditionally, since the container is running on the 10.0.3.x subnet, external access to services such as ssh, httpd, etc. will need to be actively forwarded to the lxc. In principle, the firewall on the host needs to forward incoming traffic on the expected port on the container.\n\nThe goal of this rule is to allow ssh traffic to the lxc:\n\n```\n# iptables -t nat -A PREROUTING -i eth0 -p tcp --dport 2221 -j DNAT --to-destination 10.0.3.100:22\n```\n\nThis rule forwards tcp traffic originating on port 2221 to the IP address of the lxc on port 22.\n\nTo ssh into the container from another PC on the LAN, one needs to ssh on port 2221 to the host. The host will then forward that traffic to the container.\n\n```\n$ ssh -p 2221 host.lan\n```\n\nIf using ufw, append the following at the bottom of /etc/ufw/before.rules to make this persistent:\n\n```\n/etc/ufw/before.rules\n```\n\n```\n*nat\n:PREROUTING ACCEPT [0:0]\n-A PREROUTING -i eth0 -p tcp --dport 2221 -j DNAT --to-destination 10.0.3.100:22\nCOMMIT\n```\n\nTo create and start containers as a non-root user, extra configuration must be applied.\n\nCreate the usernet file under /etc/lxc/lxc-usernet. According to lxc-usernet(5), the entry per line is:\n\n```\nuser type bridge number\n```\n\nConfigure the file with the user needing to create containers. The bridge will be the same as defined in /etc/default/lxc-net.\n\nA copy of the /etc/lxc/default.conf is needed in the non-root user's home directory, e.g. ~/.config/lxc/default.conf (create the directory if needed).\n\nRunning containers as a non-root user requires +x permissions on ~/.local/share/. Make that change with chmod before starting a container.\n\n"
    },
    {
      "title": "Container creation",
      "level": 3,
      "content": "Containers are built using lxc-create(1) command. With the release of lxc-3.0.0-1, upstream has deprecated locally stored templates.\n\nTo create an Arch container:\n\n```\n# lxc-create --name playtime --template download -- --dist archlinux --release current --arch amd64\n```\n\nTo create a container by interactively choosing from a list of supported distributions:\n\n```\n# lxc-create -n playtime -t download\n```\n\nTo see a list of download template options:\n\n```\n# lxc-create -t download --help\n```\n\n"
    },
    {
      "title": "Container configuration",
      "level": 3,
      "content": "The examples below can be used with privileged and unprivileged containers alike. Note that for unprivileged containers, additional lines will be present by default, which are not shown in the examples, including the lxc.idmap = u 0 100000 65536 and the lxc.idmap = g 0 100000 65536 values optionally defined in the #Enable support to run unprivileged containers (optional) section.\n\n"
    },
    {
      "title": "Basic configuration with networking",
      "level": 4,
      "content": "Configurations specific to a container, including system resources to be virtualized/isolated when a process is using the container, are defined in /var/lib/lxc/CONTAINER_NAME/config. Read lxc.container.conf(5) for the syntax and possible options of the configuration file.\n\nA basic configuration file generated when creating containers using templates. Read lxc.conf(5) for more information.\n\nBy default, the creation process will make a minimum setup without networking support. Below is an example configuration with networking supplied by lxc-net.service:\n\n```\n/var/lib/lxc/playtime/config\n```\n\n```\n# Template used to create this container: /usr/share/lxc/templates/lxc-archlinux\n# Parameters passed to the template:\n# For additional config options, please look at lxc.container.conf(5)\n\n# Distribution configuration\nlxc.include = /usr/share/lxc/config/common.conf\nlxc.arch = x86_64\n\n# Container specific configuration\nlxc.rootfs.path = dir:/var/lib/lxc/playtime/rootfs\nlxc.uts.name = playtime\n\n# Network configuration\nlxc.net.0.type = veth\nlxc.net.0.link = lxcbr0\nlxc.net.0.flags = up\nlxc.net.0.hwaddr = ee:ec:fa:e9:56:7d\n```\n\n"
    },
    {
      "title": "Mounts within the container",
      "level": 4,
      "content": "One can create a host volume outside the container's rootfs and then mount that volume inside the container. This can be advantageous, for example if the same architecture is being containerized and one wants to share pacman packages between the host and container. Another example could be shared directories. Read lxc.container.conf(5) § MOUNT POINTS for more information. The general syntax is:\n\n```\nlxc.mount.entry = /var/cache/pacman/pkg var/cache/pacman/pkg none bind 0 0\n```\n\n"
    },
    {
      "title": "Xorg program considerations (optional)",
      "level": 4,
      "content": "In order to run programs on the host's display, some bind mounts need to be defined so that the containerized programs can access the host's resources.\n\n```\n/var/lib/lxc/playtime/config\n```\n\n```\n## for xorg\nlxc.mount.entry = /dev/dri dev/dri none bind,optional,create=dir\nlxc.mount.entry = /dev/snd dev/snd none bind,optional,create=dir\nlxc.mount.entry = /tmp/.X11-unix tmp/.X11-unix none bind,optional,create=dir,ro\nlxc.mount.entry = /dev/video0 dev/video0 none bind,optional,create=file\n```\n\nNote: **The factual accuracy of this article or section is disputed.** The factual accuracy of this article or section is disputed.\n\nThe factual accuracy of this article or section is disputed.\n\nIf still experiencing a permission denied error in the LXC guest, call xhost + in the host to allow the guest to connect to the host's display server. Take note of the security concerns of opening up the display server by doing this. In addition, add the following line before the above bind mount lines.\n\n```\n/var/lib/lxc/playtime/config\n```\n\n```\nlxc.mount.entry = tmpfs tmp tmpfs defaults\n```\n\n"
    },
    {
      "title": "VPN considerations",
      "level": 4,
      "content": "To run a containerized OpenVPN or WireGuard, see Linux Containers/Using VPNs.\n\n"
    },
    {
      "title": "Basic usage",
      "level": 3,
      "content": "To list all installed LXC containers:\n\n```\n# lxc-ls -f\n```\n\nSystemd can be used to start and to stop LXCs via lxc@CONTAINER_NAME.service. Enable lxc@CONTAINER_NAME.service to have it start when the host system boots.\n\nUsers can also start/stop LXCs without systemd. Start a container:\n\n```\n# lxc-start -n CONTAINER_NAME\n```\n\nStop a container:\n\n```\n# lxc-stop -n CONTAINER_NAME\n```\n\nTo login into a container:\n\n```\n# lxc-console -n CONTAINER_NAME\n```\n\nOnce logged, treat the container like any other linux system, set the root password, create users, install packages, etc.\n\nTo attach to a container:\n\n```\n# lxc-attach -n CONTAINER_NAME --clear-env\n```\n\nThat works nearly the same as lxc-console, but it causes starts with a root prompt inside the container, bypassing login. Without the --clear-env flag, the host will pass its own environment variables into the container (including $PATH, so some commands will not work when the containers are based on another distribution).\n\n"
    },
    {
      "title": "LXC clones",
      "level": 4,
      "content": "Users with a need to run multiple containers can simplify administrative overhead (user management, system updates, etc.) by using snapshots. The strategy is to setup and keep up-to-date a single base container, then, as needed, clone (snapshot) it. The power in this strategy is that the disk space and system overhead are truly minimized since the snapshots use an overlayfs mount to only write out to disk, only the differences in data. The base system is read-only but changes to it in the snapshots are allowed via the overlayfs.\n\nNote: **This article or section needs expansion.** This article or section needs expansion.\n\nThis article or section needs expansion.\n\nFor example, setup a container as outlined above. We will call it \"base\" for the purposes of this guide. Now create 2 snapshots of \"base\" which we will call \"snap1\" and \"snap2\" with these commands:\n\n```\n# lxc-copy -n base -N snap1 -B overlayfs -s\n# lxc-copy -n base -N snap2 -B overlayfs -s\n```\n\nThe snapshots can be started/stopped like any other container. Users can optionally destroy the snapshots and all new data therein with the following command. Note that the underlying \"base\" lxc is untouched:\n\n```\n# lxc-destroy -n snap1 -f\n```\n\nSystemd units and wrapper scripts to manage snapshots for pi-hole and OpenVPN are available to automate the process in lxc-service-snapshots.\n\n"
    },
    {
      "title": "Converting a privileged container to an unprivileged container",
      "level": 3,
      "content": "Once the system has been configured to use unprivileged containers (see, #Enable support to run unprivileged containers (optional)), nsexec-bzrAUR contains a utility called uidmapshift which is able to convert an existing privileged container to an unprivileged container to avoid a total rebuild of the image.\n\n- It is recommended to backup the existing image before using this utility!\n- This utility will not shift UIDs and GIDs in ACL, users will need to shift them manually!\n\nInvoke the utility to convert over like so:\n\n```\n# uidmapshift -b /var/lib/lxc/foo 0 100000 65536\n```\n\nAdditional options are available simply by calling uidmapshift without any arguments.\n\n"
    },
    {
      "title": "Running Xorg programs",
      "level": 2,
      "content": "Either attach to or SSH into the target container and prefix the call to the program with the DISPLAY ID of the host's X session. For most simple setups, the display is always 0.\n\nAn example of running Firefox from the container in the host's display:\n\n```\n$ DISPLAY=:0 firefox\n```\n\nAlternatively, to avoid directly attaching to or connecting to the container, the following can be used on the host to automate the process:\n\n```\n# lxc-attach -n playtime --clear-env -- sudo -u YOURUSER env DISPLAY=:0 firefox\n```\n\n"
    },
    {
      "title": "Ping not working in an unprivileged container",
      "level": 3,
      "content": "In unprivileged containers, ping is likely to not work without an extra config step. Example error:\n\n```\n% ping www.google.com\nping: socktype: SOCK_RAW\nping: socket: Operation not permitted\nping: -> missing cap_net_raw+p capability or setuid?\n```\n\nTo fix this in container foo, on the host:\n\n```\n# lxc-attach -n foo -- chmod u+s /usr/bin/ping\n```\n\n"
    },
    {
      "title": "Root login fails",
      "level": 3,
      "content": "If presented with following error upon trying to login using lxc-console:\n\n```\nlogin: root\nLogin incorrect\n```\n\nAnd the container's journal shows:\n\n```\npam_securetty(login:auth): access denied: tty 'pts/0' is not secure !\n```\n\nDelete /etc/securetty[3] and /usr/share/factory/etc/securetty on the container file system. Optionally add them to NoExtract in /etc/pacman.conf to prevent them from getting reinstalled. See FS#45903 for details.\n\nAlternatively, create a new user in lxc-attach and use it for logging in to the system, then switch to root.\n\n```\n# lxc-attach -n playtime\n[root@playtime]# useradd -m -Gwheel newuser\n[root@playtime]# passwd newuser\n[root@playtime]# passwd root\n[root@playtime]# exit\n# lxc-console -n playtime\n[newuser@playtime]$ su\n```\n\n"
    },
    {
      "title": "No network-connection with veth in container config",
      "level": 3,
      "content": "If you cannot access your LAN or WAN with a networking interface configured as veth and setup through /etc/lxc/containername/config. If the virtual interface gets the ip assigned and should be connected to the network correctly.\n\n```\nip addr show veth0 \ninet 192.168.1.111/24\n```\n\nYou may disable all the relevant static ip formulas and try setting the ip through the booted container-os like you would normaly do.\n\nExample container/config\n\n```\n...\nlxc.net.0.type = veth\nlxc.net.0.name = veth0\nlxc.net.0.flags = up\nlxc.net.0.link = bridge\n...\n```\n\nAnd then assign the IP through a preferred method inside the container, see also Network configuration#Network management.\n\n"
    },
    {
      "title": "Error: unknown command",
      "level": 3,
      "content": "The error may happen when a basic command (ls, cat, etc.) on an attached container is typed hen a different Linux distribution is containerized relative to the host system (e.g. Debian container in Arch Linux host system). Upon attaching, use the argument --clear-env:\n\n```\n# lxc-attach -n container_name --clear-env\n```\n\n"
    },
    {
      "title": "Error: Failed at step KEYRING spawning...",
      "level": 3,
      "content": "Services in an unprivileged container may fail with the following message\n\n```\nsome.service: Failed to change ownership of session keyring: Permission denied\nsome.service: Failed to set up kernel keyring: Permission denied\nsome.service: Failed at step KEYRING spawning ....: Permission denied\n```\n\nCreate a file /etc/lxc/unpriv.seccomp containing\n\n```\n/etc/lxc/unpriv.seccomp\n```\n\n```\n2\nblacklist\n[all]\nkeyctl errno 38\n```\n\nThen add the following line to the container configuration after lxc.idmap\n\n```\nlxc.seccomp.profile = /etc/lxc/unpriv.seccomp\n```\n\n"
    },
    {
      "title": "lxc-execute fails due to missing lxc.init.static",
      "level": 3,
      "content": "lxc-execute fails with the error message Unable to open lxc.init.static. See FS#63814 for details.\n\nStarting containers using lxc-start works fine.\n\n"
    },
    {
      "title": "See also",
      "level": 2,
      "content": "- Official Website\n- Getting Started Guide\n- Documentation\n- Forum\n- Official GitHub Repository\n- Security Guide\n- Ubuntu LXD documentation\n\n"
    }
  ]
}