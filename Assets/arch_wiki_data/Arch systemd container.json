{
  "title": "Arch systemd container",
  "url": "https://wiki.archlinux.org/title/Arch_systemd_container",
  "sections": [
    {
      "title": "Introduction",
      "level": 1,
      "content": "Related articles\n\n- Docker\n- Linux Containers\n- systemd\n- systemd-networkd\n\nsystemd-nspawn is like the chroot command, but it is a chroot on steroids.\n\nsystemd-nspawn may be used to run a command or operating system in a light-weight namespace container. It is more powerful than chroot since it fully virtualizes the file system hierarchy, as well as the process tree, the various IPC subsystems and the host and domain name.\n\nsystemd-nspawn limits access to various kernel interfaces in the container to read-only, such as /sys, /proc/sys or /sys/fs/selinux. Network interfaces and the system clock may not be changed from within the container. Device nodes may not be created. The host system cannot be rebooted and kernel modules may not be loaded from within the container.\n\nsystemd-nspawn is a simpler tool to configure than LXC or Libvirt.\n\n"
    },
    {
      "title": "Installation",
      "level": 2,
      "content": "systemd-nspawn is part of and packaged with systemd.\n\n"
    },
    {
      "title": "Create and boot a minimal Arch Linux container",
      "level": 3,
      "content": "Create a directory to hold the container, in this example we will use ~/MyContainer.\n\nUse pacstrap from arch-install-scripts package to install a basic Arch system into the container. At minimum we need to install the base package.\n\n```\n# pacstrap -K -c ~/MyContainer base [additional packages/groups]\n```\n\nOnce your installation is finished, enter the container, and set a root password:\n\n```\n# systemd-nspawn -D ~/MyContainer\n# passwd\n# logout\n```\n\nFinally, boot into the container:\n\n```\n# systemd-nspawn -b -D ~/MyContainer\n```\n\nThe -b option will boot the container (i.e. run systemd as PID=1), instead of just running a shell, and -D specifies the directory that becomes the container's root directory.\n\nAfter the container starts, log in as \"root\" with your password.\n\nThe container can be powered off by running poweroff from within the container. From the host, containers can be controlled by the machinectl tool.\n\n"
    },
    {
      "title": "Create a Debian or Ubuntu environment",
      "level": 3,
      "content": "Install debootstrap, and one or both of debian-archive-keyring or ubuntu-keyring depending on which distribution you want.\n\nThen invoke deboostrap with the following structure:\n\n```\n# debootstrap [OPTIONS...] SUITE TARGET [MIRROR]\n```\n\n- SUITE (required) is the code name or alias for the specific version of the desired distribution as found in the scripts directory: for Debian, valid suite names are either the stable aliases stable, testing and unstable, or release names like bookworm and sid: see [1] for a list. For Ubuntu, only version names such as jammy and noble should be used and not version numbers: see [2] and [3] for a table of code names to version numbers. other references exist in the scripts directory for other Debian-based distributions such as Devuan, eLxr, Kali Linux, Pardus, PureOS, Trisquel and Tanglu (discontinued since 2017). Invoking those usually require acquiring their specific keyring and passing it to the --keyring option, or disabling checking OpenPGP signatures of retrieved Release files with --no-check-sig.\n- TARGET (required) is the directory that will contain the debootstrapped system; it will be created if it does not yet exist.\n- MIRROR (optional): the archive URL from which packages should be downloaded. For current Debian releases it can be any valid mirror such as the CDN-backed https://deb.debian.org/debian (default), and for Ubuntu any mirror from [4] such as the reference https://archive.ubuntu.com/ubuntu (also used by default).\n\n- for Debian, valid suite names are either the stable aliases stable, testing and unstable, or release names like bookworm and sid: see [1] for a list.\n- For Ubuntu, only version names such as jammy and noble should be used and not version numbers: see [2] and [3] for a table of code names to version numbers.\n- other references exist in the scripts directory for other Debian-based distributions such as Devuan, eLxr, Kali Linux, Pardus, PureOS, Trisquel and Tanglu (discontinued since 2017). Invoking those usually require acquiring their specific keyring and passing it to the --keyring option, or disabling checking OpenPGP signatures of retrieved Release files with --no-check-sig.\n\ndebootstrap cannot resolve dependencies on virtual package[6], and as a consequence it does not install systemd's dbus and libpam-systemd recommended dependencies by default[7][8]. As a consequence, some systemd/dbus-related functionalities (e.g. localectl) as well as managing the container with #machinectl do not work out of the box.\n\nIn order to get full functionality in systemd-based systems, add --include=dbus,libpam-systemd to the debootstrap invocation or install those packages once in the container:\n\n```\n# debootstrap --include=dbus,libpam-systemd stable /path/to/machine\n```\n\nJust like Arch, Debian and Ubuntu will not let you log in without a password. To set the root password, run systemd-nspawn without the -b option:\n\n```\n# systemd-nspawn -D /path/to/machine\n# passwd\n# logout\n```\n\n"
    },
    {
      "title": "Create a Fedora or AlmaLinux environment",
      "level": 3,
      "content": "Install dnf, and edit the /etc/dnf/dnf.conf file to add the required Fedora repositories.\n\n```\n/etc/dnf/dnf.conf\n```\n\n```\n[fedora]\nname=Fedora $releasever - $basearch\nmetalink=https://mirrors.fedoraproject.org/metalink?repo=fedora-$releasever&arch=$basearch\ngpgkey=https://getfedora.org/static/fedora.gpg\n\n[updates]\nname=Fedora $releasever - $basearch - Updates\nmetalink=https://mirrors.fedoraproject.org/metalink?repo=updates-released-f$releasever&arch=$basearch\ngpgkey=https://getfedora.org/static/fedora.gpg\n```\n\nThe fedora.gpg file contain the gpg keys for the latest Fedora releases https://getfedora.org/security/. To set up a minimal Fedora 37 container:\n\n```\n# mkdir /var/lib/machines/container-name\n# dnf --releasever=37 --best --setopt=install_weak_deps=False --repo=fedora --repo=updates --installroot=/var/lib/machines/container-name install dhcp-client dnf fedora-release glibc glibc-langpack-en iputils less ncurses passwd systemd systemd-networkd systemd-resolved util-linux vim-default-editor\n```\n\nNote: If you are using btrfs filesystem create a subvolume instead of creating a directory.\n\nIf you are using btrfs filesystem create a subvolume instead of creating a directory.\n\nAn Enterprise Linux derivative like AlmaLinux has three repositories enabled by default, BaseOS wich contains a core set that provides the basis for all installations, AppStream that includes additional applications, language packages, etc and Extras that contains packages not included in RHEL. So for a minimal container we only need to add the BaseOS repository to /etc/dnf/dnf.conf\n\n```\n/etc/dnf/dnf.conf\n```\n\n```\n[baseos]\nname=AlmaLinux $releasever - BaseOS\nmirrorlist=https://mirrors.almalinux.org/mirrorlist/$releasever/baseos\ngpgkey=https://repo.almalinux.org/almalinux/RPM-GPG-KEY-AlmaLinux-$releasever\n```\n\nTo create an AlmaLinux 9 minimal container:\n\n```\n# dnf --repo=baseos --releasever=9 --best --installroot=/var/lib/machines/container-name --setopt=install_weak_deps=False install almalinux-release dhcp-client dnf glibc-langpack-en iproute iputils less passwd systemd vim-minimal\n```\n\nThis will install the latest minor version of AlmaLinux 9, you can choose to install a specific point release, but you will need to change the gpgpkey entry to manually point to RPM-GPG-KEY-AlmaLinux-9\n\nJust like Arch, Fedora or AlmaLinux will not let you log in as root without a password. To set up the root password, run systemd-nspawn without the -b option:\n\n```\n# systemd-nspawn -D /var/lib/machines/container-name passwd\n```\n\n"
    },
    {
      "title": "Build and test packages",
      "level": 3,
      "content": "See Creating packages for other distributions for example uses.\n\n"
    },
    {
      "title": "Management",
      "level": 2,
      "content": "Containers located in /var/lib/machines/ can be controlled by the machinectl command, which internally controls instances of the systemd-nspawn@.service unit. The subdirectories in /var/lib/machines/ correspond to the container names, i.e. /var/lib/machines/container-name/.\n\n"
    },
    {
      "title": "Default systemd-nspawn options",
      "level": 3,
      "content": "Note that containers started via machinectl or systemd-nspawn@.service use different default options than containers started manually by the systemd-nspawn command. The extra options used by the service are:\n\n- -b/--boot – Managed containers automatically search for an init program and invoke it as PID 1.\n- --network-veth which implies --private-network – Managed containers get a virtual network interface and are disconnected from the host network. See #Networking for details.\n- -U – Managed containers use the user_namespaces(7) feature by default if supported by the kernel. See #Unprivileged containers for implications.\n- --link-journal=try-guest\n\nThis behavior can be overridden in per-container configuration files. See #Configuration for details.\n\n"
    },
    {
      "title": "machinectl",
      "level": 3,
      "content": "Containers can be managed by the machinectl subcommand container-name command. For example, to start a container:\n\n```\n$ machinectl start container-name\n```\n\nSimilarly, there are subcommands such as poweroff, reboot, status and show. See machinectl(1) § Machine Commands for detailed explanations.\n\nOther common commands are:\n\n- machinectl list – show a list of currently running containers\n- machinectl login container-name – open an interactive login session in a container\n- machinectl shell [username@]container-name – open an interactive shell session in a container (this immediately invokes a user process without going through the login process in the container)\n- machinectl enable container-name and machinectl disable container-name – enable or disable a container to start at boot, see #Enable container to start at boot for details\n\nmachinectl also has subcommands for managing container (or virtual machine) images and image transfers. See machinectl(1) § Image Commands and machinectl(1) § Image Transfer Commands for details. As of 2023Q1, the first 3 examples at machinectl(1) § EXAMPLES demonstrate image transfer commands. machinectl(1) § FILES AND DIRECTORIES discusses where to find suitable images.\n\n"
    },
    {
      "title": "systemd toolchain",
      "level": 3,
      "content": "Much of the core systemd toolchain has been updated to work with containers. Tools that do usually provide a -M, --machine= option which will take a container name as argument.\n\nExamples:\n\nSee journal logs for a particular machine:\n\n```\n# journalctl -M container-name\n```\n\nShow control group contents:\n\n```\n$ systemd-cgls -M container-name\n```\n\nSee startup time of container:\n\n```\n$ systemd-analyze -M container-name\n```\n\nFor an overview of resource usage:\n\n```\n$ systemd-cgtop\n```\n\n"
    },
    {
      "title": "Per-container settings",
      "level": 3,
      "content": "To specify per-container settings and not global overrides, the .nspawn files can be used. See systemd.nspawn(5) for details.\n\n- .nspawn files may be removed unexpectedly from /etc/systemd/nspawn/ when you run machinectl remove. [14]\n- The interaction of network options specified in the .nspawn file and on the command line does not work correctly when there is --settings=override (which is specified in the systemd-nspawn@.service file). [15] As a workaround, you need to include the option VirtualEthernet=on, even though the service specifies --network-veth.\n\n"
    },
    {
      "title": "Enable container to start at boot",
      "level": 3,
      "content": "When using a container frequently, you may want to start it at boot.\n\nFirst make sure that the machines.target is enabled.\n\nContainers discoverable by machinectl can be enabled or disabled:\n\n```\n$ machinectl enable container-name\n```\n\n- This has the effect of enabling the systemd-nspawn@container-name.service systemd unit.\n- As mentioned in #Default systemd-nspawn options, containers started by machinectl get a virtual Ethernet interface. To disable private networking, see #Host networking.\n\n"
    },
    {
      "title": "Resource control",
      "level": 3,
      "content": "You can take advantage of control groups to implement limits and resource management of your containers with systemctl set-property, see systemd.resource-control(5). For example, you may want to limit the memory amount or CPU usage. To limit the memory consumption of your container to 2 GiB:\n\n```\n# systemctl set-property systemd-nspawn@container-name.service MemoryMax=2G\n```\n\nOr to limit the CPU time usage to roughly the equivalent of 2 cores:\n\n```\n# systemctl set-property systemd-nspawn@container-name.service CPUQuota=200%\n```\n\nThis will create permanent files in /etc/systemd/system.control/systemd-nspawn@container-name.service.d/.\n\nAccording to the documentation, MemoryHigh is the preferred method to keep in check memory consumption, but it will not be hard-limited as is the case with MemoryMax. You can use both options leaving MemoryMax as the last line of defense. Also take in consideration that you will not limit the number of CPUs the container can see, but you will achieve similar results by limiting how much time the container will get at maximum, relative to the total CPU time.\n\n"
    },
    {
      "title": "Networking",
      "level": 3,
      "content": "systemd-nspawn containers can use either host networking or private networking:\n\n- In the host networking mode, the container has full access to the host network. This means that the container will be able to access all network services on the host and packets coming from the container will appear to the outside network as coming from the host (i.e. sharing the same IP address).\n- In the private networking mode, the container is disconnected from the host's network. This makes all network interfaces unavailable to the container, with the exception of the loopback device and those explicitly assigned to the container. There is a number of different ways to set up network interfaces for the container: An existing interface can be assigned to the container (e.g. if you have multiple Ethernet devices): see #Use an existing interface. A virtual network interface associated with an existing interface (i.e. VLAN interface) can be created and assigned to the container: see #Use a \"macvlan\" or \"ipvlan\" interface. A virtual Ethernet link between the host and the container can be created: see #Use a virtual Ethernet link.\n\n- An existing interface can be assigned to the container (e.g. if you have multiple Ethernet devices): see #Use an existing interface.\n- A virtual network interface associated with an existing interface (i.e. VLAN interface) can be created and assigned to the container: see #Use a \"macvlan\" or \"ipvlan\" interface.\n- A virtual Ethernet link between the host and the container can be created: see #Use a virtual Ethernet link.\n\nThe host networking mode is suitable for application containers which do not run any networking software that would configure the interface assigned to the container. Host networking is the default mode when you run systemd-nspawn from the shell.\n\nOn the other hand, the private networking mode is suitable for system containers that should be isolated from the host system. The creation of virtual Ethernet links is a very flexible tool allowing to create complex virtual networks. This is the default mode for containers started by machinectl or systemd-nspawn@.service.\n\nThe following subsections describe common scenarios. See systemd-nspawn(1) § Networking Options for details about the available systemd-nspawn options.\n\n"
    },
    {
      "title": "Host networking",
      "level": 4,
      "content": "To disable private networking and the creation of a virtual Ethernet link used by containers started with machinectl, add a .nspawn file with the following option:\n\n```\n/etc/systemd/nspawn/container-name.nspawn\n```\n\n```\n[Network]\nVirtualEthernet=no\n```\n\nThis will override the -n/--network-veth option used in systemd-nspawn@.service and the newly started containers will use the host networking mode.\n\n"
    },
    {
      "title": "Private networking",
      "level": 4,
      "content": "If a container is started with the -n/--network-veth option, systemd-nspawn will create a virtual Ethernet link between the host and the container. The host side of the link will be available as a network interface named ve-container-name. The container side of the link will be named host0. Note that this option implies --private-network.\n\n- If the container name is too long, the interface name will be shortened (e.g. ve-long-conKQGh instead of ve-long-container-name) to fit into the 15-characters limit. The full name will be set as the altname property of the interface (see ip-link(8)) and can be still used to reference the interface.\n- When examining the interfaces with ip link, interface names will be shown with a suffix, such as ve-container-name@if2 and host0@if9. The @ifN is not actually part of the interface name; instead, ip link appends this information to indicate which \"slot\" the virtual Ethernet cable connects to on the other end.\n\nWhen you start the container, an IP address has to be assigned to both interfaces (on the host and in the container). If you use systemd-networkd on the host as well as in the container, this is done out-of-the-box:\n\n- the /usr/lib/systemd/network/80-container-ve.network file on the host matches the ve-container-name interface and starts a DHCP server, which assigns IP addresses to the host interface as well as the container,\n- the /usr/lib/systemd/network/80-container-host0.network file in the container matches the host0 interface and starts a DHCP client, which receives an IP address from the host.\n\nNote: **without interfering with the existing network setup** \n\nIf you do not use systemd-networkd, you can configure static IP addresses or start a DHCP server on the host interface and a DHCP client in the container. See Network configuration for details.\n\nTo give the container access to the outside network, you can configure NAT as described in Internet sharing#Enable NAT. If you use systemd-networkd, this is done (partially) automatically via the IPMasquerade=both option in /usr/lib/systemd/network/80-container-ve.network. However, this issues just one iptables (or nftables) rule such as\n\n```\n-t nat -A POSTROUTING -s 192.168.163.192/28 -j MASQUERADE\n```\n\nThe filter table has to be configured manually as shown in Internet sharing#Enable NAT. You can use a wildcard to match all interfaces starting with ve-:\n\n```\n# iptables -A FORWARD -i ve-+ -o internet0 -j ACCEPT\n```\n\nAdditionally, you need to open the UDP port 67 on the ve-+ interfaces for incoming connections to the DHCP server (operated by systemd-networkd):\n\n```\n# iptables -A INPUT -i ve-+ -p udp -m udp --dport 67 -j ACCEPT\n```\n\nIf you have configured a network bridge on the host system, you can create a virtual Ethernet link for the container and add its host side to the network bridge. This is done with the --network-bridge=bridge-name option. Note that --network-bridge implies --network-veth, i.e. the virtual Ethernet link is created automatically. However, the host side of the link will use the vb- prefix instead of ve-, so the systemd-networkd options for starting the DHCP server and IP masquerading will not be applied.\n\nThe bridge management is left to the administrator. For example, the bridge can connect virtual interfaces with a physical interface, or it can connect only virtual interfaces of several containers. See systemd-networkd#Network bridge with DHCP and systemd-networkd#Network bridge with static IP addresses for example configurations using systemd-networkd.\n\nThere is also a --network-zone=zone-name option which is similar to --network-bridge but the network bridge is managed automatically by systemd-nspawn and systemd-networkd. The bridge interface named vz-zone-name is automatically created when the first container configured with --network-zone=zone-name is started, and is automatically removed when the last container configured with --network-zone=zone-name exits. Hence, this option makes it easy to place multiple related containers on a common virtual network. Note that vz-* interfaces are managed by systemd-networkd same way as ve-* interfaces using the options from the /usr/lib/systemd/network/80-container-vz.network file.\n\nInstead of creating a virtual Ethernet link (whose host side may or may not be added to a bridge), you can create a virtual interface on an existing physical interface (i.e. VLAN interface) and add it to the container. The virtual interface will be bridged with the underlying host interface and thus the container will be exposed to the outside network, which allows it to obtain a distinct IP address via DHCP from the same LAN as the host is connected to.\n\nsystemd-nspawn offers 2 options:\n\n- --network-macvlan=interface – the virtual interface will have a different MAC address than the underlying physical interface and will be named mv-interface.\n- --network-ipvlan=interface – the virtual interface will have the same MAC address as the underlying physical interface and will be named iv-interface.\n\nBoth options imply --private-network.\n\nIf the host system has multiple physical network interfaces, you can use the --network-interface=interface to assign interface to the container (and make it unavailable to the host while the container is started). Note that --network-interface implies --private-network.\n\n"
    },
    {
      "title": "Port mapping",
      "level": 3,
      "content": "When private networking is enabled, individual ports on the host can be mapped to ports on the container using the -p/--port option or by using the Port setting in an .nspawn file. For example, to map a TCP port 8000 on the host to the TCP port 80 in the container:\n\n```\n/etc/systemd/nspawn/container-name.nspawn\n```\n\n```\n[Network]\nPort=tcp:8000:80\n```\n\nThis works by issuing iptables rules to the nat table, but the FORWARD chain in the filter table needs to be configured manually as shown in #Use a virtual Ethernet link. Additionally, if you followed Simple stateful firewall, run the following command to allow new connections to the host's wan_interface on a forwarded port to be established:\n\n```\n# iptables -A FORWARD -i wan_interface -o ve-+ -p tcp --syn --dport 8000 -m conntrack --ctstate NEW -j ACCEPT\n```\n\n"
    },
    {
      "title": "Domain name resolution",
      "level": 3,
      "content": "Domain name resolution in the container can be configured the same way as on the host system. Additionally, systemd-nspawn provides options to manage the /etc/resolv.conf file inside the container:\n\n- --resolv-conf can be used on command-line\n- ResolvConf= can be used in .nspawn files\n\nThese corresponding options have many possible values which are described in systemd-nspawn(1) § Integration Options. The default value is auto, which means that:\n\n- If --private-network is enabled, the /etc/resolv.conf is left as it is in the container.\n- Otherwise, if systemd-resolved is running on the host, its stub resolv.conf file is copied or bind-mounted into the container.\n- Otherwise, the /etc/resolv.conf file is copied or bind-mounted from the host to the container.\n\nIn the last two cases, the file is copied, if the container root is writeable, and bind-mounted if it is read-only.\n\nFor the second case where systemd-resolved runs on the host, systemd-nspawn expects it to also run in the container, so that the container can use the stub symlink file /etc/resolv.conf from the host. If not, the default value auto no longer works, and you should replace the symlink by using one of the replace-* options.\n\n"
    },
    {
      "title": "Running non-shell/init commands",
      "level": 3,
      "content": "From systemd-nspawn(1) § Execution Options:\n\n"
    },
    {
      "title": "Unprivileged containers",
      "level": 3,
      "content": "systemd-nspawn supports unprivileged containers, though the containers need to be booted as root.\n\nNote: **This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.** This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.\n\nThis article or section needs language, wiki syntax or style improvements. See Help:Style for reference.\n\nThe easiest way to do this is to let systemd-nspawn automatically choose an unused range of UIDs/GIDs by using the -U option:\n\n```\n# systemd-nspawn -bUD ~/MyContainer\n```\n\nIf kernel supports user namespaces, the -U option is equivalent to --private-users=pick --private-users-ownership=auto. See systemd-nspawn(1) § User Namespacing Options for details.\n\nIf a container has been started with a private UID/GID range using the --private-users-ownership=chown option (or on a filesystem where -U requires --private-users-ownership=chown), you need to keep using it that way to avoid permission errors. Alternatively, it is possible to undo the effect of --private-users-ownership=chown on the container's file system by specifying a range of IDs starting at 0:\n\n```\n# systemd-nspawn -D ~/MyContainer --private-users=0 --private-users-ownership=chown\n```\n\n"
    },
    {
      "title": "Use an X environment",
      "level": 3,
      "content": "Note: **The factual accuracy of this article or section is disputed.** The factual accuracy of this article or section is disputed.\n\nThe factual accuracy of this article or section is disputed.\n\nSee Xhost and Change root#Run graphical applications from chroot.\n\nYou will need to set the DISPLAY environment variable inside your container session to connect to the external X server.\n\nX stores some required files in the /tmp directory. In order for your container to display anything, it needs access to those files. To do so, append the --bind-ro=/tmp/.X11-unix option when starting the container.\n\n"
    },
    {
      "title": "Avoiding xhost",
      "level": 4,
      "content": "xhost only provides rather coarse access rights to the X server. More fine-grained access control is possible via the $XAUTHORITY file. Unfortunately, just making the $XAUTHORITY file accessible in the container will not do the job: your $XAUTHORITY file is specific to your host, but the container is a different host. The following trick adapted from stackoverflow can be used to make your X server accept the $XAUTHORITY file from an X application run inside the container:\n\n```\n$ XAUTH=/tmp/container_xauth\n$ xauth nextract - \"$DISPLAY\" | sed -e 's/^..../ffff/' | xauth -f \"$XAUTH\" nmerge -\n# systemd-nspawn -D myContainer --bind=/tmp/.X11-unix --bind=\"$XAUTH\" -E DISPLAY=\"$DISPLAY\" -E XAUTHORITY=\"$XAUTH\" --as-pid2 /usr/bin/xeyes\n```\n\nThe second line above sets the connection family to \"FamilyWild\", value 65535, which causes the entry to match every display. See Xsecurity(7) for more information.\n\n"
    },
    {
      "title": "Using X nesting/Xephyr",
      "level": 4,
      "content": "Another simple way to run X applications and avoid the risks of a shared X desktop is using X nesting. The advantages here are avoiding interaction between in-container applications and non-container applications entirely and being able to run a different desktop environment or window manager. The downsides are less performance, and the lack of hardware acceleration when using Xephyr.\n\nStart Xephyr outside of the container using:\n\n```\n# Xephyr :1 -resizeable\n```\n\nThen start the container with the following options:\n\n```\n--setenv=DISPLAY=:1 --bind-ro=/tmp/.X11-unix/X1\n```\n\nNo other binds are necessary.\n\nYou might still need to manually set DISPLAY=:1 in the container under some circumstances (mostly if used with -b).\n\n"
    },
    {
      "title": "Run Firefox",
      "level": 4,
      "content": "```\n# systemd-nspawn --setenv=DISPLAY=:0 \\\n              --setenv=XAUTHORITY=~/.Xauthority \\\n              --bind-ro=$HOME/.Xauthority:/root/.Xauthority \\\n              --bind=/tmp/.X11-unix \\\n              -D ~/containers/firefox \\\n              --as-pid2 \\\n              firefox\n```\n\nAlternatively you can boot the container and let e.g. systemd-networkd set up the virtual network interface:\n\n```\n# systemd-nspawn --bind-ro=$HOME/.Xauthority:/root/.Xauthority \\\n              --bind=/tmp/.X11-unix \\\n              -D ~/containers/firefox \\\n              --network-veth -b\n```\n\nOnce your container is booted, run the Xorg binary like so:\n\n```\n# systemd-run -M firefox --setenv=DISPLAY=:0 firefox\n```\n\n"
    },
    {
      "title": "3D graphics acceleration",
      "level": 4,
      "content": "Note: **This article or section needs expansion.** This article or section needs expansion.\n\nThis article or section needs expansion.\n\nTo enable accelerated 3D graphics, it may be necessary to bind mount /dev/dri to the container by adding the following line to the .nspawn file:\n\n```\nBind=/dev/dri\n```\n\nThe above trick was adopted from patrickskiba.com. This notably solves the problem of\n\n```\nlibGL error: MESA-LOADER: failed to retrieve device information\nlibGL error: Version 4 or later of flush extension not found\nlibGL error: failed to load driver: i915\n```\n\nYou can confirm that it has been enabled by running glxinfo or glxgears.\n\nIf you cannot install the same NVIDIA driver version on the container as on the host, you may need to also bind the driver library files. You can run pacman -Ql nvidia-utils on the host to see all the files it contains. You do not need to copy everything over. The following systemd override file will bind all the necessary files over when the container is run via machinectl start container-name.\n\nNote: **The factual accuracy of this article or section is disputed.** The factual accuracy of this article or section is disputed.\n\nThe factual accuracy of this article or section is disputed.\n\n```\n/etc/systemd/system/systemd-nspawn@.service.d/nvidia-gpu.conf\n```\n\n```\n[Service]\nExecStart=\nExecStart=systemd-nspawn --quiet --keep-unit --boot --link-journal=try-guest --machine=%i \\\n--bind=/dev/dri \\\n--bind=/dev/shm \\\n--bind=/dev/nvidia0 \\\n--bind=/dev/nvidiactl \\\n--bind=/dev/nvidia-modeset \\\n--bind=/usr/bin/nvidia-bug-report.sh:/usr/bin/nvidia-bug-report.sh \\\n--bind=/usr/bin/nvidia-cuda-mps-control:/usr/bin/nvidia-cuda-mps-control \\\n--bind=/usr/bin/nvidia-cuda-mps-server:/usr/bin/nvidia-cuda-mps-server \\\n--bind=/usr/bin/nvidia-debugdump:/usr/bin/nvidia-debugdump \\\n--bind=/usr/bin/nvidia-modprobe:/usr/bin/nvidia-modprobe \\\n--bind=/usr/bin/nvidia-ngx-updater:/usr/bin/nvidia-ngx-updater \\\n--bind=/usr/bin/nvidia-persistenced:/usr/bin/nvidia-persistenced \\\n--bind=/usr/bin/nvidia-powerd:/usr/bin/nvidia-powerd \\\n--bind=/usr/bin/nvidia-sleep.sh:/usr/bin/nvidia-sleep.sh \\\n--bind=/usr/bin/nvidia-smi:/usr/bin/nvidia-smi \\\n--bind=/usr/bin/nvidia-xconfig:/usr/bin/nvidia-xconfig \\\n--bind=/usr/lib/gbm/nvidia-drm_gbm.so:/usr/lib/x86_64-linux-gnu/gbm/nvidia-drm_gbm.so \\\n--bind=/usr/lib/libEGL_nvidia.so:/usr/lib/x86_64-linux-gnu/libEGL_nvidia.so \\\n--bind=/usr/lib/libGLESv1_CM_nvidia.so:/usr/lib/x86_64-linux-gnu/libGLESv1_CM_nvidia.so \\\n--bind=/usr/lib/libGLESv2_nvidia.so:/usr/lib/x86_64-linux-gnu/libGLESv2_nvidia.so \\\n--bind=/usr/lib/libGLX_nvidia.so:/usr/lib/x86_64-linux-gnu/libGLX_nvidia.so \\\n--bind=/usr/lib/libcuda.so:/usr/lib/x86_64-linux-gnu/libcuda.so \\\n--bind=/usr/lib/libnvcuvid.so:/usr/lib/x86_64-linux-gnu/libnvcuvid.so \\\n--bind=/usr/lib/libnvidia-allocator.so:/usr/lib/x86_64-linux-gnu/libnvidia-allocator.so \\\n--bind=/usr/lib/libnvidia-cfg.so:/usr/lib/x86_64-linux-gnu/libnvidia-cfg.so \\\n--bind=/usr/lib/libnvidia-egl-gbm.so:/usr/lib/x86_64-linux-gnu/libnvidia-egl-gbm.so \\\n--bind=/usr/lib/libnvidia-eglcore.so:/usr/lib/x86_64-linux-gnu/libnvidia-eglcore.so \\\n--bind=/usr/lib/libnvidia-encode.so:/usr/lib/x86_64-linux-gnu/libnvidia-encode.so \\\n--bind=/usr/lib/libnvidia-fbc.so:/usr/lib/x86_64-linux-gnu/libnvidia-fbc.so \\\n--bind=/usr/lib/libnvidia-glcore.so:/usr/lib/x86_64-linux-gnu/libnvidia-glcore.so \\\n--bind=/usr/lib/libnvidia-glsi.so:/usr/lib/x86_64-linux-gnu/libnvidia-glsi.so \\\n--bind=/usr/lib/libnvidia-glvkspirv.so:/usr/lib/x86_64-linux-gnu/libnvidia-glvkspirv.so \\\n--bind=/usr/lib/libnvidia-ml.so:/usr/lib/x86_64-linux-gnu/libnvidia-ml.so \\\n--bind=/usr/lib/libnvidia-ngx.so:/usr/lib/x86_64-linux-gnu/libnvidia-ngx.so \\\n--bind=/usr/lib/libnvidia-opticalflow.so:/usr/lib/x86_64-linux-gnu/libnvidia-opticalflow.so \\\n--bind=/usr/lib/libnvidia-ptxjitcompiler.so:/usr/lib/x86_64-linux-gnu/libnvidia-ptxjitcompiler.so \\\n--bind=/usr/lib/libnvidia-rtcore.so:/usr/lib/x86_64-linux-gnu/libnvidia-rtcore.so \\\n--bind=/usr/lib/libnvidia-tls.so:/usr/lib/x86_64-linux-gnu/libnvidia-tls.so \\\n--bind=/usr/lib/libnvidia-vulkan-producer.so:/usr/lib/x86_64-linux-gnu/libnvidia-vulkan-producer.so \\\n--bind=/usr/lib/libnvoptix.so:/usr/lib/x86_64-linux-gnu/libnvoptix.so \\\n--bind=/usr/lib/modprobe.d/nvidia-utils.conf:/usr/lib/x86_64-linux-gnu/modprobe.d/nvidia-utils.conf \\\n--bind=/usr/lib/nvidia/wine/_nvngx.dll:/usr/lib/x86_64-linux-gnu/nvidia/wine/_nvngx.dll \\\n--bind=/usr/lib/nvidia/wine/nvngx.dll:/usr/lib/x86_64-linux-gnu/nvidia/wine/nvngx.dll \\\n--bind=/usr/lib/nvidia/xorg/libglxserver_nvidia.so:/usr/lib/x86_64-linux-gnu/nvidia/xorg/libglxserver_nvidia.so \\\n--bind=/usr/lib/vdpau/libvdpau_nvidia.so:/usr/lib/x86_64-linux-gnu/vdpau/libvdpau_nvidia.so \\\n--bind=/usr/lib/xorg/modules/drivers/nvidia_drv.so:/usr/lib/x86_64-linux-gnu/xorg/modules/drivers/nvidia_drv.so \\\n--bind=/usr/share/X11/xorg.conf.d/10-nvidia-drm-outputclass.conf:/usr/share/X11/xorg.conf.d/10-nvidia-drm-outputclass.conf \\\n--bind=/usr/share/dbus-1/system.d/nvidia-dbus.conf:/usr/share/dbus-1/system.d/nvidia-dbus.conf \\\n--bind=/usr/share/egl/egl_external_platform.d/15_nvidia_gbm.json:/usr/share/egl/egl_external_platform.d/15_nvidia_gbm.json \\\n--bind=/usr/share/glvnd/egl_vendor.d/10_nvidia.json:/usr/share/glvnd/egl_vendor.d/10_nvidia.json \\\n--bind=/usr/share/licenses/nvidia-utils/LICENSE:/usr/share/licenses/nvidia-utils/LICENSE \\\n--bind=/usr/share/vulkan/icd.d/nvidia_icd.json:/usr/share/vulkan/icd.d/nvidia_icd.json \\\n--bind=/usr/share/vulkan/implicit_layer.d/nvidia_layers.json:/usr/share/vulkan/implicit_layer.d/nvidia_layers.json \\\nDeviceAllow=/dev/dri rw\nDeviceAllow=/dev/shm rw\nDeviceAllow=/dev/nvidia0 rw\nDeviceAllow=/dev/nvidiactl rw\nDeviceAllow=/dev/nvidia-modeset rw\n```\n\n"
    },
    {
      "title": "Access host filesystem",
      "level": 3,
      "content": "See --bind and --bind-ro in systemd-nspawn(1).\n\nIf both the host and the container are Arch Linux, then one could, for example, share the pacman cache:\n\n```\n# systemd-nspawn --bind=/var/cache/pacman/pkg\n```\n\nOr you can specify per-container bind using the file:\n\n```\n/etc/systemd/nspawn/my-container.nspawn\n```\n\n```\n[Files]\nBind=/var/cache/pacman/pkg\n```\n\nSee #Per-container settings.\n\nTo bind the directory to a different path within the container, add the path be separated by a colon. For example:\n\n```\n# systemd-nspawn --bind=/path/to/host_dir:/path/to/container_dir\n```\n\nIn case of #Unprivileged containers, the resulting mount points will be owned by the nobody user. This can be modified with the idmap mount option:\n\n```\n# systemd-nspawn --bind=/path/to/host_dir:/path/to/container_dir:idmap\n```\n\n"
    },
    {
      "title": "Run on a non-systemd system",
      "level": 3,
      "content": "See Init#systemd-nspawn.\n\n"
    },
    {
      "title": "Use Btrfs subvolume as container root",
      "level": 3,
      "content": "To use a Btrfs subvolume as a template for the container's root, use the --template flag. This takes a snapshot of the subvolume and populates the root directory for the container with it.\n\nNote: **entire** \n\nFor example, to use a snapshot located at /.snapshots/403/snapshot:\n\n```\n# systemd-nspawn --template=/.snapshots/403/snapshot -b -D my-container\n```\n\nwhere my-container is the name of the directory that will be created for the container. After powering off, the newly created subvolume is retained.\n\n"
    },
    {
      "title": "Use temporary Btrfs snapshot of container",
      "level": 3,
      "content": "One can use the --ephemeral or -x flag to create a temporary btrfs snapshot of the container and use it as the container root. Any changes made while booted in the container will be lost. For example:\n\n```\n# systemd-nspawn -D my-container -xb\n```\n\nwhere my-container is the directory of an existing container or system. For example, if / is a btrfs subvolume one could create an ephemeral container of the currently running host system by doing:\n\n```\n# systemd-nspawn -D / -xb\n```\n\nAfter powering off the container, the btrfs subvolume that was created is immediately removed.\n\n"
    },
    {
      "title": "Run docker in systemd-nspawn",
      "level": 3,
      "content": "Since Docker 20.10, it is possible to run Docker containers inside an unprivileged systemd-nspawn container with cgroups v2 enabled (default in Arch Linux) without undermining security measures by disabling cgroups and user namespaces. To do so, edit /etc/systemd/nspawn/myContainer.nspawn (create if absent) and add the following configurations.\n\n```\n/etc/systemd/nspawn/myContainer.nspawn\n```\n\n```\n[Exec]\nSystemCallFilter=@keyring bpf\n```\n\nThen, Docker should work as-is inside the container.\n\nNote: **The factual accuracy of this article or section is disputed.** The factual accuracy of this article or section is disputed.\n\nThe factual accuracy of this article or section is disputed.\n\nWith recent versions of systemd, you would also need to need the following workaround:\n\n```\n/etc/systemd/nspawn/myContainer.nspawn\n```\n\n```\n[Files]\nBind=/proc:/run/proc\nBind=/sys:/run/sys\n```\n\nSee [19] for more details.\n\nSince overlayfs does not work with user namespaces and is unavailable inside systemd-nspawn, by default, Docker falls back to using the inefficient vfs as its storage driver, which creates a copy of the image each time a container is started. This can be worked around by using fuse-overlayfs as its storage driver. To do so, we need to first expose fuse to the container:\n\n```\n/etc/systemd/nspawn/myContainer.nspawn\n```\n\n```\n[Files]\nBind=/dev/fuse\n```\n\nand then allow the container to read and write the device node:\n\n```\n# systemctl set-property systemd-nspawn@myContainer DeviceAllow='/dev/fuse rwm'\n```\n\nFinally, install the package fuse-overlayfs inside the container. You need to restart the container for all the configuration to take effect.\n\n"
    },
    {
      "title": "Map a local user and bind-mount their home directory into a container",
      "level": 3,
      "content": "Create a container in /var/lib/machines/MyContainer/ as explained in #Create and boot a minimal Arch Linux container.\n\nCreate a configuration file with BindUser= to map the selected local user name into the container. Note that this requires PrivateUsers=, see systemd-nspawn(1) for details. Files created in the bind-mounted home directory both inside and outside the container will have the same UID and GID.\n\n```\n/etc/systemd/nspawn/MyContainer.nspawn\n```\n\n```\n[Exec]\nUser=username\nPrivateUsers=pick\n\n[Files]\nBindUser=username\n```\n\nHaving also User= in the configuration specifies the default user used for running commands inside the container, such as the interactive shell:\n\n```\n# systemd-nspawn -M MyContainer bash\n```\n\nThis is helpful for testing Arch Linux packages from another Linux distribution.\n\n"
    },
    {
      "title": "execv(...) failed: Permission denied",
      "level": 3,
      "content": "When trying to boot the container via systemd-nspawn -bD /path/to/container (or executing something in the container), and the following error comes up:\n\n```\nexecv(/usr/lib/systemd/systemd, /lib/systemd/systemd, /sbin/init) failed: Permission denied\n```\n\neven though the permissions of the files in question (i.e. /lib/systemd/systemd) are correct, this can be the result of having mounted the file system on which the container is stored as non-root user. For example, if you mount your disk manually with an entry in fstab that has the options noauto,user,..., systemd-nspawn will not allow executing the files even if they are owned by root.\n\n"
    },
    {
      "title": "Terminal type in TERM is incorrect (broken colors)",
      "level": 3,
      "content": "When logging into the container via machinectl login, the colors and keystrokes in the terminal within the container might be broken. This may be due to an incorrect terminal type in TERM environment variable. The environment variable is not inherited from the shell on the host, but falls back to a default fixed in systemd (vt220), unless explicitly configured. To configure, within the container create a configuration overlay for the container-getty@.service systemd service that launches the login getty for machinectl login, and set TERM to the value that matches the host terminal you are logging in from:\n\n```\n/etc/systemd/system/container-getty@.service.d/term.conf\n```\n\n```\n[Service]\nEnvironment=TERM=xterm-256color\n```\n\nAlternatively use machinectl shell. It properly inherits the TERM environment variable from the terminal.\n\n"
    },
    {
      "title": "Mounting a NFS share inside the container",
      "level": 3,
      "content": "Note: **This article or section needs expansion.** This article or section needs expansion.\n\nThis article or section needs expansion.\n\nNot possible at this time (June 2019).\n\n"
    },
    {
      "title": "See also",
      "level": 2,
      "content": "- Automatic console login\n- Creating containers with systemd-nspawn\n- Presentation by Lennart Poettering on systemd-nspawn\n- Running Firefox in a systemd-nspawn container\n- Graphical applications in systemd-nspawn\n\n"
    }
  ]
}