{
  "title": "Hadoop",
  "url": "https://wiki.archlinux.org/title/Hadoop",
  "sections": [
    {
      "title": "Introduction",
      "level": 1,
      "content": "Related articles\n\n- Apache Spark\n\nApache Hadoop is a framework for running applications on large cluster built of commodity hardware. The Hadoop framework transparently provides applications both reliability and data motion. Hadoop implements a computational paradigm named Map/Reduce, where the application is divided into many small fragments of work, each of which may be executed or re-executed on any node in the cluster. In addition, it provides a distributed file system (HDFS) that stores data on the compute nodes, providing very high aggregate bandwidth across the cluster. Both MapReduce and the Hadoop Distributed File System are designed so that node failures are automatically handled by the framework.\n\n"
    },
    {
      "title": "Installation",
      "level": 2,
      "content": "Install the hadoopAUR package.\n\n"
    },
    {
      "title": "Configuration",
      "level": 2,
      "content": "By default, hadoop is already configured for pseudo-distributed operation. Some environment variables are set in /etc/profile.d/hadoop.sh with different values than traditional hadoop.\n\nTable content:\nEnvironment variable | Value | Description | Permission\nHADOOP_CONF_DIR | /etc/hadoop | Where configuration files are stored. | Read\nHADOOP_LOG_DIR | /tmp/hadoop/log | Where log files are stored. | Read and Write\nHADOOP_WORKERS | /etc/hadoop/workers | File naming remote worker hosts. | Read\nHADOOP_PID_DIR | /tmp/hadoop/run | Where pid files are stored. | Read and Write\n\nYou also should set up the following files correctly.\n\n```\n/etc/hosts\n/etc/hostname \n/etc/locale.conf\n```\n\nYou need to tell hadoop your JAVA_HOME in /etc/hadoop/hadoop-env.sh because it does not assume the location where it is installed to in Arch Linux by itself:\n\n```\n/etc/hadoop/hadoop-env.sh\n```\n\n```\nexport JAVA_HOME=/usr/lib/jvm/default\n```\n\nCheck the installation with:\n\n```\n$ hadoop version\n```\n\nThe HADOOP_WORKERS option was previously called HADOOP_SLAVES. If you get warning message \"WARNING: HADOOP_SLAVES has been replaced by HADOOP_WORKERS. Using value of HADOOP_SLAVES.\" Then replace export HADOOP_SLAVES=/etc/hadoop/slaves in /etc/profile.d/hadoop.sh with:\n\n```\nexport HADOOP_WORKERS=/etc/hadoop/workers\n```\n\n"
    },
    {
      "title": "Standalone Operation",
      "level": 3,
      "content": "By default, Hadoop is configured to run in a non-distributed mode, as a single Java process. This is useful for debugging.\n\nThe following example copies the unpacked conf directory to use as input and then finds and displays every match of the given regular expression. Output is written to the given output directory.\n\n```\n$ HADOOP_CONF_DIR=/usr/lib/hadoop/orig_etc/hadoop/\n$ mkdir input\n$ cp /etc/hadoop/*.xml input\n$ hadoop jar /usr/lib/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar grep input output 'dfs[a-z.]+'\n$ cat output/*\n```\n\n"
    },
    {
      "title": "Pseudo-Distributed Operation",
      "level": 3,
      "content": "Hadoop can also be run on a single-node in a pseudo-distributed mode where each Hadoop daemon runs in a separate Java process.\n\nBy default, Hadoop will run as the user root. You can change the user in /etc/conf.d/hadoop:\n\n```\nHADOOP_USERNAME=\"<your user name>\"\n```\n\n"
    },
    {
      "title": "Set up passphraseless ssh",
      "level": 4,
      "content": "Make sure you have sshd enabled. Now check that you can connect to localhost without a passphrase:\n\n```\n$ ssh localhost\n```\n\nIf you cannot ssh to localhost without a passphrase, execute the following commands:\n\n```\n$ ssh-keygen -t rsa -P \"\" -f ~/.ssh/id_rsa\n$ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n$ chmod 0600 ~/.ssh/authorized_keys\n```\n\nAlso make sure this line is commented in /etc/ssh/sshd_config\n\n```\n/etc/ssh/sshd_config\n```\n\n```\n#AuthorizedKeysFile .ssh/authorized_keys\n```\n\n"
    },
    {
      "title": "Execution",
      "level": 4,
      "content": "Format a new distributed-filesystem:\n\n```\n$ hadoop namenode -format\n```\n\nEdit /etc/hadoop/core-site.xml and add below configuration:\n\n```\n<configuration>\n   <property>\n     <name>fs.defaultFS</name>\n     <value>hdfs://localhost:9000</value>\n   </property>\n </configuration>\n```\n\nStart the hadoop namenode:\n\n```\n$ hadoop namenode\n```\n\nYou may access the web GUI via http://localhost:9870/\n\nNote: **The factual accuracy of this article or section is disputed.** The factual accuracy of this article or section is disputed.\n\nThe factual accuracy of this article or section is disputed.\n\nStart the following hadoop systemd units: hadoop-datanode, hadoop-jobtracker, hadoop-namenode, hadoop-secondarynamenode, hadoop-tasktracker.\n\nThe hadoop daemon log output is written to the ${HADOOP_LOG_DIR} directory (defaults to /var/log/hadoop).\n\nBrowse the web interface for the NameNode and the JobTracker; by default they are available at:\n\n- NameNode - http://localhost:50070/\n- JobTracker - http://localhost:50030/\n\nCopy the input files into the distributed filesystem:\n\n```\n$ hadoop fs -put /etc/hadoop input\n```\n\nRun some of the examples provided:\n\n```\n$ hadoop jar /usr/lib/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar grep input output 'dfs[a-z.]+'\n```\n\nExamine the output files:\n\nCopy the output files from the distributed filesystem to the local filesystem and examine them:\n\n```\n$ hadoop fs -get output output\n$ cat output/*\n```\n\nor\n\nView the output files on the distributed filesystem:\n\n```\n$ hadoop fs -cat output/*\n```\n\nWhen you are done, stop the daemons hadoop-datanode, hadoop-jobtracker, hadoop-namenode, hadoop-secondarynamenode, hadoop-tasktracker.\n\n"
    }
  ]
}