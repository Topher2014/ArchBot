{
  "title": "NVIDIA/Tips and tricks",
  "url": "https://wiki.archlinux.org/title/NVIDIA/Tips_and_tricks",
  "sections": [
    {
      "title": "Fixing terminal resolution",
      "level": 2,
      "content": "Transitioning from nouveau may cause your startup terminal to display at a lower resolution.\n\nFor GRUB, see GRUB/Tips and tricks#Setting the framebuffer resolution for details. [1] [2]\n\nFor systemd-boot, set console-mode in esp/loader/loader.conf. See systemd-boot#Loader configuration for details.\n\nFor rEFInd, set use_graphics_for +,linux in esp/EFI/refind/refind.conf.[3] A small caveat is that this will hide the kernel parameters from being shown during boot.\n\n"
    },
    {
      "title": "Using TV-out",
      "level": 2,
      "content": "See Wikibooks:NVIDIA/TV-OUT.\n\n"
    },
    {
      "title": "X with a TV (DFP) as the only display",
      "level": 2,
      "content": "The X server falls back to CRT-0 if no monitor is automatically detected. This can be a problem when using a DVI connected TV as the main display, and X is started while the TV is turned off or otherwise disconnected.\n\nTo force NVIDIA to use DFP, store a copy of the EDID somewhere in the filesystem so that X can parse the file instead of reading EDID from the TV/DFP.\n\nTo acquire the EDID, start nvidia-settings. It will show some information in tree format, ignore the rest of the settings for now and select the GPU (the corresponding entry should be titled \"GPU-0\" or similar), click the DFP section (again, DFP-0 or similar), click on the Acquire Edid Button and store it somewhere, for example, /etc/X11/dfp0.edid.\n\nIf in the front-end mouse and keyboard are not attached, the EDID can be acquired using only the command line. Run an X server with enough verbosity to print out the EDID block:\n\n```\n$ startx -- -logverbose 6\n```\n\nAfter the X Server has finished initializing, close it and your log file will probably be in /var/log/Xorg.0.log. Extract the EDID block using nvidia-xconfig:\n\n```\n$ nvidia-xconfig --extract-edids-from-file=/var/log/Xorg.0.log --extract-edids-output-file=/etc/X11/dfp0.bin\n```\n\nEdit xorg.conf by adding to the Device section:\n\n```\nOption \"ConnectedMonitor\" \"DFP\"\nOption \"CustomEDID\" \"DFP-0:/etc/X11/dfp0.bin\"\n```\n\nThe ConnectedMonitor option forces the driver to recognize the DFP as if it were connected. The CustomEDID provides EDID data for the device, meaning that it will start up just as if the TV/DFP was connected during the X process.\n\nThis way, one can automatically start a display manager at boot time and still have a working and properly configured X screen by the time the TV gets powered on.\n\nIf the above changes did not work, in the xorg.conf under Device section you can try to remove the Option \"ConnectedMonitor\" \"DFP\" and add the following lines:\n\n```\nOption \"ModeValidation\" \"NoDFPNativeResolutionCheck\"\nOption \"ConnectedMonitor\" \"DFP-0\"\n```\n\nThe NoDFPNativeResolutionCheck prevents NVIDIA driver from disabling all the modes that do not fit in the native resolution.\n\n"
    },
    {
      "title": "Headless (no monitor) resolution",
      "level": 2,
      "content": "In headless mode, resolution falls back to 640x480, which is used by VNC or Steam Link. To start in a higher resolution e.g. 1920x1080, specify a Virtual entry under the Screen subsection in xorg.conf:\n\n```\nSection \"Screen\"\n   [...]\n   SubSection     \"Display\"\n       Depth       24\n       Virtual     1920 1080\n   EndSubSection\nEndSection\n```\n\n"
    },
    {
      "title": "Check the power source",
      "level": 2,
      "content": "The NVIDIA X.org driver can also be used to detect the GPU's current source of power. To see the current power source, check the 'GPUPowerSource' read-only parameter (0 - AC, 1 - battery):\n\n```\n$ nvidia-settings -q GPUPowerSource -t\n```\n\n```\n1\n```\n\n"
    },
    {
      "title": "Listening to ACPI events",
      "level": 2,
      "content": "NVIDIA drivers automatically try to connect to the acpid daemon and listen to ACPI events such as battery power, docking, some hotkeys, etc. If connection fails, X.org will output the following warning:\n\n```\n~/.local/share/xorg/Xorg.0.log\n```\n\n```\nNVIDIA(0): ACPI: failed to connect to the ACPI event daemon; the daemon\nNVIDIA(0):     may not be running or the \"AcpidSocketPath\" X\nNVIDIA(0):     configuration option may not be set correctly.  When the\nNVIDIA(0):     ACPI event daemon is available, the NVIDIA X driver will\nNVIDIA(0):     try to use it to receive ACPI event notifications.  For\nNVIDIA(0):     details, please see the \"ConnectToAcpid\" and\nNVIDIA(0):     \"AcpidSocketPath\" X configuration options in Appendix B: X\nNVIDIA(0):     Config Options in the README.\n```\n\nWhile completely harmless, you may get rid of this message by disabling the ConnectToAcpid option in your /etc/X11/xorg.conf.d/20-nvidia.conf:\n\n```\nSection \"Device\"\n  ...\n  Driver \"nvidia\"\n  Option \"ConnectToAcpid\" \"0\"\n  ...\nEndSection\n```\n\nIf you are on laptop, it might be a good idea to install and enable the acpid daemon instead.\n\n"
    },
    {
      "title": "Displaying GPU temperature in the shell",
      "level": 2,
      "content": "There are three methods to query the GPU temperature. nvidia-settings requires that you are using X, nvidia-smi or nvclock do not. Also note that nvclock currently does not work with newer NVIDIA cards such as GeForce 200 series cards as well as embedded GPUs such as the Zotac IONITX's 8800GS.\n\n"
    },
    {
      "title": "nvidia-settings",
      "level": 3,
      "content": "To display the GPU temp in the shell, use nvidia-settings as follows:\n\n```\n$ nvidia-settings -q gpucoretemp\n```\n\n```\nAttribute 'GPUCoreTemp' (hostname:0[gpu:0]): 49.\n    'GPUCoreTemp' is an integer attribute.\n    'GPUCoreTemp' is a read-only attribute.\n    'GPUCoreTemp' can use the following target types: GPU.\n```\n\nThe GPU temps of this board is 49 °C.\n\nIn order to get just the temperature for use in utilities such as rrdtool or conky:\n\n```\n$ nvidia-settings -q gpucoretemp -t\n```\n\n```\n49\n```\n\n"
    },
    {
      "title": "nvidia-smi",
      "level": 3,
      "content": "Use nvidia-smi which can read temps directly from the GPU without the need to use X at all, e.g. when running Wayland or on a headless server.\n\nTo display the GPU temperature in the shell, use nvidia-smi:\n\n```\n$ nvidia-smi\n```\n\n```\nWed Feb 28 14:27:35 2024\n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.54.14              Driver Version: 550.54.14      CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA GeForce GTX 1660 Ti     Off |   00000000:01:00.0  On |                  N/A |\n|  0%   49C    P8              9W /  120W |     138MiB /   6144MiB |      2%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n\n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|    0   N/A  N/A    223179      G   weston                                        120MiB |\n+-----------------------------------------------------------------------------------------+\n```\n\nOnly for temperature:\n\n```\n$ nvidia-smi -q -d TEMPERATURE\n```\n\n```\n==============NVSMI LOG==============\n\nTimestamp                                 : Wed Feb 28 14:27:35 2024\nDriver Version                            : 550.54.14\nCUDA Version                              : 12.4\n\nAttached GPUs                             : 1\nGPU 00000000:01:00.0\n    Temperature\n        GPU Current Temp                  : 49 C\n        GPU T.Limit Temp                  : N/A\n        GPU Shutdown Temp                 : 95 C\n        GPU Slowdown Temp                 : 92 C\n        GPU Max Operating Temp            : 90 C\n        GPU Target Temperature            : 83 C\n        Memory Current Temp               : N/A\n        Memory Max Operating Temp         : N/A\n```\n\nIn order to get just the temperature for use in utilities such as rrdtool or conky:\n\n```\n$ nvidia-smi --query-gpu=temperature.gpu --format=csv,noheader,nounits\n```\n\n```\n49\n```\n\n"
    },
    {
      "title": "nvclock",
      "level": 3,
      "content": "Install the nvclockAUR package.\n\nThere can be significant differences between the temperatures reported by nvclock and nvidia-settings/nv-control. According to this post by the author (thunderbird) of nvclock, the nvclock values should be more accurate.\n\n"
    },
    {
      "title": "Enabling overclocking in nvidia-settings",
      "level": 3,
      "content": "- Overclocking settings cannot be applied if the Xorg server is running in rootless mode. Consider running Xorg as root.\n- Enabling DRM kernel mode setting may cause overclocking to become unavailable, regardless of the Coolbits value.\n\nOverclocking is controlled via Coolbits option in the Device section, which enables various unsupported features:\n\n```\nOption \"Coolbits\" \"value\"\n```\n\n```\n# nvidia-xconfig --cool-bits=value\n```\n\nThe Coolbits value is the sum of its component bits in the binary numeral system. The component bits are:\n\n- 1 (bit 0) - Enables overclocking of older (pre-Fermi) cores on the Clock Frequencies page in nvidia-settings.\n- 2 (bit 1) - When this bit is set, the driver will \"attempt to initialize SLI when using GPUs with different amounts of video memory\".\n- 4 (bit 2) - Enables manual configuration of GPU fan speed on the Thermal Monitor page in nvidia-settings.\n- 8 (bit 3) - Enables overclocking on the PowerMizer page in nvidia-settings. Available since version 337.12 for the Fermi architecture and newer.[4]\n- 16 (bit 4) - Enables overvoltage using nvidia-settings CLI options. Available since version 346.16 for the Fermi architecture and newer.[5]\n\nTo enable multiple features, add the Coolbits values together. For example, to enable overclocking and overvoltage of Fermi cores, set Option \"Coolbits\" \"24\".\n\nThe documentation of Coolbits can be found in /usr/share/doc/nvidia/html/xconfigoptions.html and here.\n\n"
    },
    {
      "title": "Setting static 2D/3D clocks",
      "level": 3,
      "content": "Use kernel module parameters to enable PowerMizer at its maximum performance level (VSync will not work without this):\n\n```\n/etc/modprobe.d/nvidia.conf\n```\n\n```\noptions nvidia NVreg_RegistryDwords=\"PerfLevelSrc=0x2222\"\n```\n\n"
    },
    {
      "title": "Lowering GPU Boost Clocks",
      "level": 3,
      "content": "With Ampere (NV170/GAXXX)[dead link 2025-04-06 ⓘ] GPUs and later, clock boost works in a different way, and maximum clocks are set to the highest supported limit at boot. If that is what you want, then no further configuration is necessary.\n\nThe drawback is the lower power efficiency. As the clocks go up, increased voltage is needed for stability, resulting in a nonlinear increase in power consumption, heating, and fan noise. Lowering the boost clock limit will thus increase efficiency.\n\nBoost clock limits can be changed using nvidia-smi, running as root:\n\n- List supported clock rates: $ nvidia-smi -q -d SUPPORTED_CLOCKS\n- Set GPU boost clock limit to 1695 MHz: # nvidia-smi --lock-gpu-clocks=0,1695 --mode=1\n- Set Memory boost clock limit to 5001 MHz: # nvidia-smi --lock-memory-clocks=0,5001\n\n```\n$ nvidia-smi -q -d SUPPORTED_CLOCKS\n```\n\n```\n# nvidia-smi --lock-gpu-clocks=0,1695 --mode=1\n```\n\n```\n# nvidia-smi --lock-memory-clocks=0,5001\n```\n\nTo optimize for efficiency, use nvidia-smi to check the GPU utilization while running your favorite game. VSync should be on. Lowering the boost clock limit will increase GPU utilization, because a slower GPU will use more time to render each frame. Best efficiency is achieved with the lowest clocks that do not cause the stutter that results when the utilization hits 100%. Then, each frame can be rendered just quickly enough to keep up with the refresh rate.\n\nAs an example, using the above settings instead of default on an RTX 3090 Ti, while playing Hitman 3 at 4K@60, reduces power consumption by 30%, temperature from 75 to 63 degrees, and fan speed from 73% to 57%.\n\n"
    },
    {
      "title": "Saving overclocking settings",
      "level": 3,
      "content": "Typically, clock and voltage offsets inserted in the nvidia-settings interface are not saved, being lost after a reboot. Fortunately, there are tools that offer an interface for overclocking under the proprietary driver, able to save the user's overclocking preferences and automatically applying them on boot. Some of them are:\n\n- gweAUR - graphical, applies settings on desktop session start\n- nvclockAUR and systemd-nvclock-unitAUR - graphical, applies settings on system boot\n- nvocAUR - text based, profiles are configuration files in /etc/nvoc.d/, applies settings on desktop session start\n\nOtherwise, GPUGraphicsClockOffset and GPUMemoryTransferRateOffset attributes can be set in the command-line interface of nvidia-settings on startup. For example:\n\n```\n$ nvidia-settings -a \"GPUGraphicsClockOffset[performance_level]=offset\"\n$ nvidia-settings -a \"GPUMemoryTransferRateOffset[performance_level]=offset\"\n```\n\nWhere performance_level is the number of the highest performance level. If there are multiple GPUs on the machine, the GPU ID should be specified: [gpu:gpu_id]GPUGraphicsClockOffset[performance_level]=offset.\n\n"
    },
    {
      "title": "Custom TDP Limit",
      "level": 3,
      "content": "Modern NVIDIA graphics cards throttle frequency to stay in their TDP and temperature limits. To increase performance it is possible to change the TDP limit, which will result in higher temperatures and higher power consumption.\n\nFor example, to set the power limit to 160.30W:\n\n```\n# nvidia-smi -pl 160.30\n```\n\nTo set the power limit on boot (without driver persistence):\n\n```\n/etc/systemd/system/nvidia-tdp.timer\n```\n\n```\n[Unit]\nDescription=Set NVIDIA power limit on boot\n\n[Timer]\nOnBootSec=5\n\n[Install]\nWantedBy=timers.target\n```\n\n```\n/etc/systemd/system/nvidia-tdp.service\n```\n\n```\n[Unit]\nDescription=Set NVIDIA power limit\n\n[Service]\nType=oneshot\nExecStart=/usr/bin/nvidia-smi -pl 160.30\n```\n\nNow enable the nvidia-tdp.timer.\n\n"
    },
    {
      "title": "Set fan speed at login",
      "level": 3,
      "content": "You can adjust the fan speed on your graphics card with nvidia-settings console interface. First ensure that your Xorg configuration has enabled the bit 2 in the Coolbits option.\n\nPlace the following line in your xinitrc file to adjust the fan when you launch Xorg. Replace n with the fan speed percentage you want to set.\n\n```\nnvidia-settings -a \"[gpu:0]/GPUFanControlState=1\" -a \"[fan:0]/GPUTargetFanSpeed=n\"\n```\n\nYou can also configure a second GPU by incrementing the GPU and fan number.\n\n```\nnvidia-settings -a \"[gpu:0]/GPUFanControlState=1\" -a \"[fan:0]/GPUTargetFanSpeed=n\" \\\n                -a \"[gpu:1]/GPUFanControlState=1\" -a  [fan:1]/GPUTargetFanSpeed=n\" &\n```\n\nIf you use a login manager such as GDM or SDDM, you can create a desktop entry file to process this setting. Create ~/.config/autostart/nvidia-fan-speed.desktop and place this text inside it. Again, change n to the speed percentage you want.\n\n```\n[Desktop Entry]\nType=Application\nExec=nvidia-settings -a \"[gpu:0]/GPUFanControlState=1\" -a \"[fan:0]/GPUTargetFanSpeed=n\"\nX-GNOME-Autostart-enabled=true\nName=nvidia-fan-speed\n```\n\nTo make it possible to adjust the fanspeed of more than one graphics card, run:\n\n```\n$ nvidia-xconfig --enable-all-gpus\n$ nvidia-xconfig --cool-bits=4\n```\n\n"
    },
    {
      "title": "Simple overclocking script using NVML",
      "level": 3,
      "content": "The Nvidia Management Library (NVML) provides an API that can manage the GPU's core and memory clock offsets and power limit. To utilise this, you can install python-nvidia-ml-pyAUR and then use the following Python script with your desired settings. This script needs to be run as root after every restart to re-apply the overclock / undervolt.\n\n```\n#!/usr/bin/env python\n\nfrom pynvml import *\n\nnvmlInit()\n\n# This sets the GPU to adjust - if this gives you errors or you have multiple GPUs, set to 1 or try other values\nmyGPU = nvmlDeviceGetHandleByIndex(0)\n\n# The GPU clock offset value should replace \"000\" in the line below.\nnvmlDeviceSetGpcClkVfOffset(myGPU, 000)\n\n# The memory clock offset should be **multiplied by 2** to replace the \"000\" below\n# For example, an offset of 500 means inserting a value of 1000 in the next line\nnvmlDeviceSetMemClkVfOffset(myGPU, 000)\n\n# The power limit can be set below in mW - 216W becomes 216000, etc. Remove the below line if you don't want to adjust power limits.\nnvmlDeviceSetPowerManagementLimit(myGPU, 000000)\n```\n\n"
    },
    {
      "title": "Kernel module parameters",
      "level": 2,
      "content": "Some options can be set as kernel module parameters, a full list can be obtained by running modinfo nvidia or looking at nv-reg.h. See Gentoo:NVidia/nvidia-drivers#Kernel module parameters as well.\n\nFor example, enabling the following will enable the PAT feature [7], which affects how memory is allocated. PAT was first introduced in Pentium III [8] and is supported by most newer CPUs (see wikipedia:Page attribute table#Processors). If your system can support this feature, it should improve performance.\n\n```\n/etc/modprobe.d/nvidia.conf\n```\n\n```\noptions nvidia NVreg_UsePageAttributeTable=1\n```\n\nOn some notebooks, to enable any NVIDIA settings tweaking you must include this option, otherwise it responds with \"Setting applications clocks is not supported\" etc.\n\n```\n/etc/modprobe.d/nvidia.conf\n```\n\n```\noptions nvidia NVreg_RegistryDwords=\"OverrideMaxPerf=0x1\"\n```\n\n"
    },
    {
      "title": "Preserve video memory after suspend",
      "level": 2,
      "content": "By default the NVIDIA Linux drivers save and restore only essential video memory allocations on system suspend and resume. Quoting NVIDIA:\n\nThe \"still experimental\" interface enables saving all video memory (given enough space on disk or RAM).\n\nTo save and restore all video memory contents, NVreg_PreserveVideoMemoryAllocations=1 kernel module parameter for the nvidia kernel module needs to be set. While NVIDIA does not set this by default, Arch Linux does so for the supported drivers, making preserve work out of the box.\n\nTo verify that NVreg_PreserveVideoMemoryAllocations is enabled, execute the following:\n\n```\n# cat /proc/driver/nvidia/params | sort\n```\n\nWhich should have a line PreserveVideoMemoryAllocations: 1, and also TemporaryFilePath: \"/var/tmp\", which you can read about below.\n\nNecessary services nvidia-suspend.service, nvidia-hibernate.service, and nvidia-resume.service are enabled by default on supported drivers, as per upstream requirements.\n\nSee NVIDIA's documentation for more details.\n\nNote: **The factual accuracy of this article or section is disputed.** The factual accuracy of this article or section is disputed.\n\nThe factual accuracy of this article or section is disputed.\n\n- As per Kernel module#Using modprobe.d, you will need to regenerate the initramfs if using early KMS.\n- The video memory contents are by upstream default saved to /tmp, which is a tmpfs. NVIDIA recommends using an other filesystem to achieve the best performance. This is also required if the size is not sufficient for the amount of memory. Arch Linux thus sets nvidia.NVreg_TemporaryFilePath=/var/tmp by default on supported drivers.\n- The chosen file system containing the file needs to support unnamed temporary files (e.g. ext4 or XFS) and have sufficient capacity for storing the video memory allocations (i.e. at least 5 percent more than the sum of the memory capacities of all NVIDIA GPUs). Use the command nvidia-smi --query-gpu=memory.total --format=csv,noheader,nounits to list the memory capacities of all GPUs in the system.\n- While nvidia-resume.service is marked as required by NVIDIA, it can be optional, as its functionality is also provided by a systemd-sleep(8) hook (/usr/lib/systemd/system-sleep/nvidia) and the latter is invoked automatically. Note that GDM with Wayland however explicitly requires nvidia-resume.service to be enabled.\n\n"
    },
    {
      "title": "Driver persistence",
      "level": 2,
      "content": "NVIDIA has a daemon that can be optionally run at boot. In a standard single-GPU X desktop environment the persistence daemon is not needed and can actually create issues [9]. See the Driver Persistence section of the NVIDIA documentation for more details.\n\nTo start the persistence daemon at boot, enable the nvidia-persistenced.service. For manual usage see the upstream documentation.\n\n"
    },
    {
      "title": "Forcing YCbCr with 4:2:0 subsampling",
      "level": 2,
      "content": "If you are facing limitations of older output standards that can still be mitigated by using YUV 4:2:0, the NVIDIA driver has an undocumented X11 option to enforce that:\n\n```\nOption \"ForceYUV420\" \"True\"\n```\n\nThis will allow higher resolutions or refresh rates but have detrimental impact on the image quality.\n\n"
    },
    {
      "title": "Configure applications to render using GPU",
      "level": 2,
      "content": "See PRIME#Configure applications to render using GPU\n\n"
    }
  ]
}