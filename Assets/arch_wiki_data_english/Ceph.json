{
  "title": "Ceph",
  "url": "https://wiki.archlinux.org/title/Ceph",
  "sections": [
    {
      "title": "Introduction",
      "level": 1,
      "content": "Related articles\n\n- GlusterFS\n\nCeph is a storage platform with a focus on being distributed, resilient, and having good performance and high reliability. Ceph can also be used as a block storage solution for virtual machines or through the use of FUSE, a conventional filesystem. Ceph is extremely configurable, with administrators being able to control virtually all aspects of the system. A command line interface is used to monitor and control the cluster. The platform also contains authentication & authorization features, and various gateways to make it compatible with systems such as OpenStack Swift and Amazon S3.\n\nFrom Wikipedia: Ceph (software):\n\nFrom Ceph.com:\n\nNote: **This article or section is out of date.** This article or section is out of date.\n\nNote: **This article or section is out of date.** This article or section is out of date.\n\nThis article or section is out of date.\n\nThe official documentation states \"the manual procedure is primarily for exemplary purposes for those developing deployment scripts with Chef, Juju, Puppet, etc.\".\n\n"
    },
    {
      "title": "Terminology",
      "level": 2,
      "content": "- Client : Something which connects to a Ceph cluster to access data but is not part of the Ceph cluster itself.\n- MONs : Also known as monitors, these store cluster state and maps containing information about the cluster such as running services and data locations.\n- MDSs : Also known as metadata servers, these store metadata for the Ceph filesystem to reduce load on the storage cluster (e.g. information for commands such as ls).\n- Node : A machine which is running Ceph services, such as OSDs or MONs.\n- OSDs : Also known as OSD daemons, these are responsible for the storage of data within the cluster and also conduct various related operations such as replication, recovery, and rebalancing.\n- Storage cluster : The core set of software responsible for storing data (OSDs+MONs).\n\n"
    },
    {
      "title": "Packages",
      "level": 3,
      "content": "Install it with the package cephAUR. You may instead install ceph-gitAUR if you want a bleeding-edge installation.\n\nInstall cephAUR on all nodes that will be in the cluster.\n\n"
    },
    {
      "title": "NTP Client",
      "level": 3,
      "content": "Install and run a time synchronisation client on the node. See Time synchronization for details.\n\n"
    },
    {
      "title": "Bootstrapping a storage cluster",
      "level": 2,
      "content": "Before a storage cluster can operate, the monitors for that cluster must be bootstrapped with several identifiers and keyrings.\n\nThe upstream Ceph documentation is well-written and kept updated with the latest releases.\n\nTo boostrap a storage cluster, follow the steps documented in the official manual deployment guide.\n\n"
    },
    {
      "title": "Starting a monitor",
      "level": 3,
      "content": "Since your system most likely uses systemd, you can enable a monitor as a systemd unit.\n\nAs an example, for a monitor named node1 start and enable ceph-mon@node1.service as detailed in Systemd#Using units.\n\n"
    },
    {
      "title": "See also",
      "level": 2,
      "content": "- Official site Homepage Documentation\n- Official source code GitHub organization Ceph\n\n- Homepage\n- Documentation\n\n- GitHub organization\n- Ceph\n\n"
    }
  ]
}