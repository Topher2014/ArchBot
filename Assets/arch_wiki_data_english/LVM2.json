{
  "title": "LVM2",
  "url": "https://wiki.archlinux.org/title/LVM2",
  "sections": [
    {
      "title": "Introduction",
      "level": 1,
      "content": "Related articles\n\n- Install Arch Linux on LVM\n- LVM on software RAID\n- dm-crypt/Encrypting an entire system#LVM on LUKS\n- dm-crypt/Encrypting an entire system#LUKS on LVM\n- Resizing LVM-on-LUKS\n- Create root filesystem snapshots with LVM\n\nFrom Wikipedia:Logical Volume Manager (Linux):\n\n"
    },
    {
      "title": "LVM building blocks",
      "level": 3,
      "content": "Logical Volume Management utilizes the kernel's device-mapper feature to provide a system of partitions independent of underlying disk layout. With LVM you abstract your storage and have \"virtual partitions\", making extending/shrinking easier (subject to potential filesystem limitations).\n\nVirtual partitions allow addition and removal without worry of whether you have enough contiguous space on a particular disk, getting caught up fdisking a disk in use (and wondering whether the kernel is using the old or new partition table), or, having to move other partitions out of the way.\n\nBasic building blocks of LVM:\n\nExample:\n\n```\nPhysical disks\n\n  Disk1 (/dev/sda):\n    ┌──────────────────────────────────────┬─────────────────────────────────────┐\n    │ Partition1  50 GiB (Physical volume) │ Partition2 80 GiB (Physical volume) │\n    │ /dev/sda1                            │ /dev/sda2                           │\n    └──────────────────────────────────────┴─────────────────────────────────────┘\n\n  Disk2 (/dev/sdb):\n    ┌──────────────────────────────────────┐\n    │ Partition1 120 GiB (Physical volume) │\n    │ /dev/sdb1                            │\n    └──────────────────────────────────────┘\n```\n\n```\nLVM logical volumes\n\n  Volume Group1 (/dev/MyVolGroup/ = /dev/sda1 + /dev/sda2 + /dev/sdb1):\n    ┌─────────────────────────┬─────────────────────────┬──────────────────────────┐\n    │ Logical volume1 15 GiB  │ Logical volume2 35 GiB  │ Logical volume3 200 GiB  │\n    │ /dev/MyVolGroup/rootvol │ /dev/MyVolGroup/homevol │ /dev/MyVolGroup/mediavol │\n    └─────────────────────────┴─────────────────────────┴──────────────────────────┘\n```\n\n"
    },
    {
      "title": "Advantages",
      "level": 3,
      "content": "LVM gives you more flexibility than just using normal hard drive partitions:\n\n- Use any number of disks as one big disk.\n- Have logical volumes stretched over several disks (RAID, mirroring, striping which offer advantages such as additional resilience and performance [1]).\n- Create small logical volumes and resize them \"dynamically\" as they get filled up.\n- Resize logical volumes regardless of their order on disk. It does not depend on the position of the LV within VG, there is no need to ensure surrounding available space.\n- Resize/create/delete logical and physical volumes online. File systems on them still need to be resized, but some (such as Ext4 and Btrfs) support online resizing.\n- Online/live migration of LV (or segments) being used by services to different disks without having to restart services.\n- Snapshots allow you to backup a frozen copy of the file system, while keeping service downtime to a minimum and easily merge the snapshot into the original volume later.\n- Support for unlocking separate volumes without having to enter a key multiple times on boot (make LVM on top of LUKS).\n- Built-in support for caching of frequently used data (lvmcache(7)).\n\n"
    },
    {
      "title": "Disadvantages",
      "level": 3,
      "content": "- Additional steps in setting up the system (may require changes to mkinitcpio configuration), more complicated. Requires (multiple) daemons to constantly run.\n- If dual-booting, note that Windows does not support LVM; you will be unable to access any LVM partitions from Windows. 3rd Party software may allow to mount certain kinds of LVM setups. [2]\n- If your physical volumes are not on a RAID-1, RAID-5 or RAID-6 losing one disk can lose one or more logical volumes if you span (or extend) your logical volumes across multiple non-redundant disks.\n- It is not always easy to shrink the space used by the logical volume manager, meaning the physical volumes used for the logical volumes. If the physical extents are scattered across the physical volume until the end you might need to inspect the segments and move them (potentially to another physical device) or the same device with custom allocation settings (e.g. --alloc anywhere). If you want to dual-boot with other operating systems (e.g. with Microsoft Windows), the only space left on the device for Microsoft Windows is the space not used by LVM / not used as physical volume.\n- Potentially worse performance than using plain partitions. [3]\n- May not work well with all file systems, especially those that are designed to be (multi-)device aware. For example, Btrfs offers some of the same functionality (multi device support, (sub)volumes, snapshots and RAID) which could clash (read further about issues with LVM snapshots with Btrfs).\n\n"
    },
    {
      "title": "Installation",
      "level": 2,
      "content": "Make sure the lvm2 package is installed.\n\nIf you have LVM volumes not activated via the initramfs, enable lvm2-monitor.service, which is provided by the lvm2 package.\n\n"
    },
    {
      "title": "Creating",
      "level": 4,
      "content": "To create a PV on /dev/sda1, run:\n\n```\n# pvcreate /dev/sda1\n```\n\nYou can check the PV is created using the following command:\n\n```\n# pvs\n```\n\n"
    },
    {
      "title": "Growing",
      "level": 4,
      "content": "After extending or prior to reducing the size of a device that has a physical volume on it, you need to grow or shrink the PV using pvresize(8).\n\nTo expand the PV on /dev/sda1 after enlarging the partition, run:\n\n```\n# pvresize /dev/sda1\n```\n\nThis will automatically detect the new size of the device and extend the PV to its maximum.\n\n"
    },
    {
      "title": "Shrinking",
      "level": 4,
      "content": "To shrink a physical volume prior to reducing its underlying device, add the --setphysicalvolumesize size parameters to the command, e.g.:\n\n```\n# pvresize --setphysicalvolumesize 40G /dev/sda1\n```\n\nThe above command may leave you with this error:\n\n```\n/dev/sda1: cannot resize to 25599 extents as later ones are allocated.\n0 physical volume(s) resized / 1 physical volume(s) not resized\n```\n\nIndeed pvresize will refuse to shrink a PV if it has allocated extents after where its new end would be. One needs to run pvmove beforehand to relocate these elsewhere in the volume group if there is sufficient free space.\n\nBefore freeing up physical extents at the end of the volume, one must run pvdisplay -v -m to see them. An alternative way to view segments in a tabular form is pvs --segments -v.\n\nIn the below example, there is one physical volume on /dev/sdd1, one volume group vg1 and one logical volume backup.\n\n```\n# pvdisplay -v -m\n```\n\n```\nFinding all volume groups.\n    Using physical volume(s) on command line.\n  --- Physical volume ---\n  PV Name               /dev/sdd1\n  VG Name               vg1\n  PV Size               1.52 TiB / not usable 1.97 MiB\n  Allocatable           yes \n  PE Size               4.00 MiB\n  Total PE              399669\n  Free PE               153600\n  Allocated PE          246069\n  PV UUID               MR9J0X-zQB4-wi3k-EnaV-5ksf-hN1P-Jkm5mW\n   \n  --- Physical Segments ---\n  Physical extent 0 to 153600:\n    FREE\n  Physical extent 153601 to 307199:\n    Logical volume\t/dev/vg1/backup\n    Logical extents\t1 to 153599\n  Physical extent 307200 to 307200:\n    FREE\n  Physical extent 307201 to 399668:\n    Logical volume\t/dev/vg1/backup\n    Logical extents\t153601 to 246068\n```\n\nOne can observe FREE space are split across the volume. To shrink the physical volume, we must first move all used segments to the beginning.\n\nHere, the first free segment is from 0 to 153600 and leaves us with 153601 free extents. We can now move this segment number from the last physical extent to the first extent. The command will thus be:\n\n```\n# pvmove --alloc anywhere /dev/sdd1:307201-399668 /dev/sdd1:0-92467\n```\n\n```\n/dev/sdd1: Moved: 0.1 %\n/dev/sdd1: Moved: 0.2 %\n...\n/dev/sdd1: Moved: 99.9 %\n/dev/sdd1: Moved: 100.0 %\n```\n\nNote: **from** \n\n- This command moves 399668 - 307201 + 1 = 92468 PEs from the last segment to the first segment. This is possible as the first segment encloses 153600 free PEs, which can contain the 92467 - 0 + 1 = 92468 moved PEs.\n- The --alloc anywhere option is used as we move PEs inside the same partition. In case of different partitions, the command would look something like this: # pvmove /dev/sdb1:1000-1999 /dev/sdc1:0-999\n- This command may take a long time (one to two hours) in case of large volumes. It might be a good idea to run this command in a tmux or GNU Screen session. Any unwanted stop of the process could be fatal.\n- Once the operation is complete, run fsck to make sure your file system is valid.\n\n```\n# pvmove /dev/sdb1:1000-1999 /dev/sdc1:0-999\n```\n\nOnce all your free physical segments are on the last physical extents, run vgdisplay with root privileges and see your free PE.\n\nThen you can now run again the command:\n\n```\n# pvresize --setphysicalvolumesize size PhysicalVolume\n```\n\nSee the result:\n\n```\n# pvs\n```\n\n```\nPV         VG   Fmt  Attr PSize    PFree \n  /dev/sdd1  vg1  lvm2 a--     1t     500g\n```\n\nLast, you need to shrink the partition with your favorite partitioning tool.\n\n"
    },
    {
      "title": "Creating a volume group",
      "level": 4,
      "content": "To create a VG MyVolGroup with an associated PV /dev/sdb1, run:\n\n```\n# vgcreate MyVolGroup /dev/sdb1\n```\n\nYou can check the VG MyVolGroup is created using the following command:\n\n```\n# vgs\n```\n\nYou can bind multiple PVs when creating a VG like this:\n\n```\n# vgcreate MyVolGroup /dev/sdb1 /dev/sdb2\n```\n\n"
    },
    {
      "title": "Activating a volume group",
      "level": 4,
      "content": "```\n# vgchange -a y MyVolGroup\n```\n\nBy default, this will reactivate the volume group when applicable. For example, if you had a drive failure in a mirror and you swapped the drive; and ran (1) pvcreate, (2) vgextend and (3) vgreduce --removemissing --force.\n\n"
    },
    {
      "title": "Repairing a volume group",
      "level": 4,
      "content": "To start the rebuilding process of the degraded mirror array in this example, you would run:\n\n```\n# lvconvert --repair /dev/MyVolGroup/mirror\n```\n\nYou can monitor the rebuilding process (Cpy%Sync Column output) with:\n\n```\n# lvs -a -o +devices\n```\n\n"
    },
    {
      "title": "Deactivating a volume group",
      "level": 4,
      "content": "Just invoke\n\n```\n# vgchange -a n MyVolGroup\n```\n\nThis will deactivate the volume group and allow you to unmount the container it is stored in.\n\n"
    },
    {
      "title": "Renaming a volume group",
      "level": 4,
      "content": "Use the vgrename(8) command to rename an existing volume group.\n\nEither of the following commands renames the existing volume group MyVolGroup to my_volume_group\n\n```\n# vgrename /dev/MyVolGroup /dev/my_volume_group\n```\n\n```\n# vgrename MyVolGroup my_volume_group\n```\n\nMake sure to update all configuration files (e.g. /etc/fstab or /etc/crypttab) that reference the renamed volume group.\n\n"
    },
    {
      "title": "Add physical volume to a volume group",
      "level": 4,
      "content": "You first create a new physical volume on the block device you wish to use, then extend your volume group\n\n```\n# pvcreate /dev/sdb1\n# vgextend MyVolGroup /dev/sdb1\n```\n\nThis of course will increase the total number of physical extents on your volume group, which can be allocated by logical volumes as you see fit.\n\n"
    },
    {
      "title": "Remove partition from a volume group",
      "level": 4,
      "content": "If you created a logical volume on the partition, remove it first.\n\nAll of the data on that partition needs to be moved to another partition. Fortunately, LVM makes this easy:\n\n```\n# pvmove /dev/sdb1\n```\n\nIf you want to have the data on a specific physical volume, specify that as the second argument to pvmove:\n\n```\n# pvmove /dev/sdb1 /dev/sdf1\n```\n\nThen the physical volume needs to be removed from the volume group:\n\n```\n# vgreduce MyVolGroup /dev/sdb1\n```\n\nOr remove all empty physical volumes:\n\n```\n# vgreduce --all MyVolGroup\n```\n\nFor example: if you have a bad disk in a group that cannot be found because it has been removed or failed:\n\n```\n# vgreduce --removemissing --force MyVolGroup\n```\n\nAnd lastly, if you want to use the partition for something else, and want to avoid LVM thinking that the partition is a physical volume:\n\n```\n# pvremove /dev/sdb1\n```\n\n"
    },
    {
      "title": "Creating a logical volume",
      "level": 4,
      "content": "To create a LV homevol in a VG MyVolGroup with 300 GiB of capacity, run:\n\n```\n# lvcreate -L 300G MyVolGroup -n homevol\n```\n\nor, to create a LV homevol in a VG MyVolGroup with the rest of capacity, run:\n\n```\n# lvcreate -l 100%FREE MyVolGroup -n homevol\n```\n\nTo create the LV while restricting it to specific PVs within the VG, append them to the command:\n\n```\n# lvcreate -L 300G MyVolGroup -n homevol /dev/sda1\n```\n\nThe new LV will appear as /dev/MyVolGroup/homevol. Now you can format the LV with an appropriate file system.\n\nYou can check the LV is created using the following command:\n\n```\n# lvs\n```\n\n"
    },
    {
      "title": "Renaming a logical volume",
      "level": 4,
      "content": "To rename an existing logical volume, use the lvrename(8) command.\n\nEither of the following commands renames logical volume old_vol in volume group MyVolGroup to new_vol.\n\n```\n# lvrename /dev/MyVolGroup/old_vol /dev/MyVolGroup/new_vol\n```\n\n```\n# lvrename MyVolGroup old_vol new_vol\n```\n\nMake sure to update all configuration files (e.g. /etc/fstab or /etc/crypttab) that reference the renamed logical volume.\n\n"
    },
    {
      "title": "Resizing the logical volume and file system in one go",
      "level": 4,
      "content": "Extend the logical volume mediavol in MyVolGroup by 10 GiB and resize its file system all at once:\n\n```\n# lvresize -L +10G --resizefs MyVolGroup/mediavol\n```\n\nSet the size of logical volume mediavol in MyVolGroup to 15 GiB and resize its file system all at once:\n\n```\n# lvresize -L 15G --resizefs MyVolGroup/mediavol\n```\n\nIf you want to fill all the free space on a volume group, use the following command:\n\n```\n# lvresize -l +100%FREE --resizefs MyVolGroup/mediavol\n```\n\nSee lvresize(8) for more detailed options.\n\n"
    },
    {
      "title": "Resizing the logical volume and file system separately",
      "level": 4,
      "content": "For file systems not supported by fsadm(8) will need to use the appropriate utility to resize the file system before shrinking the logical volume or after expanding it.\n\nTo extend logical volume mediavol within volume group MyVolGroup by 2 GiB without touching its file system:\n\n```\n# lvresize -L +2G MyVolGroup/mediavol\n```\n\nNow expand the file system (ext4 in this example) to the maximum size of the underlying logical volume:\n\n```\n# resize2fs /dev/MyVolGroup/mediavol\n```\n\nFor Btrfs, btrfs-filesystem(8) expects the mountpoint instead of the device, the equivalent is:\n\n```\n# btrfs filesystem resize max /mnt/my-mountpoint\n```\n\nTo reduce the size of logical volume mediavol in MyVolGroup by 500 MiB, first calculate the resulting file system size and shrink the file system (Ext4 in this example) to the new size:\n\n```\n# resize2fs /dev/MyVolGroup/mediavol NewSize\n```\n\nUnlike Ext4, Btrfs supports online shrinking (again, a mountpoint should be specified) e.g.:\n\n```\n# btrfs filesystem resize -500M /mnt/my-mountpoint\n```\n\nWhen the file system is shrunk, reduce the size of logical volume:\n\n```\n# lvresize -L -500M MyVolGroup/mediavol\n```\n\nTo calculate the exact logical volume size for ext2, ext3, ext4 file systems, use a simple formula: LVM_EXTENTS = FS_BLOCKS × FS_BLOCKSIZE ÷ LVM_EXTENTSIZE.\n\n```\n# tune2fs -l /dev/MyVolGroup/mediavol | grep Block\n```\n\n```\nBlock count:              102400000\nBlock size:               4096\nBlocks per group:         32768\n```\n\n```\n# vgdisplay MyVolGroup | grep \"PE Size\"\n```\n\n```\nPE Size               4.00 MiB\n```\n\n```\n102400000 blocks × 4096 bytes/block ÷ 4 MiB/extent = 100000 extents\n```\n\nPassing --resizefs will confirm that the correctness.\n\n```\n# lvreduce -l 100000 --resizefs /dev/MyVolGroup/mediavol\n```\n\n```\n...\nThe filesystem is already 102400000 (4k) blocks long.  Nothing to do!\n...\nLogical volume sysvg/root successfully resized.\n```\n\nSee lvresize(8) for more detailed options.\n\n"
    },
    {
      "title": "Removing a logical volume",
      "level": 4,
      "content": "First, find out the name of the logical volume you want to remove. You can get a list of all logical volumes with:\n\n```\n# lvs\n```\n\nNext, look up the mountpoint of the chosen logical volume:\n\n```\n$ lsblk\n```\n\nThen unmount the filesystem on the logical volume:\n\n```\n# umount /mountpoint\n```\n\nFinally, remove the logical volume:\n\n```\n# lvremove volume_group/logical_volume\n```\n\nFor example:\n\n```\n# lvremove MyVolGroup/homevol\n```\n\nConfirm by typing in y.\n\nMake sure to update all configuration files (e.g. /etc/fstab or /etc/crypttab) that reference the removed logical volume.\n\nYou can verify the removal of the logical volume by typing lvs as root again (see first step of this section).\n\n"
    },
    {
      "title": "Snapshots",
      "level": 2,
      "content": "LVM supports CoW (Copy-on-Write) snapshots. A CoW snapshot initially points to the original data. When data blocks are overwritten, the original copy is left intact and the new blocks are written elsewhere on-disk. This has several desirable properties:\n\n- Creating snapshots is fast, because it does not copy data (just the much shorter list of pointers to the on-disk locations).\n- Snapshots require just enough free space to hold the new data blocks (plus a negligible amount for the pointers to the new blocks). For example, a snapshot of 35 GiB of data, where you write only 2 GiB (on both the original and snapshot), only requires 2 GiB of free space.\n\nLVM snapshots are at the block level. They make a new block device, with no apparent relationship to the original except when dealing with the LVM tools. Therefore, deleting files in the original copy does not free space in the snapshots. If you need filesystem-level snapshots, you rather need btrfs, ZFS or bcachefs.\n\nNote: **is not a backup** \n\n- A CoW snapshot is not a backup, because it does not make a second copy of the original data. For example, a damaged disk sector that affects original data also affects the snapshots. That said, a snapshot can be helpful while using other tools to make backups, as outlined below.\n- Btrfs expects different filesystems to have different UUIDs. If you snapshot a LVM volume that contains a btrfs filesystem, make sure to change the UUID of the original or the copy, before both are mounted (or made visible to the kernel, for example if an unrelated daemon triggers a btrfs device scan). For details see btrfs wiki Gotcha's.\n\n"
    },
    {
      "title": "Configuration",
      "level": 3,
      "content": "You create snapshot logical volumes just like normal ones.\n\n```\n# lvcreate --size 100M --snapshot --name snap01vol /dev/MyVolGroup/lvol\n```\n\nWith that volume, you may modify less than 100 MiB of data, before the snapshot volume fills up.\n\nReverting the modified lvol logical volume to the state when the snap01vol snapshot was taken can be done with\n\n```\n# lvconvert --merge /dev/MyVolGroup/snap01vol\n```\n\nIn case the origin logical volume is active, merging will occur on the next reboot (merging can be done even from a LiveCD).\n\nAlso multiple snapshots can be taken and each one can be merged with the origin logical volume at will.\n\n"
    },
    {
      "title": "Backups",
      "level": 3,
      "content": "A snapshot provides a frozen copy of a file system to make backups. For example, a backup taking two hours provides a more consistent image of the file system than directly backing up the partition.\n\nThe snapshot can be mounted and backed up with dd or tar. The size of the backup file done with dd will be the size of the files residing on the snapshot volume. To restore just create a snapshot, mount it, and write or extract the backup to it. And then merge it with the origin.\n\nSee Create root filesystem snapshots with LVM for automating the creation of clean root file system snapshots during system startup for backup and rollback.\n\nNote: **This article or section needs expansion.** This article or section needs expansion.\n\nThis article or section needs expansion.\n\n"
    },
    {
      "title": "Encryption",
      "level": 2,
      "content": "See dm-crypt/Encrypting an entire system#LUKS on LVM and dm-crypt/Encrypting an entire system#LVM on LUKS for the possible schemes of combining LUKS with LVM.\n\n"
    },
    {
      "title": "Cache",
      "level": 2,
      "content": "Note: **This article or section needs expansion.** This article or section needs expansion.\n\nThis article or section needs expansion.\n\nFrom lvmcache(7):\n\n"
    },
    {
      "title": "Create cache",
      "level": 3,
      "content": "Convert your fast disk (/dev/fastdisk) to PV and add to your existing VG (MyVolGroup):\n\n```\n# vgextend MyVolGroup /dev/fastdisk\n```\n\nCreate a cache pool with automatic meta data on /dev/fastdisk and convert the existing LV MyVolGroup/rootvol to a cached volume, all in one step:\n\n```\n# lvcreate --type cache --cachemode writethrough -l 100%FREE -n root_cachepool MyVolGroup/rootvol /dev/fastdisk\n```\n\nCachemode has two possible options:\n\n- writethrough ensures that any data written will be stored both in the cache pool LV and on the origin LV. The loss of a device associated with the cache pool LV in this case would not mean the loss of any data;\n- writeback ensures better performance, but at the cost of a higher risk of data loss in case the drive used for cache fails.\n\nIf a specific --cachemode is not indicated, the system will assume writethrough as default.\n\n"
    },
    {
      "title": "Remove cache",
      "level": 3,
      "content": "If you ever need to undo the one step creation operation above:\n\n```\n# lvconvert --uncache MyVolGroup/rootvol\n```\n\nThis commits any pending writes still in the cache back to the origin LV, then deletes the cache. Other options are available and described in lvmcache(7).\n\n"
    },
    {
      "title": "RAID",
      "level": 2,
      "content": "LVM may be used to create a software RAID. It is a good choice if the user does not have hardware RAID and was planning on using LVM anyway. From lvmraid(7):\n\nLVM RAID supports RAID 0, RAID 1, RAID 4, RAID 5, RAID 6 and RAID 10. See Wikipedia:Standard RAID levels for details on each level.\n\n"
    },
    {
      "title": "Setup RAID",
      "level": 3,
      "content": "Create physical volumes:\n\n```\n# pvcreate /dev/sda2 /dev/sdb2\n```\n\nCreate volume group on the physical volumes:\n\n```\n# vgcreate MyVolGroup /dev/sda2 /dev/sdb2\n```\n\n"
    },
    {
      "title": "New volumes",
      "level": 4,
      "content": "Create logical volumes using lvcreate --type raidlevel, see lvmraid(7) and lvcreate(8) for more options.\n\n```\n# lvcreate --type RaidLevel [OPTIONS] -n Name -L Size VG [PVs]\n```\n\nFor example:\n\n```\n# lvcreate -n myraid1vol -i 2 -I 64 -L 70G VolGroup00 /dev/nvme1n1p1 /dev/nvme0n1p1\n```\n\nwill create a 70 GiB striped (raid0) logical volume named \"myraid1vol\" in VolGroup00. Stripes will be spread over /dev/nvme1n1p1 and /dev/nvme0n1p1. Stripesize is set to be 64K.\n\nFor example:\n\n```\n# lvcreate --type raid1 --mirrors 1 -L 20G -n myraid1vol MyVolGroup /dev/sda2 /dev/sdb2\n```\n\nwill create a 20 GiB mirrored logical volume named \"myraid1vol\" in VolGroup00 on /dev/sda2 and /dev/sdb2.\n\nFor example:\n\n```\n# lvcreate -n myraid1vol -L 100G --type raid10 -m 1 -i 2 MyVolGroup /dev/sdd1 /dev/sdc1 /dev/sdb1 /dev/sda5\n```\n\nwill create a 100 GiB RAID10 logical volume named \"myraid1vol\" in VolGroup00 on /dev/sdd1, /dev/sdc1, /dev/sdb1, and /dev/sda5.\n\n"
    },
    {
      "title": "Existing volumes",
      "level": 4,
      "content": "You can convert easily a non-RAID (e.g. linear) volume to pretty much any other raid configuration provided that you have enough physical devices to meet the RAID requirements. Some of them will require you to go through intermediate steps which lvconvert will inform you about and prompt you to agree. raid10 below can be replaced with raid0, raid1, raid5 etc.\n\n```\n# lvconvert --type raid10 /dev/vg01/lv01\n```\n\nUse specific PVs:\n\n```\n# lvconvert --type raid10 /dev/vg01/lv01 /dev/sda1 /dev/sdb2 /dev/nvme0n1p1 ...\n```\n\nYou can keep track of the progress of conversion with:\n\n```\n# watch lvs -o name,vg_name,copy_percent\n```\n\n"
    },
    {
      "title": "Thin provisioning",
      "level": 2,
      "content": "From lvmthin(7):\n\n"
    },
    {
      "title": "Example: implementing virtual private servers",
      "level": 3,
      "content": "Here is the classic use case. Suppose you want to start your own VPS service, initially hosting about 100 VPSes on a single PC with a 930 GiB hard drive. Hardly any of the VPSes will actually use all of the storage they are allotted, so rather than allocate 9 GiB to each VPS, you could allow each VPS a maximum of 30 GiB and use thin provisioning to only allocate as much hard drive space to each VPS as they are actually using. Suppose the 930 GiB hard drive is /dev/sdb. Here is the setup.\n\nPrepare the volume group, MyVolGroup.\n\n```\n# vgcreate MyVolGroup /dev/sdb\n```\n\nCreate the thin pool LV, MyThinPool. This LV provides the blocks for storage.\n\n```\n# lvcreate --type thin-pool -n MyThinPool -l 95%FREE MyVolGroup\n```\n\nThe thin pool is composed of two sub-volumes, the data LV and the metadata LV. This command creates both automatically. But the thin pool stops working if either fills completely, and LVM currently does not support the shrinking of either of these volumes. This is why the above command allows for 5% of extra space, in case you ever need to expand the data or metadata sub-volumes of the thin pool.\n\nFor each VPS, create a thin LV. This is the block device exposed to the user for their root partition.\n\n```\n# lvcreate -n SomeClientsRoot -V 30G --thinpool MyThinPool MyVolGroup\n```\n\nThe block device /dev/MyVolGroup/SomeClientsRoot may then be used by a VirtualBox instance as the root partition.\n\n"
    },
    {
      "title": "Use thin snapshots to save more space",
      "level": 4,
      "content": "Thin snapshots are much more powerful than regular snapshots, because they are themselves thin LVs. See Red Hat's guide [4] for a complete list of advantages thin snapshots have.\n\nInstead of installing Linux from scratch every time a VPS is created, it is more space-efficient to start with just one thin LV containing a basic installation of Linux:\n\n```\n# lvcreate -n GenericRoot -V 30G --thinpool MyThinPool MyVolGroup\n*** install Linux at /dev/MyVolGroup/GenericRoot ***\n```\n\nThen create snapshots of it for each VPS:\n\n```\n# lvcreate -s MyVolGroup/GenericRoot -n SomeClientsRoot\n```\n\nThis way, in the thin pool there is only one copy the data common to all VPSes, at least initially. As an added bonus, the creation of a new VPS is instantaneous.\n\nSince these are thin snapshots, a write operation to GenericRoot only causes one COW operation in total, instead of one COW operation per snapshot. This allows you to update GenericRoot more efficiently than if each VPS were a regular snapshot.\n\n"
    },
    {
      "title": "Example: zero-downtime storage upgrade",
      "level": 3,
      "content": "There are applications of thin provisioning outside of VPS hosting. Here is how you may use it to grow the effective capacity of an already-mounted file system without having to unmount it. Suppose, again, that the server has a single 930 GiB hard drive. The setup is the same as for VPS hosting, only there is only one thin LV and the LV's size is far larger than the thin pool's size.\n\n```\n# lvcreate -n MyThinLV -V 16T --thinpool MyThinPool MyVolGroup\n```\n\nThis extra virtual space can be filled in with actual storage at a later time by extending the thin pool.\n\nSuppose some time later, a storage upgrade is needed, and a new hard drive, /dev/sdc, is plugged into the server. To upgrade the thin pool's capacity, add the new hard drive to the VG:\n\n```\n# vgextend MyVolGroup /dev/sdc\n```\n\nNow, extend the thin pool:\n\n```\n# lvextend -l +95%FREE MyVolGroup/MyThinPool\n```\n\nSince this thin LV's size is 16 TiB, you could add another 15.09 TiB of hard drive space before finally having to unmount and resize the file system.\n\n"
    },
    {
      "title": "Customizing",
      "level": 2,
      "content": "Some customisation is available by editing /etc/lvm/lvm.conf. You may find it useful to customize the output of lvs and pvs which by default does not include the % sync (useful to see progress of conversion between e.g. linear and raid type) and type of logical volume:\n\n```\n/etc/lvm/lvm.conf\n```\n\n```\nreport {\n \tlvs_cols = \"lv_name,lv_attr,lv_active,vg_name,lv_size,lv_layout,lv_allocation_policy,copy_percent,chunk_size\"\n\tpvs_cols = \"pv_name,vg_name,pv_size,pv_free,pv_used,dev_size\"\n}\n```\n\n"
    },
    {
      "title": "LVM commands do not work",
      "level": 3,
      "content": "- Load proper module:\n\n```\n# modprobe dm_mod\n```\n\nThe dm_mod module should be automatically loaded. In case it does not, explicitly load the module at boot.\n\n- Try preceding commands with lvm like this:\n\n```\n# lvm pvdisplay\n```\n\n"
    },
    {
      "title": "Logical volumes do not show up",
      "level": 3,
      "content": "If you are trying to mount existing logical volumes, but they do not show up in lvscan, you can use the following commands to activate them:\n\n```\n# vgscan\n# vgchange -ay\n```\n\n"
    },
    {
      "title": "LVM on removable media",
      "level": 3,
      "content": "Symptoms:\n\n```\n# vgscan\n```\n\n```\nReading all physical volumes.  This may take a while...\n  /dev/backupdrive1/backup: read failed after 0 of 4096 at 319836585984: Input/output error\n  /dev/backupdrive1/backup: read failed after 0 of 4096 at 319836643328: Input/output error\n  /dev/backupdrive1/backup: read failed after 0 of 4096 at 0: Input/output error\n  /dev/backupdrive1/backup: read failed after 0 of 4096 at 4096: Input/output error\n  Found volume group \"backupdrive1\" using metadata type lvm2\n  Found volume group \"networkdrive\" using metadata type lvm2\n```\n\nCause: removing an external LVM drive without deactivating the volume group(s) first. Before you disconnect, make sure to:\n\n```\n# vgchange -an volume_group_name\n```\n\nFix: assuming you already tried to activate the volume group with vgchange -ay vg, and are receiving the Input/output errors:\n\n```\n# vgchange -an volume_group_name\n```\n\nUnplug the external drive and wait a few minutes:\n\n```\n# vgscan\n# vgchange -ay volume_group_name\n```\n\n"
    },
    {
      "title": "Suspend/resume with LVM and removable media",
      "level": 4,
      "content": "Note: **The factual accuracy of this article or section is disputed.** The factual accuracy of this article or section is disputed.\n\nThe factual accuracy of this article or section is disputed.\n\nIn order for LVM to work properly with removable media – like an external USB drive – the volume group of the external drive needs to be deactivated before suspend. If this is not done, you may get buffer I/O errors on the dm device (after resume). For this reason, it is not recommended to mix external and internal drives in the same volume group.\n\nTo automatically deactivate the volume groups with external USB drives, tag each volume group with the sleep_umount tag in this way:\n\n```\n# vgchange --addtag sleep_umount vg_external\n```\n\nOnce the tag is set, use the following unit file for systemd to properly deactivate the volumes before suspend. On resume, they will be automatically activated by LVM.\n\n```\n/etc/systemd/system/ext_usb_vg_deactivate.service\n```\n\n```\n[Unit]\nDescription=Deactivate external USB volume groups on suspend\nBefore=sleep.target\n\n[Service]\nType=oneshot\nExecStart=-/etc/systemd/system/deactivate_sleep_vgs.sh\n\n[Install]\nWantedBy=sleep.target\n```\n\nand this script:\n\n```\n/etc/systemd/system/deactivate_sleep_vgs.sh\n```\n\n```\n#!/bin/sh\n\nTAG=@sleep_umount\nvgs=$(vgs --noheadings -o vg_name $TAG)\n\necho \"Deactivating volume groups with $TAG tag: $vgs\"\n\n# Unmount logical volumes belonging to all the volume groups with tag $TAG\nfor vg in $vgs; do\n    for lv_dev_path in $(lvs --noheadings  -o lv_path -S lv_active=active,vg_name=$vg); do\n        echo \"Unmounting logical volume $lv_dev_path\"\n        umount $lv_dev_path\n    done\ndone\n\n# Deactivate volume groups tagged with sleep_umount\nfor vg in $vgs; do\n    echo \"Deactivating volume group $vg\"\n    vgchange -an $vg\ndone\n```\n\nFinally, enable the unit.\n\n"
    },
    {
      "title": "Resizing a contiguous logical volume fails",
      "level": 3,
      "content": "If trying to extend a logical volume errors with:\n\n```\n\" Insufficient suitable contiguous allocatable extents for logical volume \"\n```\n\nThe reason is that the logical volume was created with an explicit contiguous allocation policy (options -C y or --alloc contiguous) and no further adjacent contiguous extents are available.[5]\n\nTo fix this, prior to extending the logical volume, change its allocation policy with lvchange --alloc inherit logical_volume. If you need to keep the contiguous allocation policy, an alternative approach is to move the volume to a disk area with sufficient free extents. See [6].\n\n"
    },
    {
      "title": "Command \"grub-mkconfig\" reports \"unknown filesystem\" errors",
      "level": 3,
      "content": "Make sure to remove snapshot volumes before generating grub.cfg.\n\n"
    },
    {
      "title": "Thinly-provisioned root volume device times out",
      "level": 3,
      "content": "With a large number of snapshots, thin_check runs for a long enough time so that waiting for the root device times out. To compensate, add the rootdelay=60 kernel boot parameter to your boot loader configuration. Or, make thin_check skip checking block mappings (see [7]) and regenerate the initramfs:\n\n```\n/etc/lvm/lvm.conf\n```\n\n```\nthin_check_options = [ \"-q\", \"--clear-needs-check-flag\", \"--skip-mappings\" ]\n```\n\n"
    },
    {
      "title": "Delay on shutdown",
      "level": 3,
      "content": "If you use RAID, snapshots or thin provisioning and experience a delay on shutdown, make sure lvm2-monitor.service is started. See FS#50420.\n\n"
    },
    {
      "title": "Hibernating into a thinly-provisioned swap volume",
      "level": 3,
      "content": "See Power management/Suspend and hibernate#Hibernation into a thinly-provisioned LVM volume.\n\n"
    },
    {
      "title": "See also",
      "level": 2,
      "content": "- LVM2 Resource Page on SourceWare.org\n- Gentoo:LVM\n- Red Hat Enterprise 9: Configuring and managing logical volumes\n- Ubuntu LVM Guide Part 1Part 2 details snapshots\n\n"
    }
  ]
}