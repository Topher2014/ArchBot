{
  "title": "Torque",
  "url": "https://wiki.archlinux.org/title/Torque",
  "sections": [
    {
      "title": "Introduction",
      "level": 1,
      "content": "Related articles\n\n- distcc\n- Slurm\n\nNote: **not** \n\nTORQUE is a resource manager providing control over batch jobs and distributed compute nodes. Basically, one can setup a home or small office Linux cluster and queue jobs with this software. A cluster consists of one head node and many compute nodes. The head node runs the torque-server daemon and the compute nodes run the torque-client daemon. The head node also runs a scheduler daemon.\n\n"
    },
    {
      "title": "Installation",
      "level": 2,
      "content": "Install the torqueAUR package.\n\n"
    },
    {
      "title": "/etc/hosts",
      "level": 3,
      "content": "Make sure that /etc/hosts on all of the boxes in the cluster contains the hostnames of every PC in the cluster. Example, cluster consists of 3 PCs, mars, phobos, and deimos.\n\n```\n192.168.0.20   mars\n192.168.0.21   phobos\n192.168.0.22   deimos\n```\n\n"
    },
    {
      "title": "Firewall configuration (if installed)",
      "level": 3,
      "content": "Be sure to open TCP for all machines using TORQUE.\n\nThe pbs_server (server) and pbs_mom (client) by default use TCP and UDP ports 15001-15004. pbs_mom (client) also uses UDP ports 1023 and below if privileged ports are configured (the default).\n\n"
    },
    {
      "title": "NFS",
      "level": 3,
      "content": "Technically, one does not need to use NFS but doing so simplifies the whole process. An NFS share either on the server or another machine is highly recommended to simplify the process of sharing common build disk space.\n\n"
    },
    {
      "title": "Server (head node) configuration",
      "level": 3,
      "content": "Follow these steps on the head node/scheduler.\n\nEdit /var/spool/torque/server_name to name the head node. It is recommended to match the hostname in /etc/hostname for simplicity's sake.\n\nCreate and configure the torque server:\n\n```\n# pbs_server -t create\nPBS_Server localhost.localdomain: Create mode and server database exists,\ndo you wish to continue y/(n)?y\n```\n\nThen start trqauthd by running\n\n```\n# trqauthd\n```\n\nA minimal set of options are provided here. Adjust the first line substituting \"mars\" with the hostname entered in /var/spool/torque/server_name:\n\n```\nqmgr -c \"set server acl_hosts = mars\"\nqmgr -c \"set server scheduling=true\"\nqmgr -c \"create queue batch queue_type=execution\"\nqmgr -c \"set queue batch started=true\"\nqmgr -c \"set queue batch enabled=true\"\nqmgr -c \"set queue batch resources_default.nodes=1\"\nqmgr -c \"set queue batch resources_default.walltime=3600\"\nqmgr -c \"set server default_queue=batch\"\n```\n\nIt may be of interest to keep finished jobs in the queue for a period of time.\n\n```\nqmgr -c \"set server keep_completed = 86400\"\n```\n\nHere, 86400 sec = 24 h after which point, the job will be auto removed from the queue. One can see the full log of jobs removed from the queue with the -f switch on qstat:\n\n```\nqstat -f\n```\n\nVerify the server configuration with this command:\n\n```\n# qmgr -c 'p s'\n```\n\nEdit /var/spool/torque/server_priv/nodes adding all compute nodes. Again, it is recommended to match the hostname(s) of the machines on the LAN. The syntax is HOSTNAME np=x gpus=y properties\n\n- HOSTNAME=the hostname of the machine\n- np=number of processors\n- gpus=number of gpus\n- properties=comments\n\nOnly the hostname is required, all other fields are optional.\n\nExample:\n\n```\nmars np=4\nphobos np=2\ndeimos np=2\n```\n\n- One can run both the server and client on the same box.\n- Re-running pbs_server -t create may delete this nodes file.\n\nRestart the server and the new options are sourced.\n\n"
    },
    {
      "title": "Client (compute node) configuration",
      "level": 3,
      "content": "Follow these steps on each compute node in the cluster.\n\nEdit /var/spool/torque/mom_priv/config to contain some basic info identifying the server:\n\n```\n$pbsserver      mars          # note: this is the hostname of the headnode\n$logevent       255           # bitmap of which events to log\n```\n\n"
    },
    {
      "title": "Restart the server",
      "level": 3,
      "content": "That should be it. Now restart the server so the settings can take effect.\n\n```\n# killall -s 9 pbs_server\n# pbs_server\n```\n\n"
    },
    {
      "title": "Starting the client(s)",
      "level": 3,
      "content": "In order to start the clients run the following on each of the clients, including the server if it is also a client:\n\n```\n# pbs_mom\n```\n\n"
    },
    {
      "title": "Verifying cluster status",
      "level": 2,
      "content": "To check the status of the cluster, issue the following:\n\n```\n$ pbsnodes -a\n```\n\nEach node if up should indicate that it is ready to receive jobs echoing a state of free. If a node is not working, it will report a state of down.\n\nExample output:\n\n```\nmars\n     state = free\n     np = 4\n     ntype = cluster\n     status = rectime=1308479899,varattr=,jobs=0.localhost.localdomain,state=free,netload=1638547057,\ngres=,loadave=2.69,ncpus=4,physmem=8195892kb,availmem=7172508kb,totmem=8195892kb,\nidletime=24772,nusers=1,nsessions=5,sessions=1333 1349 1353 1388 9095,\nuname=Linux mars 2.6.39-ck #1 SMP PREEMPT Sat Jun 18 14:19:01 EDT 2011 x86_64,opsys=linux\n     mom_service_port = 15002\n     mom_manager_port = 15003\n     gpus = 2\n\nphobos\n     state = free\n     np = 2\n     ntype = cluster\n     status = rectime=1308479933,varattr=,jobs=,state=free,netload=1085755815,\ngres=,loadave=2.84,ncpus=2,physmem=4019704kb,availmem=5753552kb,totmem=6116852kb,\nidletime=7324,nusers=2,nsessions=6,sessions=1565 1562 1691 1716 1737 1851,\nuname=Linux phobos 2.6.37-ck #1 SMP PREEMPT Sun Apr 3 17:16:35 EDT 2011 x86_64,opsys=linux\n     mom_service_port = 15002\n     mom_manager_port = 15003\n     gpus = 1\n\ndeimos\n     state = free\n     np = 2\n     ntype = cluster\n     status = rectime=1308479890,varattr=,jobs=2.localhost.localdomain,state=free,netload=527239670,\ngres=,loadave=0.52,ncpus=2,physmem=4057808kb,availmem=3955624kb,totmem=4057808kb,\nidletime=644,nusers=1,nsessions=1,sessions=865,\nuname=Linux deimos 2.6.39-ck #1 SMP PREEMPT Sat Jun 11 12:36:21 EDT 2011 x86_64,opsys=linux\n     mom_service_port = 15002\n     mom_manager_port = 15003\n     gpus = 1\n```\n\n"
    },
    {
      "title": "Queuing jobs",
      "level": 2,
      "content": "Queuing to the cluster is accomplished via the qsub command.\n\nA trivial test is to simply run sleep:\n\n```\n$ echo \"sleep 30\" | qsub\n```\n\nCheck the status of the queue via the qstat command described below. At this point, the work will have a status of \"Q\" which means queued. To start it, run the scheduler:\n\n```\n# pbs_sched\n```\n\nOne can modify the torque-server systemd daemon to activate pbs_sched at boot.\n\nAnother usage of qsub is to name a job and queue a script:\n\n```\n$ qsub -N x264 /home/facade/bin/x264_HQ.sh\n```\n\nAnother example can use a wrapper script to make and queue work en mass automatically.\n\n"
    },
    {
      "title": "Checking job status",
      "level": 2,
      "content": "qstat is used to check work status.\n\n```\n$ qstat\n```\n\n```\nJob id                    Name             User            Time Use S Queue\n------------------------- ---------------- --------------- -------- - -----\n13.localhost               generic-i686.pbs facade         00:05:06 R batch          \n14.localhost               atom-i686.pbs    facade         00:03:09 R batch          \n15.localhost               core2-i686.pbs   facade         00:01:02 R batch          \n16.localhost               k7-i686.pbs      facade                0 Q batch          \n17.localhost               k8-i686.pbs      facade                0 Q batch          \n18.localhost               k10-i686.pbs     facade                0 Q batch          \n19.localhost               p4-i686.pbs      facade                0 Q batch          \n20.localhost               pentm-i686.pbs   facade                0 Q batch          \n21.localhost               ...ic-x86_64.pbs facade                0 Q batch          \n22.localhost               atom-x86_64.pbs  facade                0 Q batch          \n23.localhost               core2-x86_64.pbs facade                0 Q batch          \n24.localhost               k8-x86_64.pbs    facade                0 Q batch          \n25.localhost               k10-x86_64.pbs   facade                0 Q batch\n```\n\nAppend the -n switch to see which nodes are doing which jobs.\n\n```\n$ qstat -n\n```\n\n```\nlocalhost.localdomain:\n405.localhost.lo     facade  batch    i686-generic       3035     1   0    --  01:00 C 00:12\n   mars/3+mars/2+mars/1+mars/0\n406.localhost.lo     facade  batch    i686-atom          5768     1   0    --  01:00 C 00:46\n   phobos/1+phobos/0\n407.localhost.lo     facade  batch    i686-core2        22941     1   0    --  01:00 C 00:12\n   mars/3+mars/2+mars/1+mars/0\n408.localhost.lo     facade  batch    i686-k7           10152     1   0    --  01:00 C 00:12\n   mars/3+mars/2+mars/1+mars/0\n409.localhost.lo     facade  batch    i686-k8           29657     1   0    --  01:00 C 00:12\n   mars/3+mars/2+mars/1+mars/0\n410.localhost.lo     facade  batch    i686-k10          16838     1   0    --  01:00 C 00:12\n   mars/3+mars/2+mars/1+mars/0\n411.localhost.lo     facade  batch    i686-p4           25340     1   0    --  01:00 C 00:46\n   deimos/1+deimos/0\n412.localhost.lo     facade  batch    i686-pentm        12544     1   0    --  01:00 R 00:20\n   phobos/1+phobos/0\n413.localhost.lo     facade  batch    x86_64-generic     4024     1   0    --  01:00 C 00:13\n   mars/3+mars/2+mars/1+mars/0\n414.localhost.lo     facade  batch    x86_64-atom       19330     1   0    --  01:00 C 00:13\n   mars/3+mars/2+mars/1+mars/0\n415.localhost.lo     facade  batch    x86_64-core2       2146     1   0    --  01:00 C 00:13\n   mars/3+mars/2+mars/1+mars/0\n416.localhost.lo     facade  batch    x86_64-k8         17234     1   0    --  01:00 R 00:11\n   mars/3+mars/2+mars/1+mars/0\n417.localhost.lo     facade  batch    x86_64-k10          --      1   0    --  01:00 Q   -- \n    --\n```\n\n"
    },
    {
      "title": "See also",
      "level": 2,
      "content": "- TORQUE short course from University of California, San Francisco - Good guide with templates.\n- TORQUE admin manual - Great resource and easy to read.\n- Boston College's Torque user guide - Guide not extensive but gives a flavor for how end-users can use a cluster. Probably overkill for home clusters where only one user is submitting work.\n- TORQUE mailing lists - The TORQUE community is very knowledgeable and a key asset.\n- TORQUE users mailing list archives - Searchable archive of TORQUE-users.\n\n"
    }
  ]
}