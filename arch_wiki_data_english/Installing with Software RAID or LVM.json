{
  "title": "Installing with Software RAID or LVM",
  "url": "https://wiki.archlinux.org/title/Installing_with_Software_RAID_or_LVM",
  "sections": [
    {
      "title": "Introduction",
      "level": 1,
      "content": "Related articles\n\n- RAID\n- LVM\n- Installing with Fake RAID\n- Convert a single drive system to RAID\n\nThis article will provide an example of how to install and configure Arch Linux with Logical Volume Manager (LVM) on top of a software RAID.\n\n"
    },
    {
      "title": "Introduction",
      "level": 2,
      "content": "Note: **This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.** This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.\n\nThis article or section needs language, wiki syntax or style improvements. See Help:Style for reference.\n\nAlthough RAID and LVM may seem like analogous technologies they each present unique features. This article uses an example with three similar 1TB SATA hard drives. The article assumes that the drives are accessible as /dev/sda, /dev/sdb, and /dev/sdc.\n\nTable content:\nLVM Logical Volumes | / | /var | /swap | /home\nLVM Volume Groups | /dev/VolGroupArray\nRAID Arrays | /dev/md0 | /dev/md1\nPhysical Partitions | /dev/sda1 | /dev/sdb1 | /dev/sdc1 | /dev/sda2 | /dev/sdb2 | /dev/sdc2\nHard Drives | /dev/sda | /dev/sdb | /dev/sdc\n\n"
    },
    {
      "title": "Swap space",
      "level": 3,
      "content": "Many tutorials treat the swap space differently, either by creating a separate RAID1 array or a LVM logical volume. Creating the swap space on a separate array is not intended to provide additional redundancy, but instead, to prevent a corrupt swap space from rendering the system inoperable, which is more likely to happen when the swap space is located on the same partition as the root directory.\n\n"
    },
    {
      "title": "Boot loader",
      "level": 3,
      "content": "This tutorial will use Syslinux instead of GRUB. GRUB when used in conjunction with GPT requires an additional BIOS boot partition.\n\nGRUB supports the default style of metadata currently created by mdadm (i.e. 1.2) when combined with an initramfs, which has replaced in Arch Linux with mkinitcpio. Syslinux only supports version 1.0, and therefore requires the --metadata=1.0 option.\n\nSome boot loaders (e.g. GRUB Legacy, LILO) will not support any 1.x metadata versions, and instead require the older version, 0.90. If you would like to use one of those boot loaders make sure to add the option --metadata=0.90 to the /boot array during RAID installation.\n\n"
    },
    {
      "title": "Installation",
      "level": 2,
      "content": "Obtain the latest installation media and boot the Arch Linux installer as outlined in the installation guide.\n\n"
    },
    {
      "title": "Load kernel modules",
      "level": 3,
      "content": "Load the appropriate RAID (e.g. raid0, raid1, raid5, raid6, raid10) and LVM (i.e. dm-mod) modules. The following example makes use of RAID1 and RAID5.\n\n```\n# modprobe raid1\n# modprobe raid5\n# modprobe dm-mod\n```\n\n"
    },
    {
      "title": "Prepare the hard drives",
      "level": 3,
      "content": "Partition each hard drive with a 1 GiB /boot partition, a 4 GiB /swap partition, and a / partition that takes up the remainder of the disk.\n\nThe boot partition must be RAID1; i.e it cannot be striped (RAID0) or RAID5, RAID6, etc.. This is because GRUB does not have RAID drivers. Any other level will prevent your system from booting. Additionally, if there is a problem with one boot partition, the boot loader can boot normally from the other two partitions in the /boot array.\n\n"
    },
    {
      "title": "RAID installation",
      "level": 3,
      "content": "After creating the physical partitions, you are ready to setup the /boot, /swap, and / arrays with mdadm. It is an advanced tool for RAID management that will be used to create a /etc/mdadm.conf within the installation environment.\n\nCreate the / array at /dev/md0:\n\n```\n# mdadm --create /dev/md0 --level=5 --raid-devices=3 /dev/sd[abc]3\n```\n\nCreate the /swap array at /dev/md1:\n\n```\n# mdadm --create /dev/md1 --level=1 --raid-devices=3 /dev/sd[abc]2\n```\n\n- If the only reason you are using RAID is to prevent stored data loss (i.e. you are not concerned about some running applications crashing in the event of a disk failure), then there is no reason to RAID the swap partitions -- you can use them as multiple individual swap partitions.\n- If you plan on installing a boot loader that does not support the 1.x version of RAID metadata make sure to add the --metadata=0.90 option to the following command.\n\nCreate the /boot array at /dev/md2:\n\n```\n# mdadm --create /dev/md2 --level=1 --raid-devices=3 --metadata=1.0 /dev/sd[abc]1\n```\n\n"
    },
    {
      "title": "Synchronization",
      "level": 4,
      "content": "After you create a RAID volume, it will synchronize the contents of the physical partitions within the array. You can monitor the progress by refreshing the output of /proc/mdstat ten times per second with:\n\n```\n# watch -n .1 cat /proc/mdstat\n```\n\nFurther information about the arrays is accessible with:\n\n```\n# mdadm --misc --detail /dev/md[012]\n```\n\nOnce synchronization is complete the State line should read clean. Each device in the table at the bottom of the output should read spare or active sync in the State column. active sync means each device is actively in the array.\n\n"
    },
    {
      "title": "Scrubbing",
      "level": 4,
      "content": "It is good practice to regularly run data scrubbing to check for and fix errors.\n\nTo initiate a data scrub:\n\n```\n# echo check > /sys/block/md0/md/sync_action\n```\n\nAs with many tasks/items relating to mdadm, the status of the scrub can be queried:\n\n```\n# cat /proc/mdstat\n```\n\nExample:\n\n```\n$ cat /proc/mdstat\n```\n\n```\nPersonalities : [raid6] [raid5] [raid4] [raid1] \nmd0 : active raid1 sdb1[0] sdc1[1]\n      3906778112 blocks super 1.2 [2/2] [UU]\n      [>....................]  check =  4.0% (158288320/3906778112) finish=386.5min speed=161604K/sec\n      bitmap: 0/30 pages [0KB], 65536KB chunk\n```\n\nTo stop a currently running data scrub safely:\n\n```\n# echo idle > /sys/block/md0/md/sync_action\n```\n\nWhen the scrub is complete, admins may check how many blocks (if any) have been flagged as bad:\n\n```\n# cat /sys/block/md0/md/mismatch_cnt\n```\n\nThe check operation scans the drives for bad sectors and mismatches. Bad sectors are automatically repaired. If it finds mismatches, i.e., good sectors that contain bad data (the data in a sector does not agree with what the data from another disk indicates that it should be, for example the parity block + the other data blocks would cause us to think that this data block is incorrect), then no action is taken, but the event is logged (see below). This \"do nothing\" allows admins to inspect the data in the sector and the data that would be produced by rebuilding the sectors from redundant information and pick the correct data to keep.\n\nIt is a good idea to set up a cron job as root to schedule a periodic scrub. See raid-checkAUR which can assist with this.\n\nDue to the fact that RAID1 and RAID10 writes in the kernel are unbuffered, an array can have non-0 mismatch counts even when the array is healthy. These non-0 counts will only exist in transient data areas where they do not pose a problem. However, since we cannot tell the difference between a non-0 count that is just in transient data or a non-0 count that signifies a real problem. This fact is a source of false positives for RAID1 and RAID10 arrays. It is however recommended to still scrub to catch and correct any bad sectors there might be in the devices.\n\n"
    },
    {
      "title": "LVM installation",
      "level": 3,
      "content": "This section will convert the two RAIDs into physical volumes (PVs). Then combine those PVs into a volume group (VG). The VG will then be divided into logical volumes (LVs) that will act like physical partitions (e.g. /, /var, /home). If you did not understand that make sure you read the LVM Introduction section.\n\n"
    },
    {
      "title": "Create physical volumes",
      "level": 4,
      "content": "Make the RAIDs accessible to LVM by converting them into physical volumes (PVs) using the following command. Repeat this action for each of the RAID arrays created above.\n\n```\n# pvcreate /dev/md0\n```\n\nConfirm that LVM has added the PVs with:\n\n```\n# pvdisplay\n```\n\n"
    },
    {
      "title": "Create the volume group",
      "level": 4,
      "content": "Next step is to create a volume group (VG) on the PVs.\n\nCreate a volume group (VG) with the first PV:\n\n```\n# vgcreate VolGroupArray /dev/md0\n```\n\nConfirm that LVM has added the VG with:\n\n```\n# vgdisplay\n```\n\n"
    },
    {
      "title": "Create logical volumes",
      "level": 4,
      "content": "In this example we will create separate /, /var, /swap, /home LVs. The LVs will be accessible as /dev/VolGroupArray/<lvname>.\n\nCreate a / LV:\n\n```\n# lvcreate -L 20G VolGroupArray -n lvroot\n```\n\nCreate a /var LV:\n\n```\n# lvcreate -L 15G VolGroupArray -n lvvar\n```\n\n```\n# lvcreate -C y -L 2G VolGroupArray -n lvswap\n```\n\nCreate a /home LV that takes up the remainder of space in the VG:\n\n```\n# lvcreate -l 100%FREE VolGroupArray -n lvhome\n```\n\nConfirm that LVM has created the LVs with:\n\n```\n# lvdisplay\n```\n\n"
    },
    {
      "title": "Update RAID configuration",
      "level": 3,
      "content": "Since the installer builds the initrd using /etc/mdadm.conf in the target system, you should update that file with your RAID configuration. The original file can simply be deleted because it contains comments on how to fill it correctly, and that is something mdadm can do automatically for you. So let us delete the original and have mdadm create you a new one with the current setup:\n\n```\n# mdadm --examine --scan >> /etc/mdadm.conf\n```\n\n"
    },
    {
      "title": "Prepare hard drive",
      "level": 3,
      "content": "Follow the directions outlined the in #Installation section until you reach the Prepare Hard Drive section. Skip the first two steps and navigate to the Manually Configure block devices, filesystems and mountpoints page. Remember to only configure the PVs (e.g. /dev/VolGroupArray/lvhome) and not the actual disks (e.g. /dev/sda1).\n\n"
    },
    {
      "title": "mkinitcpio.conf",
      "level": 4,
      "content": "mkinitcpio can use a hook to assemble the arrays on boot. For more information see mkinitcpio Using RAID. Add the mdadm_udev and lvm2 hooks to the HOOKS array in /etc/mkinitcpio.conf after udev.\n\n"
    },
    {
      "title": "Conclusion",
      "level": 3,
      "content": "Once it is complete you can safely reboot your machine:\n\n```\n# reboot\n```\n\n"
    },
    {
      "title": "Install the bootloader on the Alternate Boot Drives",
      "level": 3,
      "content": "Once you have successfully booted your new system for the first time, you will want to install the bootloader onto the other two disks (or on the other disk if you have only 2 HDDs) so that, in the event of disk failure, the system can be booted from any of the remaining drives (e.g. by switching the boot order in the BIOS). The method depends on the bootloader system you are using:\n\n"
    },
    {
      "title": "Syslinux",
      "level": 4,
      "content": "Log in to your new system as root and do:\n\n```\n# /usr/sbin/syslinux-install_update -iam\n```\n\nSyslinux will deal with installing the bootloader to the MBR on each of the members of the RAID array:\n\n```\nDetected RAID on /boot - installing Syslinux with --raid\nSyslinux install successful\n```\n\n```\nAttribute Legacy Bios Bootable Set - /dev/sda1\nAttribute Legacy Bios Bootable Set - /dev/sdb1\nInstalled MBR (/usr/lib/syslinux/gptmbr.bin) to /dev/sda\nInstalled MBR (/usr/lib/syslinux/gptmbr.bin) to /dev/sdb\n```\n\n"
    },
    {
      "title": "Archive your filesystem partition scheme",
      "level": 3,
      "content": "Now that you are done, it is worth taking a second to archive off the partition state of each of your drives. This guarantees that it will be trivially easy to replace/rebuild a disk in the event that one fails. See fdisk#Backup and restore partition table.\n\n"
    },
    {
      "title": "Management",
      "level": 2,
      "content": "For further information on how to maintain your software RAID or LVM review the RAID and LVM articles.\n\n"
    },
    {
      "title": "See also",
      "level": 2,
      "content": "- What is better LVM on RAID or RAID on LVM? on Server Fault\n- Managing RAID and LVM with Linux (v0.5) by Gregory Gulik\n- 2011-09-08 - Arch Linux - LVM & RAID (1.2 metadata) + SYSLINUX\n- 2011-04-20 - Arch Linux - Software RAID and LVM questions\n- 2011-03-12 - Arch Linux - Some newbie questions about installation, LVM, grub, RAID\n\n"
    }
  ]
}