{
  "title": "Apache Spark",
  "url": "https://wiki.archlinux.org/title/Apache_Spark",
  "sections": [
    {
      "title": "Introduction",
      "level": 1,
      "content": "Related articles\n\n- Hadoop\n\nApache Spark is an open-source cluster computing framework originally developed in the AMPLab at UC Berkeley. In contrast to Hadoop's two-stage disk-based MapReduce paradigm, Spark's in-memory primitives provide performance up to 100 times faster for certain applications. By allowing user programs to load data into a cluster's memory and query it repeatedly, Spark is well-suited to machine learning algorithms.\n\n"
    },
    {
      "title": "Installation",
      "level": 2,
      "content": "Install the apache-sparkAUR package.\n\n"
    },
    {
      "title": "Configuration",
      "level": 2,
      "content": "Some environment variables are set in /etc/profile.d/apache-spark.sh.\n\nTable content:\nENV | Value | Description\nPATH | $PATH:/opt/apache-spark/bin | Spark binaries\n\nYou may need to adjust your PATH environment variable if your shell inhibits /etc/profile.d:\n\n```\nexport PATH=$PATH:/opt/apache-spark/bin\n```\n\n"
    },
    {
      "title": "Enable R support",
      "level": 2,
      "content": "The R package sparkR is distributed with the package but not built during installation. To connect to Spark from R you must first build the package by running\n\n```\n# $SPARK_HOME/R/install-dev.sh\n```\n\nas described in $SPARK_HOME/R/README.md. You may also wish to build the package documentation following the instructions in $SPARK_HOME/R/DOCUMENTATION.md.\n\nOnce the sparkR R package has been built you can connect using /usr/bin/sparkR.\n\n"
    }
  ]
}