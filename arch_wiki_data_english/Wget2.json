{
  "title": "Wget2",
  "url": "https://wiki.archlinux.org/title/Wget2",
  "sections": [
    {
      "title": "Introduction",
      "level": 1,
      "content": "GNU Wget is a free software package for retrieving files using HTTP, HTTPS, FTP and FTPS (FTPS since version 1.18). It is a non-interactive commandline tool, so it may easily be called from scripts.\n\n"
    },
    {
      "title": "wget 1.x",
      "level": 3,
      "content": "Install the wget package. The git version is present in the AUR by the name wget-gitAUR.\n\n"
    },
    {
      "title": "wget2",
      "level": 3,
      "content": "An alternative with near identical command line syntax is wget2, which: \"is the successor of GNU Wget\" and is \"Designed and written from scratch\". This rewrite \"works multi-threaded and uses many features to allow fast operation. In many cases Wget2 downloads much faster than Wget1.x due to HTTP2, HTTP compression, parallel connections and use of If-Modified-Since HTTP header.\" [1]\n\nIn order to use wget2, install wget2AUR from the AUR. Note that this has a makedepend on pandoc, which is dependency heavy; you can alternatively install wget2-no-docsAUR also in the AUR, to avoid this makedepend.\n\n"
    },
    {
      "title": "Configuration",
      "level": 2,
      "content": "Configuration is performed in /etc/wgetrc. Not only is the default configuration file well documented; altering it is seldom necessary. See wget(1) ยง OPTIONS for more intricate options.\n\n"
    },
    {
      "title": "FTP automation",
      "level": 3,
      "content": "Normally, SSH is used to securely transfer files among a network. However, FTP is lighter on resources compared to scp and rsyncing over SSH. FTP is not secure, but when transfering large amounts of data inside a firewall protected environment on CPU-bound systems, using FTP can prove beneficial.\n\n```\nwget ftp://root:somepassword@10.13.X.Y//ifs/home/test/big/\"*.tar\"\n\n3,562,035,200 74.4M/s   in 47s\n```\n\nIn this case, Wget transfered a 3.3 GiB file at 74.4MB/second rate.\n\nIn short, this procedure is:\n\n- scriptable\n- faster than ssh\n- easily used by languages than can substitute string variables\n- globbing capable\n\n"
    },
    {
      "title": "Proxy",
      "level": 3,
      "content": "Wget uses the standard proxy environment variables. See Proxy settings.\n\nTo use the proxy authentication feature:\n\n```\n$ wget --proxy-user \"DOMAIN\\USER\" --proxy-password \"PASSWORD\" URL\n```\n\nProxies that use HTML authentication forms are not covered.\n\n"
    },
    {
      "title": "pacman integration",
      "level": 3,
      "content": "To have pacman automatically use Wget and a proxy with authentication, place the Wget command into /etc/pacman.conf, in the [options] section:\n\n```\nXferCommand = /usr/bin/wget --proxy-user \"domain\\user\" --proxy-password=\"password\" --passive-ftp --quiet --show-progress --continue --output-document=%o %u\n```\n\n"
    },
    {
      "title": "Usage",
      "level": 2,
      "content": "This section explains some of the use case scenarios for Wget.\n\n"
    },
    {
      "title": "Basic usage",
      "level": 3,
      "content": "One of the most basic and common use cases for Wget is to download a file from the internet.\n\n```\n$ wget <url>\n```\n\nWhen you already know the URL of a file to download, this can be much faster than the usual routine downloading it on your browser and moving it to the correct directory manually. Needless to say, just from the simplest usage, you can probably see a few ways of utilising this for some automated downloading if that's what you want.\n\n"
    },
    {
      "title": "Archive a complete website",
      "level": 3,
      "content": "Wget can archive a complete website whilst preserving the correct link destinations by changing absolute links to relative links.\n\n```\n$ wget --recursive --no-parent --convert-links 'target-url-here'\n```\n\nIn case of a dynamic website, some additional options for conversion into static HTML are available.\n\n```\n$ wget --recursive --no-parent --page-requisites --adjust-extension --convert-links --backup-converted 'target-url-here'\n```\n\nwget also provides options for bypassing download-prevention mechanisms.\n\n```\n$ wget --recursive --no-parent --convert-links --random-wait --execute robots=off --user-agent \"Mozilla/5.0\" 'target-url-here'\n```\n\nAnd if third-party content is to be included in the download, -H/--span-hosts switch can be used alongside -r/--recursive to recurse to linked hosts.\n\n"
    }
  ]
}