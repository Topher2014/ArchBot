{
  "title": "K8s",
  "url": "https://wiki.archlinux.org/title/K8s",
  "sections": [
    {
      "title": "Introduction",
      "level": 1,
      "content": "Note: **This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.** This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.\n\nThis article or section needs language, wiki syntax or style improvements. See Help:Style for reference.\n\nKubernetes (aka. k8s) is an open-source system for automating the deployment, scaling, and management of containerized applications.\n\nA k8s cluster consists of its control-plane components and node components (each representing one or more host machines running a container runtime and kubelet.service). There are two options to install kubernetes, \"the real one\", described here, and a local install with k3s, kind, or minikube.\n\n"
    },
    {
      "title": "Installation",
      "level": 2,
      "content": "There are many methods to setup a kubernetes cluster. This article will focus on bootstrapping with kubeadm.\n\n"
    },
    {
      "title": "kubeadm",
      "level": 4,
      "content": "When bootstrapping a Kubernetes cluster with kubeadm, install kubeadm and kubelet on each node.\n\n"
    },
    {
      "title": "Manual installation",
      "level": 4,
      "content": "When manually creating a Kubernetes cluster install etcdAUR and the package group kubernetes-control-plane (for a control-plane node) and kubernetes-node (for a worker node).\n\n"
    },
    {
      "title": "Cluster management",
      "level": 4,
      "content": "To control a kubernetes cluster, install kubectl on the control-plane hosts and any external host that is supposed to be able to interact with the cluster.\n\n"
    },
    {
      "title": "Container runtime",
      "level": 3,
      "content": "Both control-plane and regular worker nodes require a container runtime for their kubelet instances which is used for hosting containers. Install either containerd or cri-o to meet this dependency.\n\n"
    },
    {
      "title": "containerd runtime",
      "level": 4,
      "content": "There are two methods available to install containerd:\n\n1. Install the containerd package.\n1. To install a rootless containerd, use nerdctl-full-binAUR, which is a full nerdctl package, bundled with containerd, CNI plugin, and RootlessKit. Rootless containerd can be launched with containerd-rootless-setuptool.sh install.\n\nRemember that Arch Linux uses systemd as its init system, so you need to choose systemd cgroup driver before deploying the control plane(s).\n\n"
    },
    {
      "title": "(Optional) Package manager",
      "level": 3,
      "content": "helm is a tool for managing pre-configured Kubernetes resources which may be helpful for getting started.\n\n"
    },
    {
      "title": "Configuration",
      "level": 2,
      "content": "All nodes in a cluster (control-plane and worker) require a running instance of kubelet.service.\n\nAll provided systemd services accept CLI overrides in environment files:\n\n- kubelet.service: /etc/kubernetes/kubelet.env\n- kube-apiserver.service: /etc/kubernetes/kube-apiserver.env\n- kube-controller-manager.service: /etc/kubernetes/kube-controller-manager.env\n- kube-proxy.service: /etc/kubernetes/kube-proxy.env\n- kube-scheduler.service: /etc/kubernetes/kube-scheduler.env\n\nNote: **This article or section needs expansion.** This article or section needs expansion.\n\nThis article or section needs expansion.\n\n- Example for setup without kubeadm, using kube-apiserver.service, kube-controller-manager.service, kube-proxy.service and kube-scheduler.service.\n- Example for setup with kubeadm using configuration files.\n\n"
    },
    {
      "title": "Disable swap",
      "level": 3,
      "content": "Kubernetes currently does not support having swap enabled on the system. See KEP-2400: Node system swap support for details.\n\nSee Swap#Disabling swap for instructions on how to disable swap.\n\n"
    },
    {
      "title": "Choose cgroup driver for containerd",
      "level": 3,
      "content": "To use the systemd cgroup driver in /etc/containerd/config.toml with runc, set\n\n```\n[plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc]\n  ...\n  [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options]\n    SystemdCgroup = true\n```\n\nIf /etc/containerd/config.toml does not exist, the default configuration can be generated with [1]\n\n```\n# mkdir -p /etc/containerd/\n# containerd config default > /etc/containerd/config.toml\n```\n\nRemember to restart containerd.service to make the change take effect.\n\nSee the official documentation for a deeper discussion on whether to keep cgroupfs driver or use systemd cgroup driver.\n\n"
    },
    {
      "title": "Choose container runtime interface (CRI)",
      "level": 3,
      "content": "A container runtime has to be configured and started, before kubelet.service can make use of it.\n\nYou will pass flag --cri-socket with the container runtime interface endpoint to kubeadm init or kubeadm join in order to create or join a cluster.\n\nFor example, if you choose containerd as CRI runtime, the flag --cri-socket will be:\n\n```\nkubeadm init --cri-socket /run/containerd/containerd.sock\n```\n\n"
    },
    {
      "title": "Containerd",
      "level": 4,
      "content": "Before Kubernetes version 1.27.4, when using containerd as container runtime, it is required to provide kubeadm init or kubeadm join with its CRI endpoint. To do so, specify their flag --cri-socket to /run/containerd/containerd.sock[2].\n\n```\nkubeadm join --cri-socket=/run/containerd/containerd.sock\n```\n\nAfter Kubernetes version 1.27.4, kubeadm will auto detect this CRI for you, flag --cri-socket is only needed when you installed multiple CRI.\n\n"
    },
    {
      "title": "CRI-O",
      "level": 4,
      "content": "When using CRI-O as container runtime, it is required to provide kubeadm init or kubeadm join with its CRI endpoint: --cri-socket='unix:///run/crio/crio.sock'\n\n"
    },
    {
      "title": "Choose a pod CIDR range",
      "level": 4,
      "content": "The networking setup for the cluster has to be configured for the respective container runtime. This can be done using cni-plugins.\n\nThe pod CIDR addresses refer to the IP address range that is assigned to pods within a Kubernetes cluster. When pods are scheduled to run on nodes in the cluster, they are assigned IP addresses from this CIDR range.\n\nThe pod CIDR range is specified when deploying a Kubernetes cluster and is confined within the cluster network. It should not overlap with other IP ranges used within the cluster, such as the service CIDR range.\n\nYou will pass flag --pod-network-cidr with value of the virtual network's CIDR to kubeadm init or kubeadm join in order to create or join a cluster.\n\nFor example:\n\n```\nkubeadm init --pod-network-cidr='10.85.0.0/16'\n```\n\nwill set your kubernetes' pod CIDR range to 10.85.0.0/16.\n\n"
    },
    {
      "title": "(Optional) Choose API server advertising address",
      "level": 4,
      "content": "If your node for control plane is in multiple subnets (for example you may have installed a tailscale tailnet), when initializing the Kubernetes master with kubeadm init, you can specify the IP address that the API server will advertise with the --apiserver-advertise-address flag. This IP address should be accessible to all nodes in your cluster.\n\n"
    },
    {
      "title": "(Optional) Choose alternative node network proxy provider",
      "level": 4,
      "content": "Node proxy provider like kube-proxy is a network proxy that runs on each node in your cluster, maintaining network rules on nodes to allow network communication to your Pods from network sessions inside or outside of your cluster.\n\nBy default kubeadm choose kube-proxy as the node proxy that runs on each node in your cluster.\n\nContainer Network Interface (CNI) plugins like cilium offer a complete replacement for kube-proxy.\n\nIf you want to use cilium's implementation of node network proxy to fully leverage cilium's network policy feature, you should pass flag --skip-phases=addon/kube-proxy to kubeadm init to skip the install of kube-proxy.\n\nCilium will install a full replacement during its installation. See this[3] for details.\n\n"
    },
    {
      "title": "Create cluster",
      "level": 2,
      "content": "Before creating a new kubernetes cluster with kubeadm start and enable kubelet.service.\n\nNote: **This article or section needs expansion.** This article or section needs expansion.\n\nThis article or section needs expansion.\n\n- Example for setup without kubeadm, using kube-apiserver.service, kube-controller-manager.service, kube-proxy.service and kube-scheduler.service.\n- Example for setup with kubeadm using configuration files.\n\n"
    },
    {
      "title": "kubeadm without config",
      "level": 3,
      "content": "When creating a new kubernetes cluster with kubeadm a control-plane has to be created before further worker nodes can join it.\n\n- If the cluster is supposed to be turned into a high availability cluster (a stacked etcd topology) later on kubeadm init needs to be provided with --control-plane-endpoint=<IP or domain> (it is not possible to do this retroactively!).\n- It is possible to use a config file for kubeadm init instead of a set of parameters.\n\n"
    },
    {
      "title": "Initialize control-plane",
      "level": 4,
      "content": "To initialize control-plane, you need pass the following necessary flags to kubeadm init\n\nIf run successfully, kubeadm init will have generated configurations for the kubelet and various control-plane components below /etc/kubernetes/ and /var/lib/kubelet/.\n\nFinally, it will output commands ready to be copied and pasted to setup kubectl and make a worker node join the cluster (based on a token, valid for 24 hours).\n\nTo use kubectl with the freshly created control-plane node, setup the configuration (either as root or as a normal user):\n\n```\n$ mkdir -p $HOME/.kube\n# cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n# chown $(id -u):$(id -g) $HOME/.kube/config\n```\n\n"
    },
    {
      "title": "Installing CNI plugins (pod network addon)",
      "level": 4,
      "content": "Note: **Container Network Interface (CNI)** \n\nPod network add-ons (CNI plugins) implement the Kubernetes network model differently from simple solutions like flannel to more complicated solutions like calico. See this list for more options.\n\nAn increasingly adopted advanced CNI plugin is cilium, which achieves impressive performance with eBPF. To install cilium as CNI plugin, use cilium-cli:\n\n```\n# cilium-cli install\n```\n\nThis will create the /opt/cni/bin/cilium-cni plugin, config file /etc/cni/net.d/05-cilium.conflist and deploy two pods on the Kubernetes cluster.\n\n"
    },
    {
      "title": "kubeadm with config",
      "level": 3,
      "content": "You will most likely find that creating the control plane requires several attempts find the optimal configuration for your particular setup. To make this easier (and the process with kubeadm more repeatable generally), you may run the initialization step using config files.\n\n"
    },
    {
      "title": "Create the init config",
      "level": 4,
      "content": "You can create this file anywhere, but we will go with /etc/kubeadm for this example.\n\n```\n# mkdir -pv /etc/kubeadm\n# cd /etc/kubeadm\n# kubeadm config print init-defaults > init.yaml\n```\n\nThis will produce the following file.\n\n```\n/etc/kubeadm/init.yaml\n```\n\n```\napiVersion: kubeadm.k8s.io/v1beta3\nbootstrapTokens:\n- groups:\n  - system:bootstrappers:kubeadm:default-node-token\n  token: abcdef.0123456789abcdef\n  ttl: 24h0m0s\n  usages:\n  - signing\n  - authentication\nkind: InitConfiguration\nlocalAPIEndpoint:\n  advertiseAddress: 1.2.3.4\n  bindPort: 6443\nnodeRegistration:\n  criSocket: unix:///var/run/containerd/containerd.sock\n  imagePullPolicy: IfNotPresent\n  name: node\n  taints: null\n---\napiServer:\n  timeoutForControlPlane: 4m0s\napiVersion: kubeadm.k8s.io/v1beta3\ncertificatesDir: /etc/kubernetes/pki\nclusterName: kubernetes\ncontrollerManager: {}\ndns: {}\netcd:\n  local:\n    dataDir: /var/lib/etcd\nimageRepository: registry.k8s.io\nkind: ClusterConfiguration\nkubernetesVersion: 1.29.0\nnetworking:\n  dnsDomain: cluster.local\n  serviceSubnet: 10.96.0.0/12\nscheduler: {}\n```\n\nMost of the default settings should work, though you will need to update a few of them.\n\nCreate a token with kubeadm token generate and use it instead of token: abcdef.0123456789abcdef in the config.\n\nThe advertiseAddress: 1.2.3.4 should be an IPv4 address of a network interface on the control plane being initialized, probably something in the 192.168.0.0/16 subnet.\n\nThe default node name can either be left at node and added to your local DNS server or hosts file, or you can change it to an address that is routable on your local network. It should be a DNS-compatible hostname, such as kcp01.example.com. This will allow your control plane to be found on your local network when you join other nodes.\n\n"
    },
    {
      "title": "Init the cluster",
      "level": 4,
      "content": "With all of these changes set, we can initialize our cluster.\n\n```\n# kubeadm init --config /etc/kubeadm/init.yaml\n```\n\nThis will produce a good amount of output that will provide instructions on how to join nodes to the cluster, update your kubeconfig to interact with the new cluster, and other tasks.\n\nThe last thing you need before you can start adding nodes and running workloads is a properly configured CNI. This example will use calico for that.\n\n```\n# cd /etc/cni/net.d\n# curl https://raw.githubusercontent.com/projectcalico/calico/v3.27.2/manifests/calico.yaml -O\n# kubectl create -f calico.yaml\n```\n\nIf this completes successfully, you are ready to start adding nodes and running workloads on your cluster.\n\n"
    },
    {
      "title": "Create the reset config",
      "level": 4,
      "content": "Just in case kubeadm does not land the init the first time, you can also create a config file for use with the reset command:\n\n```\n# kubeadm config print reset-defaults > /etc/kubeadm/reset.yaml\n```\n\nThis will create the following file:\n\n```\n/etc/kubeadm/reset.yaml\n```\n\n```\napiVersion: kubeadm.k8s.io/v1beta4\ncertificatesDir: /etc/kubernetes/pki\ncriSocket: unix:///var/run/containerd/containerd.sock\nkind: ResetConfiguration\n```\n\nTo reset the cluster back to zero, run the following command:\n\n```\n# kubeadm reset --config /etc/kubeadm/reset.yaml\n```\n\nThis can be done as many times as required to sort out your cluster's ideal configuration.\n\n"
    },
    {
      "title": "Create the join config",
      "level": 4,
      "content": "Most likely once you init the cluster you'll be able to join any nodes with the command listed in the output of the init command, but if you happen to run in to trouble it will be helpful to have a join config available on the nodes you're joining. You can either create this file on your control plane, or run the command on nodes that you intend to join to the cluster, we'll assume you did the latter.\n\n```\n# kubeadm config print join-defaults > /etc/kubeadm/join.yaml\n```\n\nThis will create the following file.\n\n```\n/etc/kubeadm/join.yaml\n```\n\n```\napiVersion: kubeadm.k8s.io/v1beta3\ncaCertPath: /etc/kubernetes/pki/ca.crt\ndiscovery:\n  bootstrapToken:\n    apiServerEndpoint: kcp01.example.com:6443\n    token: abcdef.0123456789abcdef\n    unsafeSkipCAVerification: true\n  timeout: 5m0s\n  tlsBootstrapToken: abcdef.0123456789abcdef\nkind: JoinConfiguration\nnodeRegistration:\n  criSocket: unix:///var/run/containerd/containerd.sock\n  imagePullPolicy: IfNotPresent\n  name: node01.example.com\n  taints: null\n```\n\n"
    },
    {
      "title": "Join cluster",
      "level": 2,
      "content": "With the token information generated in #Create cluster it is possible to make another machine join the cluster as worker node with command kubeadm join.\n\nRemember you need to choose a container runtime interface for working nodes as well by passing flag <SOCKET> to command kubeadm join.\n\nFor example:\n\n```\n# kubeadm join <api-server-ip>:<port> --token <token> --discovery-token-ca-cert-hash sha256:<hash> --node-name=<name_of_the_node> --cri-socket=<SOCKET>\n```\n\nTo generate new bootstrap token,\n\n```\nkubeadm token create --print-join-command\n```\n\nIf you are using Cilium and find the working node remains to be NotReady, check the status of working node using:\n\n```\nkubectl describe node <node-id> --namespace=kube-system\n```\n\nIf you found the following condition status:\n\n```\nType                  Status       Reason\n----                  ------       ------\nNetworkUnavailable    False        CiliumIsUp\nReady                 False        KubeletNotReady container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\n```\n\nRestart containerd.service and kubelet.service on the working node\n\n"
    },
    {
      "title": "Tear down a cluster",
      "level": 3,
      "content": "When it is necessary to start from scratch, use kubectl to tear down a cluster.\n\n```\nkubectl drain <node name> --delete-local-data --force --ignore-daemonsets\n```\n\nHere <node name> is the name of the node that should be drained and reset. Use kubectl get node -A to list all nodes.\n\nThen reset the node:\n\n```\n# kubeadm reset\n```\n\n"
    },
    {
      "title": "Operating from behind a proxy",
      "level": 3,
      "content": "kubeadm reads the https_proxy, http_proxy, and no_proxy environment variables. Kubernetes internal networking should be included in the latest one, for example\n\n```\nexport no_proxy=\"192.168.122.0/24,10.96.0.0/12,192.168.123.0/24\"\n```\n\nwhere the second one is the default service network CIDR.\n\n"
    },
    {
      "title": "Failed to get container stats",
      "level": 3,
      "content": "If kubelet.service emits\n\n```\nFailed to get system container stats for \"/system.slice/kubelet.service\": failed to get cgroup stats for \"/system.slice/kubelet.service\": failed to get container info for \"/system.slice/kubelet.service\": unknown container \"/system.slice/kubelet.service\"\n```\n\nit is necessary to add configuration for the kubelet (see relevant upstream ticket).\n\n```\n/var/lib/kubelet/config.yaml\n```\n\n```\nsystemCgroups: '/systemd/system.slice'\nkubeletCgroups: '/systemd/system.slice'\n```\n\n"
    },
    {
      "title": "Pods cannot communicate when using Flannel CNI and systemd-networkd",
      "level": 3,
      "content": "See upstream bug report.\n\nsystemd-networkd assigns a persistent MAC address to every link. This policy is defined in its shipped configuration file /usr/lib/systemd/network/99-default.link. However, Flannel relies on being able to pick its own MAC address. To override systemd-networkd's behaviour for flannel* interfaces, create the following configuration file:\n\n```\n/etc/systemd/network/50-flannel.link\n```\n\n```\n[Match]\nOriginalName=flannel*\n\n[Link]\nMACAddressPolicy=none\n```\n\nThen restart systemd-networkd.service.\n\nIf the cluster is already running, you might need to manually delete the flannel.1 interface and the kube-flannel-ds-* pod on each node, including the master. The pods will be recreated immediately and they themselves will recreate the flannel.1 interfaces.\n\nDelete the interface flannel.1:\n\n```\n# ip link delete flannel.1\n```\n\nDelete the kube-flannel-ds-* pod. Use the following command to delete all kube-flannel-ds-* pods on all nodes:\n\n```\n$ kubectl -n kube-system delete pod -l=\"app=flannel\"\n```\n\n"
    },
    {
      "title": "CoreDNS Pod pending forever, the control plane node remains \"NotReady\"",
      "level": 3,
      "content": "When bootstrap the Kubernetes with kubeadm init on a single machine, and there is no other machine kubeadm join the cluster, the control-plane node is default to be tainted. As a result, no workload will be scheduled on the working machine.\n\nOne can confirm the control-plane node is tainted by the following commands:\n\n```\nkubectl get nodes -o json | jq '.items[].spec.taints\n```\n\nTo temporarily allow scheduling on the control-plane node, execute:\n\n```\nkubectl taint nodes <your-node-name> node-role.kubernetes.io/control-plane:NoSchedule-\n```\n\nThen restart containerd.service and kubelet.service to apply the updates.\n\n"
    },
    {
      "title": "[kubelet-finalize] malformed header: missing HTTP content-type",
      "level": 3,
      "content": "You may have forgotten to choose systemd cgroup driver. See kubeadm issue 2767 reporting this.\n\n"
    },
    {
      "title": "CoreDNS Pod does not start due to loops",
      "level": 3,
      "content": "When the host node runs a local DNS cache such as systemd-resolved, the CoreDNS may fail to start due to detecting a forwarding loop. This can be checked as follows:\n\n```\n# kubectl get pods -n kube-system\n```\n\n```\nNAME                               READY   STATUS             RESTARTS      AGE\ncilium-jc98m                       1/1     Running            0             21m\ncilium-operator-64664858c8-zjzcq   1/1     Running            0             21m\ncoredns-7db6d8ff4d-29zfg           0/1     CrashLoopBackOff   6 (41s ago)   21m\ncoredns-7db6d8ff4d-zlvsm           0/1     CrashLoopBackOff   6 (50s ago)   21m\netcd-k8s                           1/1     Running            19            21m\nkube-apiserver-k8s                 1/1     Running            17            21m\nkube-controller-manager-k8s        1/1     Running            16            21m\nkube-proxy-cvntt                   1/1     Running            0             21m\nkube-scheduler-k8s                 1/1     Running            23            21m\n```\n\n```\n# kubectl logs -n kube-system coredns-7db6d8ff4d-29zfg\n```\n\n```\n...\n[FATAL] plugin/loop: Loop ([::1]:46171 -> :53) detected for zone \".\", see https://coredns.io/plugins/loop#troubleshooting. Query: \"HINFO 64811921068182325.3042126689798234092.\"\n```\n\nThis is caused by kubelet passing the host /etc/resolv.conf file, to all Pods using the default dnsPolicy. CoreDNS uses this /etc/resolv.conf as a list of upstreams to forward requests to. Since it contains a loopback address such as 127.0.0.53, CoreDNS ends up forwarding requests to itself.\n\nSee https://coredns.io/plugins/loop/#troubleshooting to resolve the issue.\n\n"
    },
    {
      "title": "See also",
      "level": 2,
      "content": "- Kubernetes Documentation - The upstream documentation\n- Kubernetes Cluster with Kubeadm - Upstream documentation on how to setup a Kubernetes cluster using kubeadm\n- Kubernetes Glossary - The official glossary explaining all Kubernetes specific terminology\n- Kubernetes Addons - A list of third-party addons\n- Kubelet Config File - Documentation on the Kubelet configuration file\n- Taints and Tolerations - Documentation on node affinities and taints\n\n"
    }
  ]
}