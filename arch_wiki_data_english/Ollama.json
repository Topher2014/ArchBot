{
  "title": "Ollama",
  "url": "https://wiki.archlinux.org/title/Ollama",
  "sections": [
    {
      "title": "Introduction",
      "level": 1,
      "content": "Ollama is an application which lets you run offline large language models locally.\n\n"
    },
    {
      "title": "Installation",
      "level": 2,
      "content": "- Install ollama to run models on CPU\n- To run models on GPU: Install ollama-cuda for NVIDIA Install ollama-rocm for AMD.\n\n- Install ollama-cuda for NVIDIA\n- Install ollama-rocm for AMD.\n\nNext, enable/start ollama.service. Then, verify Ollama's status:\n\n```\n$ ollama --version\n```\n\nIf it says Warning: could not connect to a running Ollama instance, then the Ollama service has not been run; otherwise, the Ollama service is running and is ready to accept user requests.\n\nNext, verify that you can run models. The following command downloads the latest Distilled DeepSeek-R1 model and returns an Ollama prompt that allows you to talk to the model:\n\n```\n$ ollama run deepseek-r1:1.5b\n```\n\n```\n>>> Send a message (/? for help)\n```\n\n"
    },
    {
      "title": "Usage",
      "level": 2,
      "content": "The Ollama executable does not provide a search interface. There is no such command as ollama search. To search for a model, you need to visit their search page.\n\nTo run a model:\n\n```\n$ ollama run model\n```\n\nTo stop a model:\n\n```\n$ ollama stop model\n```\n\nTo update a model:\n\n```\n$ ollama pull model\n```\n\nTo remove a model:\n\n```\n$ ollama rm model\n```\n\n"
    },
    {
      "title": "ROCm is not utilizing my AMD integrated GPU",
      "level": 3,
      "content": "You may have used utilities like amdgpu_top to monitor the utilization of your integrated GPU during an Ollama session, but only to notice that your integrated GPU has not been used at all.\n\nThat is expected: without configuration, ROCm simply ignores your integrated GPU, causing everything to be computed on CPU.\n\nThe required configuration is, however, very simple because all you need is to create a drop-in file for ollama.service:\n\n```\n/etc/systemd/system/ollama.service.d/override_gfx_version.conf\n```\n\n```\n[Service]\nEnvironment=\"HSA_OVERRIDE_GFX_VERSION=X.Y.Z\"\n```\n\nWhere X.Y.Z is dependent to the GFX version that is shipped with your system.\n\nTo determine which GFX version to use, first make sure rocminfo has already been installed. It should be pulled in to your system as a dependency of rocblas, which is itself a dependency of ollama-rocm.\n\nNext, query the actual GFX version of your system:\n\n```\n$ /opt/rocm/bin/rocminfo | grep amdhsa\n```\n\nYou need to remember the digits printed after the word gfx, because this is the actual GFX version of your system. The digits are interpreted as follows:\n\n- If the digits are 4-digit, they are interpreted as XX.Y.Z, where the first two digits are interpreted as the X part.\n- If the digits are 3-digit, they are interpreted as X.Y.Z.\n\nThen, find all installed rocblas kernels:\n\n```\n$ find /opt/rocm/lib/rocblas/library -name 'Kernels.so-*'\n```\n\nYou need to set X.Y.Z to one of the available versions listed there. The rules are summarized as follows:\n\n1. For the X part, it must be strictly equal to the actual version.\n1. For the Y part, mismatch is allowed, but it must be no greater than the actual version.\n1. For the Z part, mismatch is allowed, but it must be no greater than the actual version.\n\nAfter setting the correct X.Y.Z, perform a daemon-reload and restart ollama.service.\n\nThen, run your model as usual. You may wish to monitor GPU utilization with amdgpu_top again.\n\n"
    },
    {
      "title": "See also",
      "level": 2,
      "content": "- Ollama Blog\n- Ollama Docs\n- What is rocBLAS\n\n"
    }
  ]
}