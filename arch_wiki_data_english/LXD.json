{
  "title": "LXD",
  "url": "https://wiki.archlinux.org/title/LXD",
  "sections": [
    {
      "title": "Introduction",
      "level": 1,
      "content": "Related articles\n\n- Linux Containers\n- Incus\n\nNote: **This article or section needs expansion.** This article or section needs expansion.\n\nThis article or section needs expansion.\n\nLXD is a manager/hypervisor by Canonical for containers (via LXC) and virtual-machines (via QEMU).\n\n"
    },
    {
      "title": "Installation",
      "level": 2,
      "content": "Install the lxd package, then enable the lxd.socket.\n\nAlternatively, you can enable/start the lxd.service directly, in case you want instances to autostart for example.\n\n"
    },
    {
      "title": "Unprivileged containers",
      "level": 3,
      "content": "It is recommended to use unprivileged containers (See Linux Containers#Privileged or unprivileged containers for an explanation of the difference).\n\nFor this, modify both /etc/subuid and /etc/subgid (if these files are not present, create them) to contain the mapping to the containerized uid/gid pairs for each user who shall be able to run the containers. The example below is simply for the root user (and systemd system unit):\n\n```\n# usermod -v 1000000-1000999999 -w 1000000-1000999999 root\n```\n\nNow, every container will be started unprivileged by default.\n\nFor the alternative, see #Privileged containers.\n\n"
    },
    {
      "title": "Configure LXD",
      "level": 3,
      "content": "On the first start, LXD needs to be configured.\n\nRun as root:\n\n```\n# lxd init\n```\n\nThis will start an interactive configuration guide in the terminal, that covers different topics like storages, networks etc.\n\nYou can find an overview in the official Getting Started Guide.\n\n"
    },
    {
      "title": "Accessing LXD as an unprivileged user",
      "level": 3,
      "content": "By default, the LXD daemon allows users in the lxd group access, so add your user to this group.\n\n"
    },
    {
      "title": "Usage",
      "level": 2,
      "content": "LXD consists of two parts:\n\n- the daemon (the lxd binary)\n- the client (the lxc binary)\n\nThe client is used to control one or multiple daemon(s).\n\nThe client can also be used to control remote LXD servers.\n\n"
    },
    {
      "title": "Overview of commands",
      "level": 3,
      "content": "You can get an overview of all available commands by typing:\n\n```\n$ lxc\n```\n\n"
    },
    {
      "title": "Create a container",
      "level": 3,
      "content": "You can create a container with lxc launch, for example:\n\n```\n$ lxc launch ubuntu:20.04\n```\n\nContainer are based on images, that are downloaded from image servers or remote LXD servers.\n\nYou can see the list of already added servers with:\n\n```\n$ lxc remote list\n```\n\nYou can list all images on a server with lxc image list, for example:\n\n```\n$ lxc image list images:\n```\n\nThis will show you all images on one of the default servers: images.linuxcontainers.org\n\nYou can also search for images by adding terms like the distribution name:\n\n```\n$ lxc image list images:debian\n```\n\nLaunch a container with an image from a specific server with:\n\n```\n$ lxc launch servername:imagename\n```\n\nFor example:\n\n```\n$ lxc launch images:centos/8/amd64 centos\n```\n\nTo create an amd64 Arch container:\n\n```\n$ lxc launch images:archlinux/current/amd64 arch\n```\n\n"
    },
    {
      "title": "Create a virtual machine",
      "level": 3,
      "content": "Just add the flag --vm to lxc launch:\n\n```\n$ lxc launch ubuntu:20.04 --vm\n```\n\n- For now, virtual machines support less features than containers (see Details on virtual machines for example).\n- Only cloud variants of the official images enable the lxd-agent out-of-the-box (which is needed for the usual lxc commands like lxc exec). You can search for cloud images with lxc image list images: cloud or lxc image list images: distribution-name cloud. If you use other images or encounter problems, take a look at #lxd-agent inside a virtual machine.\n\n"
    },
    {
      "title": "Use and manage a container or VM",
      "level": 3,
      "content": "See \"Manage instances\" in the official Getting Started Guide of LXD.\n\n"
    },
    {
      "title": "Container/VM configuration",
      "level": 3,
      "content": "You can add various options to instances (containers and VMs).\n\nSee Configuration of instances in the official Advanced Guide of LXD for details.\n\n"
    },
    {
      "title": "Access the containers by name on the host",
      "level": 3,
      "content": "This assumes that you are using the default bridge that it is named lxdbr0 and that you are using systemd-resolved.\n\n```\n# systemd-resolve --interface lxdbr0 --set-domain '~lxd' --set-dns $(lxc network get lxdbr0 ipv4.address | cut -d / -f 1)\n```\n\nYou can now access the containers by name:\n\n```\n$ ping containername.lxd\n```\n\n"
    },
    {
      "title": "Other solution",
      "level": 4,
      "content": "Note: **The factual accuracy of this article or section is disputed.** The factual accuracy of this article or section is disputed.\n\nThe factual accuracy of this article or section is disputed.\n\nIt seems that the systemd-resolve solution stops working after some time.\n\nAnother solution is to use systemd-networkd with the following lxd.network (replace x and y to match your bridge IP):\n\n```\n/etc/systemd/network/lxd.network\n```\n\n```\n[Match]\nName=lxdbr0\n\n[Network]\nDNS=10.x.y.1\nDomains=~lxd\nIgnoreCarrierLoss=yes\n\n[Address]\nAddress=10.x.y.1/24\nGateway=10.x.y.1\n```\n\n"
    },
    {
      "title": "Use Wayland and Xorg applications",
      "level": 3,
      "content": "There are multiple methods to use GUI applications inside containers, you can find an overview in the official LXD forum.\n\nThe following method grants containers access to the host's sockets of Wayland (+Xwayland) or Xorg.\n\n"
    },
    {
      "title": "Add the following devices to a containers profile",
      "level": 4,
      "content": "See also LXD documentation regarding devices.\n\nGeneral device for the GPU:\n\n```\nmygpu:\n   type: gpu\n```\n\nDevice for the Wayland socket:\n\n- Adjust the Display (wayland-0) accordingly.\n- Add the directories in /mnt and /tmp inside the container, if they do not already exist.\n\n```\nWaylandsocket:\n    bind: container\n    connect: unix:/run/user/1000/wayland-0\n    listen: unix:/mnt/wayland1/wayland-0\n    uid: \"1000\"\n    gid: \"1000\"\n    security.gid: \"1000\"\n    security.uid: \"1000\"\n    mode: \"0777\"\n    type: proxy\n```\n\nDevice for the Xorg (or Xwayland) Socket:\n\n```\nXsocket:\n    bind: container\n    connect: unix:/tmp/.X11-unix/X0\n    listen: unix:/mnt/xorg1/X0\n    uid: \"1000\"\n    gid: \"1000\"\n    security.gid: \"1000\"\n    security.uid: \"1000\"\n    mode: \"0777\"\n    type: proxy\n```\n\n"
    },
    {
      "title": "Link the sockets to the right location inside the container",
      "level": 4,
      "content": "Shell script to link the Wayland socket:\n\n```\n#!/bin/sh\nmkdir /run/user/1000\nln -s /mnt/wayland1/wayland-0 /run/user/1000/wayland-0\n```\n\nLink the Xorg (or Xwayland) socket:\n\n```\n#!/bin/sh\nln -s /mnt/xorg1/X0 /tmp/.X11-unix/X0\n```\n\n"
    },
    {
      "title": "Add environment variables to the users config inside the container",
      "level": 4,
      "content": "For Wayland:\n\n```\n$ echo \"export XDG_RUNTIME_DIR=/run/user/1000\" >> ~/.profile\n$ echo \"export WAYLAND_DISPLAY=wayland-0\" >> ~/.profile\n$ echo \"export QT_QPA_PLATFORM=wayland\" >> ~/.profile\n```\n\nFor Xorg (or Xwayland):\n\n```\n$ echo \"export DISPLAY=:0\" >> ~/.profile\n```\n\nReload the ~/.profile:\n\n```\n$ source ~/.profile\n```\n\n"
    },
    {
      "title": "Install necessary software in the container",
      "level": 4,
      "content": "Necessary software needs to be added. For now, you can install an example GUI application; this will probably install all necessary packages as well.\n\n"
    },
    {
      "title": "Start GUI applications",
      "level": 4,
      "content": "Now, you should be able to start GUI applications inside the container (via terminal for example) and make them appear as a window on your hosts display.\n\nYou can try out glxgears for example.\n\n"
    },
    {
      "title": "Privileged containers",
      "level": 3,
      "content": "- Privileged containers are not isolated from the host!\n- The root user in the container is the root user on the host.\n- Use unprivileged containers instead whenever possible.\n\nIf you want to set up a privileged container, you must provide the config key security.privileged=true.\n\nEither during container creation:\n\n```\n$ lxc launch ubuntu:20.04 ubuntu -c security.privileged=true\n```\n\nOr for an already existing container, you may edit the configuration:\n\n```\n$ lxc config edit ubuntu\n```\n\n```\nname: ubuntu\nprofiles:\n- default\nconfig:\n  ...\n  security.privileged: \"true\"\n  ...\n```\n\n"
    },
    {
      "title": "Read-Only",
      "level": 4,
      "content": "If you want to share a disk device from the host to a container, all you need to do is add a disk device to your container. The virtual disk device needs a name (only used internally in the LXC configuration file), a path on the host's filesystem pointing to the disk you want to mount, as well as a desired mountpoint on the container's filesystem.\n\n```\n$ lxc config device add containername virtualdiskname disk source=/path/to/host/disk/ path=/path/to/mountpoint/on/container\n```\n\n"
    },
    {
      "title": "Read-Write (unprivileged container)",
      "level": 4,
      "content": "The preferred method for read/write access is to use the \"shift\" method included in LXD.\n\nshift is based on Linux kernel functionality and available in two different versions:\n\n- the most recent version is called \"idmapped mounts\" and is included in all upstream kernels >5.12 by default. So it is also included in the regular Arch Linux kernel (linux).\n- the old version is called \"shiftfs\" and needs to be added manually to most kernels as a kernel module. It is available as a legacy version to support older kernels. You can take a look at this GitHub repo that uses the shiftfs kernel module from Ubuntu kernels: https://github.com/toby63/shiftfs-dkms\n\nShift should be available and activated by default on Arch with the regular Arch Linux kernel (linux) and the lxd package.\n\n1. To check whether shift is available on your system, run lxc info\n\nThe first part of the output shows you:\n\n```\nkernel_features:\n    idmapped_mounts: \"true\"\n    shiftfs: \"false\"\n```\n\nIf either idmapped_mounts or shiftfs is true, then your kernel includes it already and you can use shift. If it is not true, you should check your kernel version and might try the \"shiftfs\" legacy version mentioned above.\n\nThe second part of the output shows you either:\n\n```\nlxc_features:\n    idmapped_mounts_v2: \"true\"\n```\n\nor:\n\n```\nlxc_features:\n    shiftfs: \"true\"\n```\n\nIf either idmapped_mounts or shiftfs is true, then LXD has already enabled it. If it is not enabled, you must enable it first.\n\n2. Usage\n\nThen you can simply set the \"shift\" config key to \"true\" in the disk device options. See: LXD Documentation on disk devices\n\nSee also: tutorial in the LXD forums\n\n"
    },
    {
      "title": "Bash completion doesn't work",
      "level": 3,
      "content": "This workaround may fix the issue:\n\n```\n# ln -s /usr/share/bash-completion/completions/lxd /usr/share/bash-completion/completions/lxc\n```\n\n"
    },
    {
      "title": "lxd-agent inside a virtual machine",
      "level": 3,
      "content": "Inside some virtual machine images, the lxd-agent is not enabled by default.\n\nIn this case, you have to enable it manually, for example by mounting a 9p network share. This requires console access with a valid user.\n\n1. Login with lxc console and replace virtualmachine-name accordingly.\n\n```\n$ lxc console virtualmachine-name\n```\n\nLogin as root:\n\n```\n$ su root\n```\n\nMount the network share:\n\n```\n$ mount -t 9p config /mnt/\n```\n\nGo into the folder and run the install script (this will enable the lxd-agent inside the VM):\n\n```\n$ cd /mnt/\n$ ./install.sh\n```\n\nAfter a successful install, reboot with:\n\n```\n$ reboot\n```\n\nAfterwards, the lxd-agent is available and lxc exec should work.\n\n"
    },
    {
      "title": "Check kernel config",
      "level": 3,
      "content": "By default, the Arch Linux kernel is compiled correctly for Linux Containers and its frontend LXD. However, if you are using a custom kernel or changed the kernel options, the kernel might be configured incorrectly. Verify that your kernel is properly configured:\n\n```\n$ lxc-checkconfig\n```\n\n"
    },
    {
      "title": "Resource limits are not applied when viewed from inside a container",
      "level": 3,
      "content": "Install lxcfs and start lxcfs.service.\n\nlxd will need to be restarted. Enable lxcfs.service for the service to be started at boot time.\n\n"
    },
    {
      "title": "Starting a virtual machine fails",
      "level": 3,
      "content": "If you see the error:\n\n```\nError: Couldn't find one of the required UEFI firmware files: [{code:OVMF_CODE.4MB.fd vars:OVMF_VARS.4MB.ms.fd} {code:OVMF_CODE.2MB.fd vars:OVMF_VARS.2MB.ms.fd} {code:OVMF_CODE.fd vars:OVMF_VARS.ms.fd} {code:OVMF_CODE.fd vars:qemu.nvram}]\n```\n\nIt's because Arch Linux does not distribute secure boot signed ovmf firmware. To boot virtual machines you need to disable secure boot for the time being:\n\n```\n$ lxc launch ubuntu:18.04 test-vm --vm -c security.secureboot=false\n```\n\nThis can also be added to the default profile by doing:\n\n```\n$ lxc profile set default security.secureboot=false\n```\n\n"
    },
    {
      "title": "No IPv4 with systemd-networkd",
      "level": 3,
      "content": "Starting with version version 244.1, systemd detects if /sys is writable by containers. If it is, udev is automatically started and breaks IPv4 in unprivileged containers. See commit bf331d8 and discussion on linuxcontainers.\n\nOn containers created past 2020, there should already be a systemd-networkd.service override to work around this issue, create it if it is not:\n\n```\n/etc/systemd/system/systemd-networkd.service.d/lxc.conf\n```\n\n```\n[Service]\nBindReadOnlyPaths=/sys\n```\n\nYou could also work around this issue by setting raw.lxc: lxc.mount.auto = proc:rw sys:ro in the profile of the container to ensure /sys is read-only for the entire container, although this may be problematic, as per the linked discussion above.\n\n"
    },
    {
      "title": "No networking with ufw",
      "level": 3,
      "content": "When running LXD on a system with ufw, the output of lxc ls will contain an empty IPv4 field, outbound requests will not be forwarded out of the container, and inbound requests will not be forwarded into the container. As seen in a thread on LXC's Discourse instance, ufw will block traffic from LXD bridges by default. The solution is to configure two new ufw rules for each bridge:\n\n```\n# ufw route allow in on lxdbr0\n# ufw allow in on lxdbr0\n```\n\nFor more information on these two commands, check out this thread which describes these commands and their limitations in more detail.\n\n"
    },
    {
      "title": "No networking with Docker installed",
      "level": 3,
      "content": "You have Docker installed on the host, and you're not able to access LAN or internet from within a lxc container.\n\n```\n# iptables -I DOCKER-USER -i lxdbr0 -o interface -j ACCEPT\n# iptables -I DOCKER-USER -o lxdbr0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT\n```\n\nOn the first line, replace interface with the external network interface (what connects the host to LAN/internet, e.g. enp6so, wlp5s0, ...). Also replace lxdbr0 if needed.\n\nFor more details, see this note in the LXD documentation.\n\n"
    },
    {
      "title": "Building a snap with snapcraft: craft-providers error: LXD requires additional permissions.",
      "level": 3,
      "content": "If you get the following error:\n\n```\ncraft-providers error: LXD requires additional permissions.\nEnsure that the user is in the 'lxd' group.\nVisit https://documentation.ubuntu.com/lxd/en/latest/getting_started/ for instructions on installing and configuring LXD for your operating system.\nFull execution log: '/home/your_user/.local/state/snapcraft/log/snapcraft-20231129-221308.209638.log'\n```\n\nThis might occur if you try to build a snap with the snapcraft command. If the error is still there even after you added yourself to the lxd group and rebooted, try installing lxd via snap instead of as an Arch package (don't install both), that might solve this problem.\n\n"
    },
    {
      "title": "Uninstall",
      "level": 2,
      "content": "Stop and disable lxd.service and lxd.socket. Then uninstall the lxd package.\n\nIf you uninstalled the package without disabling the service, you might have a lingering broken symlink at /etc/systemd/system/multi-user.wants/lxd.service.\n\nIf you want to remove all data:\n\n```\n# rm -r /var/lib/lxd\n```\n\nIf you used any of the example networking configuration, you should remove those as well.\n\n"
    },
    {
      "title": "See also",
      "level": 2,
      "content": "- The official LXD homepage\n- Official documentation\n- Getting Started Guide\n- Advanced Guide\n- Official Forum\n- The LXD GitHub page\n- Tutorials in LXD Forum\n- Release Notes in LXD Forum\n\n"
    }
  ]
}